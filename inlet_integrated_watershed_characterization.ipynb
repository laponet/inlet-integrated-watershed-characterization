{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inlet-Integrated Watershed Characterization\n",
    "\n",
    "This notebook implements an inlet-integrated watershed characterization workflow for urban hydrology and stormwater analysis.\n",
    "\n",
    "## Credits\n",
    "- **Author:** Lapone Techapinyawat, Ph.D. (Texas A&M University–Corpus Christi), 2025  \n",
    "- **Author:** Hua Zhang, Ph.D. (Texas A&M University–Corpus Christi), 2025\n",
    "\n",
    "## Affiliation\n",
    "Geospatial Computer Science Program, Texas A&M University–Corpus Christi (TAMUCC)\n",
    "\n",
    "## Notes\n",
    "- This repository shares the workflow and documentation for research and educational use.\n",
    "- Please cite/credit appropriately if you reuse or adapt this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b55b7eb",
   "metadata": {},
   "source": [
    "# Urban Fill & Spill (Jupyter Notebook)\n",
    "\n",
    "This notebook implements a multi-step workflow for **depression-aware urban runoff modeling** using high‑resolution\n",
    "topography and stormwater infrastructure. The main components are:\n",
    "\n",
    "1. **Iterative depression merging & downspout allocation** (Algorithm 1)\n",
    "2. **Inlet watershed delineation via flow tracing** (Algorithm 2)\n",
    "3. **Water depth mapping via depression filling** (Algorithm 3)\n",
    "4. **Drainage-tree construction and event simulation** (Algorithms 4–6)\n",
    "\n",
    "## What you need\n",
    "- Python geospatial stack (GeoPandas, Rasterio, Shapely, NetworkX, NumPy/Pandas, Matplotlib).\n",
    "- Input layers: depression basins, lowest points, inlets, study area (and any supporting rasters).\n",
    "\n",
    "## How to run\n",
    "Run the notebook **top to bottom**. Each algorithm section writes intermediate outputs that are used by the next section.\n",
    "Update the **User inputs** blocks (file paths, iteration numbers, and calibration factors) before running.\n",
    "\n",
    "## Notes\n",
    "- Comments are written to be GitHub-friendly (clear assumptions, inputs/outputs, and where files come from).\n",
    "- If you use or adapt this workflow in a publication, please cite the associated paper(s) and acknowledge the data sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877001d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from shapely.geometry import (\n",
    "    Point,\n",
    "    LineString,\n",
    "    MultiLineString,\n",
    "    Polygon,\n",
    "    MultiPolygon,\n",
    "    GeometryCollection,\n",
    "    MultiPoint\n",
    ")\n",
    "from shapely.ops import (\n",
    "    linemerge,\n",
    "    unary_union,\n",
    "    nearest_points,\n",
    "    split,\n",
    "    voronoi_diagram\n",
    ")\n",
    "from shapely.validation import make_valid\n",
    "from scipy.spatial import Voronoi\n",
    "from rtree import index\n",
    "import warnings\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Input and Output File Paths\n",
    "BASIN_SHAPEFILE_INITIAL = \"BS_filtered.shp\"\n",
    "DEM_PATH = \"DEM.tif\"\n",
    "INLETS_SHAPEFILE = \"inlet.shp\"\n",
    "SINKS_SHAPEFILE = \"SNK.shp\"\n",
    "DOWNSPOUTS_SHAPEFILE = \"DS.shp\"\n",
    "STUDY_AREA_SHAPEFILE = \"studyarea.shp\"\n",
    "\n",
    "BASIN_HOLES_OUTPUT_PREFIX = \"basin_holes\"\n",
    "BASIN_OUTPUT_PREFIX = \"basin\"\n",
    "BOUNDARY_SEGMENTS_OUTPUT_PREFIX = \"boundary_segments\"\n",
    "BOUNDARY_POINTS_OUTPUT_PREFIX = \"boundary_points\"\n",
    "MID_SEGMENT_OUTPUT_PREFIX = \"mid_segment\"\n",
    "LOWEST_SEGMENT_POINTS_OUTPUT_PREFIX = \"lowest_segment_points\"\n",
    "BASIN_SEG_PNTS_OUTPUT_PREFIX = \"basin_seg_pnts\"\n",
    "TEMP_SNAPPED_DOWNSPOUTS_OUTPUT = \"temp_snapped_downspouts.shp\"\n",
    "LOWEST_BASIN_POINTS_OUTPUT_PREFIX = \"lowest_basin_points\"\n",
    "SINKS_PROCESSING_OUTPUT = \"sinks_processing.shp\"\n",
    "UPDATED_SINKS_OUTPUT = \"updated_sinks.shp\"\n",
    "FINAL_BASINS_OUTPUT_PREFIX = \"final_basins\"\n",
    "FINAL_SNAPPED_DOWNSPOUTS_OUTPUT_PREFIX = \"final_snapped_downspouts\"\n",
    "\n",
    "\n",
    "def polygon_to_line(polygon):\n",
    "    return polygon.boundary\n",
    "\n",
    "def create_points_along_line(line, interval=1.00):\n",
    "    n_points = int(line.length // interval)\n",
    "    if n_points < 1:\n",
    "        return [line.interpolate(0.5, normalized=True)]\n",
    "    return [line.interpolate(i * interval, normalized=False) for i in range(1, n_points + 1)]\n",
    "\n",
    "def ensure_numeric_basin_id(gdf, old_col=\"Id\", new_col=\"basin_id\"):\n",
    "    if old_col in gdf.columns:\n",
    "        gdf = gdf.rename(columns={old_col: new_col})\n",
    "    if new_col in gdf.columns:\n",
    "        gdf[new_col] = pd.to_numeric(gdf[new_col], errors='coerce')\n",
    "    else:\n",
    "        print(f\"Warning: Column '{new_col}' not found in GeoDataFrame.\")\n",
    "    return gdf\n",
    "\n",
    "def get_elevation_at_point(point, dem_array, dem_transform):\n",
    "    if point is None or point.is_empty:\n",
    "        return np.nan\n",
    "    row, col = rasterio.transform.rowcol(dem_transform, point.x, point.y)\n",
    "    if 0 <= row < dem_array.shape[0] and 0 <= col < dem_array.shape[1]:\n",
    "        elevation = float(dem_array[row, col])\n",
    "        if elevation == dem_array.max() or elevation == 0:\n",
    "            return np.nan\n",
    "        return elevation\n",
    "    return np.nan\n",
    "\n",
    "def assign_basin_ids_to_segments(boundary_segments, basins):\n",
    "    print(\"Creating spatial index for basins...\")\n",
    "    basin_sindex = index.Index()\n",
    "    for idx, basin in basins.iterrows():\n",
    "        basin_sindex.insert(idx, basin.geometry.bounds)\n",
    "\n",
    "    basin_ids_list = []\n",
    "    print(\"Assigning basin IDs to segments...\")\n",
    "    for segment in boundary_segments.geometry:\n",
    "        potential_basins_idx = list(basin_sindex.intersection(segment.bounds))\n",
    "        if potential_basins_idx:\n",
    "            matching_basins = []\n",
    "            for idx in potential_basins_idx:\n",
    "                basin = basins.iloc[idx]\n",
    "                if basin.geometry.boundary.contains(segment):\n",
    "                    matching_basins.append(str(basin['basin_id']))\n",
    "                if len(matching_basins) >= 2:\n",
    "                    break\n",
    "            basin_ids_list.append(','.join(sorted(matching_basins)[:2]))\n",
    "        else:\n",
    "            basin_ids_list.append('')\n",
    "    boundary_segments['bas_ids'] = basin_ids_list\n",
    "    print(f\"Assigned IDs to {len(boundary_segments)} segments\")\n",
    "    return boundary_segments\n",
    "\n",
    "def create_midpoint_buffer(segment, buffer_distance=0.01):\n",
    "    midpoint = segment.interpolate(0.5, normalized=True)\n",
    "    return midpoint.buffer(buffer_distance)\n",
    "\n",
    "def identify_and_export_hole_basins(basins, output_hole_shapefile):\n",
    "    all_polygons = unary_union(basins['geometry'])\n",
    "    holes = []\n",
    "    if isinstance(all_polygons, Polygon):\n",
    "        holes.extend([Polygon(hole) for hole in all_polygons.interiors])\n",
    "    elif isinstance(all_polygons, MultiPolygon):\n",
    "        for polygon in all_polygons.geoms:\n",
    "            holes.extend([Polygon(hole) for hole in polygon.interiors])\n",
    "    print(f\"  - Found {len(holes)} holes\")\n",
    "    holes_gdf = gpd.GeoDataFrame(geometry=holes, crs=basins.crs)\n",
    "    holes_gdf.to_file(output_hole_shapefile)\n",
    "    print(f\"  - Exported {len(holes)} holes to {output_hole_shapefile}\")\n",
    "    return holes_gdf\n",
    "\n",
    "def process_segment(row, dem_array, dem_transform, interval=0.30):\n",
    "    segment = row.geometry\n",
    "    segment_id = row.seg_id\n",
    "    basin_ids_str = row.bas_ids\n",
    "    pts = []\n",
    "\n",
    "    points_on_segment = create_points_along_line(segment, interval)\n",
    "    if not points_on_segment:\n",
    "        return ([], {'seg_id': segment_id, 'bas_ids': basin_ids_str, 'geometry': create_midpoint_buffer(segment)})\n",
    "\n",
    "    xs = [pt.x for pt in points_on_segment]\n",
    "    ys = [pt.y for pt in points_on_segment]\n",
    "    rows, cols = rasterio.transform.rowcol(dem_transform, xs, ys)\n",
    "    rows = np.clip(rows, 0, dem_array.shape[0] - 1)\n",
    "    cols = np.clip(cols, 0, dem_array.shape[1] - 1)\n",
    "    elevations = dem_array[rows, cols]\n",
    "\n",
    "    for i, pt in enumerate(points_on_segment):\n",
    "        elev = float(elevations[i])\n",
    "        if elev == dem_array.max() or elev == 0:\n",
    "            elev = np.nan\n",
    "        if not np.isnan(elev):\n",
    "            pts.append({\n",
    "                'seg_id': segment_id,\n",
    "                'bas_ids': basin_ids_str,\n",
    "                'elev': elev,\n",
    "                'geometry': pt\n",
    "            })\n",
    "    buffer_dict = {\n",
    "        'seg_id': segment_id,\n",
    "        'bas_ids': basin_ids_str,\n",
    "        'geometry': create_midpoint_buffer(segment)\n",
    "    }\n",
    "    return pts, buffer_dict\n",
    "\n",
    "def process_basins_and_segments(basins, dem_path, iteration, holes_gdf=None, save_outputs=False, interval=0.30):\n",
    "    print(f\"Iteration {iteration}: Processing basin data\")\n",
    "    basins = basins.copy()\n",
    "    basins['geometry'] = basins['geometry'].apply(make_valid)\n",
    "    if save_outputs:\n",
    "        basins[['basin_id', 'geometry']].to_file(f\"{BASIN_OUTPUT_PREFIX}_{iteration}.shp\")\n",
    "        print(f\"  - Basins saved to '{BASIN_OUTPUT_PREFIX}_{iteration}.shp'\")\n",
    "    print(f\"  - Processing {len(basins)} basins\")\n",
    "\n",
    "    if iteration == 0:\n",
    "        print(\"Identifying and exporting hole basins\")\n",
    "        all_polygons = unary_union(basins['geometry'])\n",
    "        holes = []\n",
    "        if isinstance(all_polygons, Polygon):\n",
    "            holes.extend([Polygon(hole) for hole in all_polygons.interiors])\n",
    "        elif isinstance(all_polygons, MultiPolygon):\n",
    "            for polygon in all_polygons.geoms:\n",
    "                holes.extend([Polygon(hole) for hole in polygon.interiors])\n",
    "        holes_gdf = gpd.GeoDataFrame(geometry=holes, crs=basins.crs)\n",
    "        holes_gdf.to_file(f\"{BASIN_HOLES_OUTPUT_PREFIX}_{iteration}.shp\")\n",
    "        print(f\"  - Found and exported {len(holes)} holes\")\n",
    "    else:\n",
    "        print(\"Using existing hole basins\")\n",
    "\n",
    "    print(\"Creating basin lines\")\n",
    "    basin_lines = basins.geometry.boundary\n",
    "    merged_line = linemerge(unary_union(basin_lines))\n",
    "    if isinstance(merged_line, LineString):\n",
    "        boundary_segments = gpd.GeoDataFrame(geometry=[merged_line], crs=basins.crs)\n",
    "    elif isinstance(merged_line, MultiLineString):\n",
    "        boundary_segments = gpd.GeoDataFrame(geometry=list(merged_line.geoms), crs=basins.crs)\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected geometry type for merged_line\")\n",
    "\n",
    "    boundary_segments['seg_id'] = [f\"segment_{i}\" for i in range(len(boundary_segments))]\n",
    "    print(f\"  - Created {len(boundary_segments)} boundary segments\")\n",
    "\n",
    "    print(\"Assigning basin IDs to segments\")\n",
    "    boundary_segments = assign_basin_ids_to_segments(boundary_segments, basins)\n",
    "\n",
    "    print(\"Creating points along segments and calculating elevations\")\n",
    "    with rasterio.open(dem_path) as src:\n",
    "        dem_array = src.read(1)\n",
    "        dem_transform = src.transform\n",
    "\n",
    "    all_points = []\n",
    "    midpoint_buffers = []\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [\n",
    "            executor.submit(process_segment, row, dem_array, dem_transform, interval)\n",
    "            for idx, row in boundary_segments.iterrows()\n",
    "        ]\n",
    "        for future in as_completed(futures):\n",
    "            pts, buffer_dict = future.result()\n",
    "            all_points.extend(pts)\n",
    "            midpoint_buffers.append(buffer_dict)\n",
    "\n",
    "    for i, pt in enumerate(all_points):\n",
    "        pt['pnt_id'] = f\"point_{i}\"\n",
    "\n",
    "    all_points_gdf = gpd.GeoDataFrame(all_points, crs=basins.crs, geometry='geometry')\n",
    "    midpoint_buffers_gdf = gpd.GeoDataFrame(midpoint_buffers, crs=basins.crs, geometry='geometry')\n",
    "\n",
    "    print(\"Filtering out points on hole basin boundaries\")\n",
    "    if holes_gdf is not None and not holes_gdf.empty:\n",
    "        hole_boundaries_gdf = gpd.GeoDataFrame(geometry=holes_gdf.boundary, crs=holes_gdf.crs)\n",
    "        points_on_holes = gpd.sjoin(all_points_gdf, hole_boundaries_gdf, how=\"inner\", predicate=\"intersects\")\n",
    "        filtered_points_gdf = all_points_gdf.drop(points_on_holes.index)\n",
    "        print(f\"  - Filtered out {len(points_on_holes)} points\")\n",
    "    else:\n",
    "        filtered_points_gdf = all_points_gdf\n",
    "        print(\"  - No holes found; no points filtered.\")\n",
    "\n",
    "    if save_outputs:\n",
    "        print(\"Saving output files\")\n",
    "        boundary_segments.to_file(f\"{BOUNDARY_SEGMENTS_OUTPUT_PREFIX}_{iteration}.shp\")\n",
    "        filtered_points_gdf.to_file(f\"{BOUNDARY_POINTS_OUTPUT_PREFIX}_{iteration}.shp\")\n",
    "        midpoint_buffers_gdf.to_file(f\"{MID_SEGMENT_OUTPUT_PREFIX}_{iteration}.shp\")\n",
    "        print(\"  - All output files saved successfully\")\n",
    "    else:\n",
    "        print(f\"Skipping output file saving for iteration {iteration}\")\n",
    "\n",
    "    return boundary_segments, filtered_points_gdf, midpoint_buffers_gdf, holes_gdf, basins\n",
    "\n",
    "def filter_lowest_elevation_points(points_gdf):\n",
    "    print(\"Filtering lowest elevation points per segment\")\n",
    "    points_gdf['elev'] = pd.to_numeric(points_gdf['elev'], errors='coerce')\n",
    "    lowest_elevation_points = points_gdf.sort_values('elev', ascending=True).drop_duplicates(subset='seg_id', keep='first')\n",
    "    print(\"  - Filtered lowest elevation points\")\n",
    "    return lowest_elevation_points\n",
    "\n",
    "def merge_basins_with_redundant_first_points(basin_info_df):\n",
    "    first_point_to_basins = {}\n",
    "    for idx, row in basin_info_df.iterrows():\n",
    "        seg_pnts = row['seg_pnts']\n",
    "        basin_id = row['basin_id']\n",
    "        if pd.notna(seg_pnts) and isinstance(seg_pnts, str) and seg_pnts.strip():\n",
    "            first_point = seg_pnts.split(',')[0]\n",
    "            first_point_to_basins.setdefault(first_point, []).append(basin_id)\n",
    "    basins_to_merge = []\n",
    "    for first_point, basin_ids in first_point_to_basins.items():\n",
    "        if len(basin_ids) > 1:\n",
    "            basins_to_merge.append({'first_point': first_point, 'basin_ids': basin_ids})\n",
    "    if basins_to_merge:\n",
    "        print(f\"  - Found {len(basins_to_merge)} groups of basins to merge\")\n",
    "        return basins_to_merge\n",
    "    else:\n",
    "        print(\"  - No basins to merge\")\n",
    "        return None\n",
    "\n",
    "def merge_basins(basins_no_inlets, basins_to_merge, basins_with_inlets_ids, original_basins):\n",
    "    merged_basins = basins_no_inlets.copy()\n",
    "    for group in basins_to_merge:\n",
    "        basin_ids = group['basin_ids']\n",
    "        new_basin_id = min(basin_ids)\n",
    "        basins_to_merge_geom = basins_no_inlets[basins_no_inlets['basin_id'].isin(basin_ids)]['geometry']\n",
    "        merged_geometry = unary_union(basins_to_merge_geom)\n",
    "        merged_basins = merged_basins[~merged_basins['basin_id'].isin(basin_ids)]\n",
    "        new_basin = gpd.GeoDataFrame([{'basin_id': new_basin_id, 'geometry': merged_geometry}], crs=basins_no_inlets.crs)\n",
    "        merged_basins = pd.concat([merged_basins, new_basin], ignore_index=True)\n",
    "    basins_with_inlets = original_basins[original_basins['basin_id'].isin(basins_with_inlets_ids)]\n",
    "    final_basins = pd.concat([merged_basins, basins_with_inlets], ignore_index=True)\n",
    "    return final_basins\n",
    "\n",
    "def iterative_merge_basins(basin_info_df_func, basins, basins_with_inlets_ids, max_iter=10):\n",
    "    iteration = 0\n",
    "    while iteration < max_iter:\n",
    "        basin_info_df = basin_info_df_func(basins)\n",
    "        groups = merge_basins_with_redundant_first_points(basin_info_df)\n",
    "        prev_count = len(basins)\n",
    "        if groups:\n",
    "            basins_no_inlets = basins[~basins['basin_id'].isin(basins_with_inlets_ids)]\n",
    "            basins = merge_basins(basins_no_inlets, groups, basins_with_inlets_ids, basins)\n",
    "            new_count = len(basins)\n",
    "            print(f\"Merge iteration {iteration}: {prev_count} -> {new_count} basins\")\n",
    "            if new_count == prev_count:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "        iteration += 1\n",
    "    return basins\n",
    "\n",
    "def get_basin_info_df(basins):\n",
    "    return pd.DataFrame({\n",
    "        'basin_id': basins['basin_id'],\n",
    "        'seg_pnts': basins.apply(lambda r: r.get('seg_pnts', ''), axis=1)\n",
    "    })\n",
    "\n",
    "def update_sinks_after_merging(sinks_gdf, basins_updated_gdf, dem_data, dem_transform,\n",
    "                                output_sinks_shapefile, output_sinks_processing_shapefile,\n",
    "                                lowest_basin_points_gdf):\n",
    "    print(\"Updating sinks after merging basins\")\n",
    "    basins_updated_gdf = ensure_numeric_basin_id(basins_updated_gdf, old_col='basin_id', new_col='basin_id')\n",
    "    sinks_gdf = sinks_gdf.to_crs(basins_updated_gdf.crs)\n",
    "    sinks_with_basins = gpd.sjoin(sinks_gdf[['geometry']], basins_updated_gdf[['basin_id', 'geometry']], how='left', predicate='within')\n",
    "    sinks_with_basins['basin_id'] = sinks_with_basins['basin_id'].fillna(-1).astype(int)\n",
    "    sinks_with_basins['elev'] = sinks_with_basins['geometry'].apply(lambda point: get_elevation_at_point(point, dem_data, dem_transform))\n",
    "    sinks_with_basins['basin_id'] = pd.to_numeric(sinks_with_basins['basin_id'], errors='coerce')\n",
    "    sinks_with_basins.to_file(output_sinks_processing_shapefile)\n",
    "    print(f\"  - Intermediate sinks exported to {output_sinks_processing_shapefile}\")\n",
    "    valid_sinks = sinks_with_basins[sinks_with_basins['basin_id'] != -1]\n",
    "    lowest_elevations = lowest_basin_points_gdf.set_index('basin_id')['elev'].to_dict()\n",
    "    grouped = valid_sinks.groupby('basin_id')\n",
    "    final_sinks = []\n",
    "    for basin_id, basin_geom in zip(basins_updated_gdf['basin_id'], basins_updated_gdf['geometry']):\n",
    "        if basin_id in grouped.groups:\n",
    "            basin_sinks = grouped.get_group(basin_id)\n",
    "            lowest_sink = basin_sinks.loc[basin_sinks['elev'].idxmin()]\n",
    "            elev = lowest_sink['elev']\n",
    "            geometry = lowest_sink['geometry']\n",
    "        else:\n",
    "            centroid = basin_geom.centroid\n",
    "            elev = get_elevation_at_point(centroid, dem_data, dem_transform)\n",
    "            geometry = centroid\n",
    "            print(f\"  - Created synthetic sink for basin {basin_id}\")\n",
    "        deep_elev = elev\n",
    "        if elev == 0:\n",
    "            print(f\"Warning: Basin {basin_id} has a sink with elevation 0. Setting 'deep_elev' to NaN.\")\n",
    "            deep_elev = np.nan\n",
    "        elev = np.round(elev, 5) if not np.isnan(elev) else np.nan\n",
    "        deep_elev = np.round(deep_elev, 5) if not np.isnan(deep_elev) else np.nan\n",
    "        if basin_id in lowest_elevations:\n",
    "            lowest_elev = lowest_elevations[basin_id]\n",
    "        else:\n",
    "            lowest_elev = np.nan\n",
    "            print(f\"Warning: Basin {basin_id} does not have a lowest basin point elevation.\")\n",
    "        if not np.isnan(lowest_elev) and not np.isnan(elev):\n",
    "            dZ = np.round(lowest_elev - elev, 5)\n",
    "        else:\n",
    "            dZ = np.nan\n",
    "        final_sinks.append({\n",
    "            'basin_id': basin_id,\n",
    "            'elev': elev,\n",
    "            'deep_elev': deep_elev,\n",
    "            'dZ': dZ,\n",
    "            'geometry': geometry\n",
    "        })\n",
    "    final_sinks_gdf = gpd.GeoDataFrame(final_sinks, crs=basins_updated_gdf.crs)\n",
    "    for col in ['elev', 'deep_elev', 'dZ']:\n",
    "        final_sinks_gdf[col] = final_sinks_gdf[col].astype('float64')\n",
    "    final_sinks_gdf.to_file(output_sinks_shapefile)\n",
    "    print(f\"  - Final sinks with 'elev', 'deep_elev', and 'dZ' exported to {output_sinks_shapefile}\")\n",
    "    print(f\"  - Total sinks: {len(final_sinks_gdf)}, Synthetic sinks: {len(final_sinks_gdf) - len(valid_sinks)}\\n\")\n",
    "\n",
    "def process_downspouts(downspouts_shapefile, all_basins, hole_basins, dem_data, dem_transform, output_snapped_downspouts_shapefile):\n",
    "    print(\"Processing and snapping downspouts to hole basin boundaries.\")\n",
    "    downspouts = gpd.read_file(downspouts_shapefile)\n",
    "    hole_boundaries = hole_basins.geometry.boundary.unary_union\n",
    "    snapped_downspouts = []\n",
    "    for _, downspout in downspouts.iterrows():\n",
    "        nearest_point = nearest_points(downspout.geometry, hole_boundaries)[1]\n",
    "        elevation = get_elevation_at_point(nearest_point, dem_data, dem_transform)\n",
    "        snapped_downspouts.append({\n",
    "            'geometry': Point(nearest_point.x, nearest_point.y),\n",
    "            'elevation': elevation,\n",
    "            'is_downsp': True,\n",
    "            'basin_id': -1\n",
    "        })\n",
    "    snapped_downspouts_gdf = gpd.GeoDataFrame(snapped_downspouts, crs=all_basins.crs)\n",
    "    snapped_downspouts_gdf.to_file(output_snapped_downspouts_shapefile)\n",
    "    print(f\"Snapped downspouts saved to '{output_snapped_downspouts_shapefile}'\")\n",
    "    return snapped_downspouts_gdf\n",
    "\n",
    "def create_mid_segment_points(basins_gdf):\n",
    "    mid_segments = []\n",
    "    for idx, row in basins_gdf.iterrows():\n",
    "        boundary = row.geometry.boundary\n",
    "        if isinstance(boundary, MultiLineString):\n",
    "            for line in boundary.geoms:\n",
    "                midpoint = line.interpolate(0.5, normalized=True)\n",
    "                mid_segments.append({\n",
    "                    'geometry': midpoint,\n",
    "                    'basin_ids': str(row['basin_id'])\n",
    "                })\n",
    "        else:\n",
    "            midpoint = boundary.interpolate(0.5, normalized=True)\n",
    "            mid_segments.append({\n",
    "                'geometry': midpoint,\n",
    "                'basin_ids': str(row['basin_id'])\n",
    "            })\n",
    "    return gpd.GeoDataFrame(mid_segments, crs=basins_gdf.crs)\n",
    "\n",
    "def split_hole_basins_and_update_ids(hole_basins_gdf, snapped_downspouts_gdf, mid_segment_gdf, max_basin_id):\n",
    "    print(\"Splitting hole basins...\")\n",
    "    split_basins = []\n",
    "    for idx, hole_basin in hole_basins_gdf.iterrows():\n",
    "        try:\n",
    "            basin_geometry = hole_basin.geometry\n",
    "            original_basin_id = hole_basin['basin_id']\n",
    "            basin_downspouts = snapped_downspouts_gdf[snapped_downspouts_gdf.intersects(basin_geometry)].copy()\n",
    "            n_points = len(basin_downspouts)\n",
    "            if n_points == 0:\n",
    "                print(f\"Basin {idx}: No downspouts found\")\n",
    "                split_basins.append({\n",
    "                    'basin_id': original_basin_id,\n",
    "                    'geometry': basin_geometry,\n",
    "                    'is_hole': True,\n",
    "                    'is_split': False,\n",
    "                    'shp_length': float(basin_geometry.length),\n",
    "                    'shp_area': float(basin_geometry.area)\n",
    "                })\n",
    "            elif n_points == 1:\n",
    "                print(f\"Basin {idx}: Single downspout - assigning whole basin\")\n",
    "                split_basins.append({\n",
    "                    'basin_id': original_basin_id,\n",
    "                    'geometry': basin_geometry,\n",
    "                    'is_hole': True,\n",
    "                    'is_split': False,\n",
    "                    'shp_length': float(basin_geometry.length),\n",
    "                    'shp_area': float(basin_geometry.area)\n",
    "                })\n",
    "                ds_idx = basin_downspouts.index[0]\n",
    "                snapped_downspouts_gdf.at[ds_idx, 'basin_id'] = original_basin_id\n",
    "            else:\n",
    "                try:\n",
    "                    points = MultiPoint([point for point in basin_downspouts.geometry])\n",
    "                    vor_polys = voronoi_diagram(points, envelope=basin_geometry)\n",
    "                    for i, voronoi_poly in enumerate(voronoi_diagram(points, envelope=basin_geometry).geoms):\n",
    "                        split_geom = basin_geometry.intersection(voronoi_poly)\n",
    "                        if not split_geom.is_empty and split_geom.area > 0:\n",
    "                            max_basin_id += 1\n",
    "                            new_basin_id = max_basin_id\n",
    "                            split_basins.append({\n",
    "                                'basin_id': new_basin_id,\n",
    "                                'geometry': split_geom,\n",
    "                                'is_hole': True,\n",
    "                                'is_split': True,\n",
    "                                'shp_length': float(split_geom.length),\n",
    "                                'shp_area': float(split_geom.area)\n",
    "                            })\n",
    "                            for ds_idx, downspout in basin_downspouts.iterrows():\n",
    "                                if split_geom.contains(downspout.geometry):\n",
    "                                    snapped_downspouts_gdf.at[ds_idx, 'basin_id'] = new_basin_id\n",
    "                    print(f\"Basin {idx}: Successfully split into {len(voronoi_diagram(points, envelope=basin_geometry).geoms)} parts\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Voronoi failed for basin {idx}, using alternative splitting\")\n",
    "                    buffer_distance = np.sqrt(basin_geometry.area) / (2 * len(basin_downspouts))\n",
    "                    for i, (ds_idx, downspout) in enumerate(basin_downspouts.iterrows()):\n",
    "                        max_basin_id += 1\n",
    "                        new_basin_id = max_basin_id\n",
    "                        buffer = downspout.geometry.buffer(buffer_distance)\n",
    "                        split_geom = basin_geometry.intersection(buffer)\n",
    "                        if not split_geom.is_empty and split_geom.area > 0:\n",
    "                            split_basins.append({\n",
    "                                'basin_id': new_basin_id,\n",
    "                                'geometry': split_geom,\n",
    "                                'is_hole': True,\n",
    "                                'is_split': True,\n",
    "                                'shp_length': float(split_geom.length),\n",
    "                                'shp_area': float(split_geom.area)\n",
    "                            })\n",
    "                            snapped_downspouts_gdf.at[ds_idx, 'basin_id'] = new_basin_id\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing basin {idx}: {str(e)}\")\n",
    "            split_basins.append({\n",
    "                'basin_id': original_basin_id,\n",
    "                'geometry': basin_geometry,\n",
    "                'is_hole': True,\n",
    "                'is_split': False,\n",
    "                'shp_length': float(basin_geometry.length),\n",
    "                'shp_area': float(basin_geometry.area)\n",
    "            })\n",
    "    split_basins_gdf = gpd.GeoDataFrame(split_basins, crs=hole_basins_gdf.crs)\n",
    "    print(f\"Created {len(split_basins_gdf)} split basins\")\n",
    "    return split_basins_gdf, snapped_downspouts_gdf, max_basin_id\n",
    "\n",
    "def assign_basin_ids_to_downspouts(snapped_downspouts, all_basins, buffer_distance=0.01):\n",
    "    for idx, downspout in snapped_downspouts.iterrows():\n",
    "        buffer = downspout.geometry.buffer(buffer_distance)\n",
    "        intersecting_basins = all_basins[all_basins.intersects(buffer)]\n",
    "        if not intersecting_basins.empty:\n",
    "            split_hole_basins = intersecting_basins[intersecting_basins['is_hole'] == True]\n",
    "            if not split_hole_basins.empty:\n",
    "                snapped_downspouts.at[idx, 'basin_id'] = split_hole_basins.iloc[0]['basin_id']\n",
    "            else:\n",
    "                snapped_downspouts.at[idx, 'basin_id'] = intersecting_basins.iloc[0]['basin_id']\n",
    "        else:\n",
    "            print(f\"Warning: No basin found for downspout at index {idx}\")\n",
    "    return snapped_downspouts\n",
    "\n",
    "def process_basin_info(basins, lowest_segment_points, snapped_downspouts, basins_with_inlets_ids, iteration, save_outputs):\n",
    "    basin_points_dict = {}\n",
    "    if not lowest_segment_points.empty:\n",
    "        lsp = lowest_segment_points.copy()\n",
    "        lsp['bas_ids_list'] = lsp['bas_ids'].str.split(',')\n",
    "        lsp = lsp.explode('bas_ids_list')\n",
    "        lsp['bas_ids_list'] = lsp['bas_ids_list'].str.strip()\n",
    "        lsp['bas_ids_list'] = pd.to_numeric(lsp['bas_ids_list'], errors='ignore')\n",
    "        grouped = lsp.groupby('bas_ids_list')\n",
    "        for basin_id, group in grouped:\n",
    "            basin_points_dict[basin_id] = group\n",
    "\n",
    "    snapped_downspouts_dict = {}\n",
    "    if snapped_downspouts is not None and not snapped_downspouts.empty:\n",
    "        sds_group = snapped_downspouts.groupby('basin_id')\n",
    "        for basin_id, group in sds_group:\n",
    "            snapped_downspouts_dict[basin_id] = group\n",
    "\n",
    "    basin_info_list = []\n",
    "    for idx, basin in basins.iterrows():\n",
    "        basin_id = basin['basin_id']\n",
    "        if basin_id in basins_with_inlets_ids:\n",
    "            basin_info_list.append({\n",
    "                'basin_id': basin_id,\n",
    "                'seg_pnts': '',\n",
    "                'pnt1_ele': np.nan\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        seg_pnts = ''\n",
    "        pnt1_ele = np.nan\n",
    "\n",
    "        if snapped_downspouts is not None and basin_id in snapped_downspouts_dict:\n",
    "            basin_downspouts = snapped_downspouts_dict[basin_id]\n",
    "            point_ids = ['D' + str(i) for i in range(len(basin_downspouts))]\n",
    "            seg_pnts = ','.join(point_ids)\n",
    "            pnt1_ele = basin_downspouts.iloc[0]['elevation']\n",
    "        elif basin_id in basin_points_dict:\n",
    "            matching_points = basin_points_dict[basin_id]\n",
    "            matching_points_sorted = matching_points.sort_values('elev', ascending=True)\n",
    "            point_ids = matching_points_sorted['pnt_id'].tolist()\n",
    "            seg_pnts = ','.join(point_ids)\n",
    "            pnt1_ele = matching_points_sorted.iloc[0]['elev']\n",
    "\n",
    "        basin_info_list.append({\n",
    "            'basin_id': basin_id,\n",
    "            'seg_pnts': seg_pnts,\n",
    "            'pnt1_ele': pnt1_ele\n",
    "        })\n",
    "\n",
    "    basin_info_df = pd.DataFrame(basin_info_list)\n",
    "\n",
    "    if save_outputs:\n",
    "        basin_info_df.to_csv(f\"{BASIN_SEG_PNTS_OUTPUT_PREFIX}_{iteration}.csv\", index=False)\n",
    "\n",
    "    return basin_info_df\n",
    "\n",
    "def create_final_lowest_points(lowest_segment_points, snapped_downspouts, basins, basins_with_inlets_ids, iteration):\n",
    "    print(\"Creating final lowest points\")\n",
    "    lowest_points_list = []\n",
    "    print(\"Processing segment points...\")\n",
    "    for idx, basin in basins.iterrows():\n",
    "        basin_id = basin['basin_id']\n",
    "        if basin_id in basins_with_inlets_ids:\n",
    "            continue\n",
    "        basin_points = lowest_segment_points[\n",
    "            lowest_segment_points['bas_ids'].apply(lambda x: str(basin_id) in str(x).split(','))\n",
    "        ]\n",
    "        if not basin_points.empty:\n",
    "            lowest_point = basin_points.loc[basin_points['elev'].idxmin()]\n",
    "            point_buffer = lowest_point.geometry.buffer(0.01)\n",
    "            intersecting_basins = basins[basins.intersects(point_buffer)]\n",
    "            other_basins = intersecting_basins[intersecting_basins['basin_id'] != basin_id]\n",
    "            to_basin = other_basins.iloc[0]['basin_id'] if not other_basins.empty else -1\n",
    "            lowest_points_list.append({\n",
    "                'basin_id': basin_id,\n",
    "                'geometry': lowest_point.geometry,\n",
    "                'elevation': lowest_point['elev'],\n",
    "                'type': 'segment',\n",
    "                'to_basin': to_basin\n",
    "            })\n",
    "    if snapped_downspouts is not None and not snapped_downspouts.empty:\n",
    "        print(\"Processing downspout points...\")\n",
    "        for _, downspout in snapped_downspouts.iterrows():\n",
    "            buffer_distance = 0.01\n",
    "            downspout_buffer = downspout.geometry.buffer(buffer_distance)\n",
    "            containing_basins = basins[\n",
    "                (basins.intersects(downspout_buffer)) &\n",
    "                (basins['is_hole'] == True)\n",
    "            ]\n",
    "            if containing_basins.empty:\n",
    "                print(f\"Warning: No split hole basin found for downspout at {downspout.geometry}\")\n",
    "                continue\n",
    "            from_basin_id = containing_basins.iloc[0]['basin_id']\n",
    "            draining_basins = basins[\n",
    "                (basins.intersects(downspout_buffer)) &\n",
    "                (~basins['is_hole'].fillna(False))\n",
    "            ]\n",
    "            to_basin = draining_basins.iloc[0]['basin_id'] if not draining_basins.empty else -1\n",
    "            lowest_points_list.append({\n",
    "                'basin_id': from_basin_id,\n",
    "                'geometry': downspout.geometry,\n",
    "                'elevation': downspout['elevation'],\n",
    "                'type': 'downspout',\n",
    "                'to_basin': to_basin\n",
    "            })\n",
    "    lowest_points_gdf = gpd.GeoDataFrame(lowest_points_list, crs=basins.crs)\n",
    "    print(\"\\nVerifying basin assignments...\")\n",
    "    print(f\"Total points: {len(lowest_points_gdf)}\")\n",
    "    print(f\"Points with valid basin_id: {len(lowest_points_gdf[lowest_points_gdf['basin_id'] != -1])}\")\n",
    "    print(f\"Points with valid to_basin: {len(lowest_points_gdf[lowest_points_gdf['to_basin'] != -1])}\")\n",
    "    output_file = f\"{LOWEST_BASIN_POINTS_OUTPUT_PREFIX}_{iteration}.shp\"\n",
    "    lowest_points_gdf.to_file(output_file)\n",
    "    print(f\"\\nResults saved to: {output_file}\")\n",
    "    print(f\"  - Segments: {len(lowest_points_gdf[lowest_points_gdf['type'] == 'segment'])}\")\n",
    "    print(f\"  - Downspouts: {len(lowest_points_gdf[lowest_points_gdf['type'] == 'downspout'])}\")\n",
    "    downspouts = lowest_points_gdf[lowest_points_gdf['type'] == 'downspout']\n",
    "    print(\"\\nDownspout Statistics:\")\n",
    "    print(f\"  - Total downspouts: {len(downspouts)}\")\n",
    "    print(f\"  - Downspouts with valid basin_id: {len(downspouts[downspouts['basin_id'] != -1])}\")\n",
    "    print(f\"  - Downspouts with valid to_basin: {len(downspouts[downspouts['to_basin'] != -1])}\")\n",
    "    return lowest_points_gdf\n",
    "\n",
    "def process_final_sinks(sinks_shapefile, basins, dem_data, dem_transform, lowest_points_gdf, snapped_downspouts, study_area_shapefile):\n",
    "    print(\"Processing final sinks\")\n",
    "    sinks = gpd.read_file(sinks_shapefile)\n",
    "    study_area = gpd.read_file(study_area_shapefile)\n",
    "    sinks = sinks.to_crs(basins.crs)\n",
    "    study_area = study_area.to_crs(basins.crs)\n",
    "    basins = ensure_numeric_basin_id(basins, old_col='basin_id', new_col='basin_id')\n",
    "    basins['basin_area'] = basins.geometry.area\n",
    "    basins['basin_perimeter'] = basins.geometry.length\n",
    "    sinks_with_basins = gpd.sjoin(sinks[['geometry']], basins[['basin_id', 'geometry']], how='left', predicate='within')\n",
    "    sinks_with_basins['basin_id'] = sinks_with_basins['basin_id'].fillna(-1).astype(int)\n",
    "    sinks_with_basins['elev'] = sinks_with_basins['geometry'].apply(lambda point: get_elevation_at_point(point, dem_data, dem_transform))\n",
    "    sinks_with_basins['type'] = 'existing'\n",
    "    sinks_with_basins.to_file(SINKS_PROCESSING_OUTPUT)\n",
    "    print(f\"  - Intermediate sinks exported to {SINKS_PROCESSING_OUTPUT}\")\n",
    "    valid_sinks = sinks_with_basins[sinks_with_basins['basin_id'] != -1]\n",
    "    lowest_elevations = lowest_points_gdf.set_index('basin_id')['elevation'].to_dict()\n",
    "    grouped = valid_sinks.groupby('basin_id')\n",
    "    final_sinks = []\n",
    "\n",
    "    if 'is_hole' not in basins.columns:\n",
    "        basins['is_hole'] = False\n",
    "    if 'is_split' not in basins.columns:\n",
    "        basins['is_split'] = False\n",
    "\n",
    "    for _, basin_row in basins.iterrows():\n",
    "        basin_id = basin_row['basin_id']\n",
    "        basin_geom = basin_row['geometry']\n",
    "        is_hole = basin_row['is_hole']\n",
    "        is_split = basin_row['is_split']\n",
    "\n",
    "        elev, geometry, sink_type = np.nan, None, 'unassigned'\n",
    "\n",
    "        if is_hole and is_split:\n",
    "            geometry = basin_geom.centroid\n",
    "            elev = get_elevation_at_point(geometry, dem_data, dem_transform)\n",
    "            sink_type = 'synthetic'\n",
    "        elif basin_id in grouped.groups:\n",
    "            basin_sinks = grouped.get_group(basin_id)\n",
    "            lowest_sink = basin_sinks.loc[basin_sinks['elev'].idxmin()]\n",
    "            elev = lowest_sink['elev']\n",
    "            geometry = lowest_sink['geometry']\n",
    "            sink_type = 'existing'\n",
    "        else:\n",
    "            basin_downspouts = snapped_downspouts[snapped_downspouts['basin_id'] == basin_id]\n",
    "            if not basin_downspouts.empty:\n",
    "                downspout = basin_downspouts.iloc[0]\n",
    "                geometry = downspout.geometry\n",
    "                elev = downspout['elevation']\n",
    "                sink_type = 'downspout'\n",
    "            else:\n",
    "                geometry = basin_geom.centroid\n",
    "                elev = get_elevation_at_point(geometry, dem_data, dem_transform)\n",
    "                sink_type = 'synthetic'\n",
    "\n",
    "        if geometry is None or geometry.is_empty:\n",
    "            print(f\"Warning: Could not create a valid sink geometry for basin {basin_id}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        deep_elev = elev if not np.isnan(elev) and elev != 0 else np.nan\n",
    "        lowest_elev = lowest_elevations.get(basin_id, np.nan)\n",
    "        dZ = np.round(lowest_elev - elev, 5) if not np.isnan(lowest_elev) and not np.isnan(elev) else np.nan\n",
    "        final_sinks.append({\n",
    "            'basin_id': basin_id,\n",
    "            'elev': np.round(elev, 5) if not np.isnan(elev) else np.nan,\n",
    "            'deep_elev': np.round(deep_elev, 5),\n",
    "            'dZ': dZ,\n",
    "            'type': sink_type,\n",
    "            'basin_area': basin_geom.area,\n",
    "            'basin_perimeter': basin_geom.length,\n",
    "            'geometry': geometry\n",
    "        })\n",
    "    final_sinks_gdf = gpd.GeoDataFrame(final_sinks, crs=basins.crs)\n",
    "    numeric_cols = ['elev', 'deep_elev', 'dZ', 'basin_area', 'basin_perimeter']\n",
    "    for col in numeric_cols:\n",
    "        final_sinks_gdf[col] = final_sinks_gdf[col].astype('float64')\n",
    "    if not study_area.empty:\n",
    "        final_sinks_gdf = gpd.clip(final_sinks_gdf, study_area)\n",
    "    final_sinks_gdf.to_file(UPDATED_SINKS_OUTPUT)\n",
    "    print(f\"  - Final sinks saved to '{UPDATED_SINKS_OUTPUT}'\")\n",
    "    print(f\"  - Total sinks: {len(final_sinks_gdf)}\")\n",
    "    print(f\"    - Existing: {len(final_sinks_gdf[final_sinks_gdf['type'] == 'existing'])}\")\n",
    "    print(f\"    - Downspout: {len(final_sinks_gdf[final_sinks_gdf['type'] == 'downspout'])}\")\n",
    "    print(f\"    - Synthetic: {len(final_sinks_gdf[final_sinks_gdf['type'] == 'synthetic'])}\")\n",
    "    return final_sinks_gdf\n",
    "\n",
    "def validate_geometries(gdf, min_area=0.0001):\n",
    "    print(\"Validating geometries...\")\n",
    "    initial_count = len(gdf)\n",
    "    valid_gdf = gdf.copy()\n",
    "    valid_geometries = []\n",
    "    valid_indices = []\n",
    "    for idx, row in valid_gdf.iterrows():\n",
    "        try:\n",
    "            geom = row.geometry\n",
    "            if geom is None or geom.is_empty:\n",
    "                print(f\"Skipping empty geometry at index {idx}\")\n",
    "                continue\n",
    "            valid_geom = make_valid(geom)\n",
    "            if isinstance(valid_geom, (Polygon, MultiPolygon)):\n",
    "                if valid_geom.area >= min_area:\n",
    "                    valid_geometries.append(valid_geom)\n",
    "                    valid_indices.append(idx)\n",
    "                else:\n",
    "                    print(f\"Skipping too small geometry at index {idx}\")\n",
    "            else:\n",
    "                print(f\"Skipping non-polygon geometry at index {idx}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing geometry at index {idx}: {e}\")\n",
    "            continue\n",
    "    if valid_geometries:\n",
    "        valid_gdf = gdf.loc[valid_indices].copy()\n",
    "        valid_gdf['geometry'] = valid_geometries\n",
    "        valid_gdf['geometry'] = valid_gdf['geometry'].apply(make_valid)\n",
    "        print(f\"Validated geometries: {len(valid_gdf)} out of {initial_count}\")\n",
    "        return valid_gdf\n",
    "    else:\n",
    "        print(\"No valid geometries found!\")\n",
    "        return gpd.GeoDataFrame(geometry=[], crs=gdf.crs)\n",
    "\n",
    "class CheckpointManager:\n",
    "    def __init__(self, checkpoint_dir='checkpoints'):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    def save_checkpoint(self, iteration, data):\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        checkpoint_path = os.path.join(self.checkpoint_dir, f'checkpoint_{iteration}_{timestamp}.pkl')\n",
    "        self._cleanup_old_checkpoints(iteration)\n",
    "        with open(checkpoint_path, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "        print(f\"Saved checkpoint for iteration {iteration}\")\n",
    "\n",
    "    def load_latest_checkpoint(self):\n",
    "        checkpoints = [f for f in os.listdir(self.checkpoint_dir) if f.startswith('checkpoint_')]\n",
    "        if not checkpoints:\n",
    "            return None, -1\n",
    "        latest = max(checkpoints, key=lambda x: os.path.getctime(os.path.join(self.checkpoint_dir, x)))\n",
    "        iteration = int(latest.split('_')[1])\n",
    "        checkpoint_path = os.path.join(self.checkpoint_dir, latest)\n",
    "        with open(checkpoint_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        print(f\"Loaded checkpoint from iteration {iteration}\")\n",
    "        return data, iteration\n",
    "\n",
    "    def _cleanup_old_checkpoints(self, current_iteration, keep_last=5):\n",
    "        checkpoints = [f for f in os.listdir(self.checkpoint_dir)\n",
    "                        if f.startswith(f'checkpoint_{current_iteration}_')]\n",
    "        if len(checkpoints) > keep_last:\n",
    "            checkpoints.sort(key=lambda x: os.path.getctime(os.path.join(self.checkpoint_dir, x)))\n",
    "            for checkpoint in checkpoints[:-keep_last]:\n",
    "                os.remove(os.path.join(self.checkpoint_dir, checkpoint))\n",
    "\n",
    "def main(max_iterations, force_restart):\n",
    "    checkpoint_manager = CheckpointManager()\n",
    "    if not force_restart:\n",
    "        checkpoint_data, start_iteration = checkpoint_manager.load_latest_checkpoint()\n",
    "    else:\n",
    "        checkpoint_data = None\n",
    "        start_iteration = -1\n",
    "        print(\"Forced restart - ignoring existing checkpoints\")\n",
    "\n",
    "    try:\n",
    "        if checkpoint_data is not None:\n",
    "            print(f\"Resuming from iteration {start_iteration}\")\n",
    "            basins = checkpoint_data['basins']\n",
    "            holes_gdf = checkpoint_data['holes_gdf']\n",
    "            basins_with_inlets_ids = checkpoint_data['basins_with_inlets_ids']\n",
    "            iteration = start_iteration + 1\n",
    "            merging_needed = True\n",
    "            last_iteration = start_iteration\n",
    "        else:\n",
    "            print(\"Starting fresh run\")\n",
    "            iteration = 0\n",
    "            merging_needed = True\n",
    "            holes_gdf = None\n",
    "            try:\n",
    "                inlets_gdf = gpd.read_file(INLETS_SHAPEFILE)\n",
    "                print(\"\\n==== Inlets GeoDataFrame Info ====\")\n",
    "                print(\"Columns:\", list(inlets_gdf.columns))\n",
    "                print(\"CRS:\", inlets_gdf.crs)\n",
    "                print(\"Number of features:\", len(inlets_gdf))\n",
    "                if not inlets_gdf.empty:\n",
    "                    inlets_gdf = ensure_numeric_basin_id(inlets_gdf)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error loading inlets: {e}\")\n",
    "                inlets_gdf = gpd.GeoDataFrame(geometry=[], crs=None)\n",
    "            basins = gpd.read_file(BASIN_SHAPEFILE_INITIAL)\n",
    "            print(\"\\n==== Basins GeoDataFrame Info ====\")\n",
    "            print(\"Columns:\", list(basins.columns))\n",
    "            print(\"CRS:\", basins.crs)\n",
    "            print(\"Number of features:\", len(basins))\n",
    "            if basins.empty:\n",
    "                raise ValueError(\"No data found in basins file\")\n",
    "            basins = ensure_numeric_basin_id(basins)\n",
    "            basins_with_inlets_ids = []\n",
    "            if not inlets_gdf.empty:\n",
    "                if inlets_gdf.crs != basins.crs:\n",
    "                    inlets_gdf = inlets_gdf.to_crs(basins.crs)\n",
    "                for predicate in ['contains', 'intersects', 'within']:\n",
    "                    basins_with_inlets = gpd.sjoin(basins, inlets_gdf, how='inner', predicate=predicate)\n",
    "                    if not basins_with_inlets.empty:\n",
    "                        basins_with_inlets_ids = basins_with_inlets['basin_id'].unique().tolist()\n",
    "                        print(f\"\\nFound {len(basins_with_inlets_ids)} basins containing inlets\")\n",
    "                        break\n",
    "\n",
    "        while merging_needed:\n",
    "            print(f\"\\n=== Iteration {iteration} ===\")\n",
    "            if max_iterations >= 0 and iteration >= max_iterations:\n",
    "                print(f\"Reached maximum iterations ({max_iterations})\")\n",
    "                merging_needed = False\n",
    "                last_iteration = iteration\n",
    "                break\n",
    "            save_outputs = (iteration == 0)\n",
    "            boundary_segments, filtered_points_gdf, midpoint_buffers_gdf, holes_gdf, basins = process_basins_and_segments(\n",
    "                basins, DEM_PATH, iteration, holes_gdf, save_outputs\n",
    "            )\n",
    "            checkpoint_data = {\n",
    "                'basins': basins,\n",
    "                'holes_gdf': holes_gdf,\n",
    "                'basins_with_inlets_ids': basins_with_inlets_ids,\n",
    "                'boundary_segments': boundary_segments,\n",
    "                'filtered_points_gdf': filtered_points_gdf,\n",
    "                'iteration': iteration\n",
    "            }\n",
    "            checkpoint_manager.save_checkpoint(iteration, checkpoint_data)\n",
    "            lowest_segment_points = filter_lowest_elevation_points(filtered_points_gdf)\n",
    "            if save_outputs:\n",
    "                lowest_segment_points.to_file(f\"{LOWEST_SEGMENT_POINTS_OUTPUT_PREFIX}_{iteration}.shp\")\n",
    "            basin_info_df = process_basin_info(\n",
    "                basins, lowest_segment_points, None,\n",
    "                basins_with_inlets_ids, iteration, save_outputs\n",
    "            )\n",
    "            basin_info_df_no_inlets = basin_info_df[~basin_info_df['basin_id'].isin(basins_with_inlets_ids)]\n",
    "            basins_no_inlets = basins[~basins['basin_id'].isin(basins_with_inlets_ids)]\n",
    "            basins_to_merge = merge_basins_with_redundant_first_points(basin_info_df_no_inlets)\n",
    "            if basins_to_merge:\n",
    "                merging_needed = True\n",
    "                basins = merge_basins(basins_no_inlets, basins_to_merge, basins_with_inlets_ids, basins)\n",
    "                iteration += 1\n",
    "            else:\n",
    "                merging_needed = False\n",
    "                last_iteration = iteration\n",
    "\n",
    "        print(\"\\n=== Final Iteration (Snapping and Splitting) ===\")\n",
    "        all_polygons = unary_union(basins['geometry'])\n",
    "        holes = []\n",
    "        if isinstance(all_polygons, Polygon):\n",
    "            holes.extend([Polygon(hole) for hole in all_polygons.interiors])\n",
    "        elif isinstance(all_polygons, MultiPolygon):\n",
    "            for polygon in all_polygons.geoms:\n",
    "                holes.extend([Polygon(hole) for hole in polygon.interiors])\n",
    "        max_basin_id = basins['basin_id'].max()\n",
    "        hole_basins_data = []\n",
    "        for hole in holes:\n",
    "            max_basin_id += 1\n",
    "            hole_basins_data.append({\n",
    "                'basin_id': max_basin_id,\n",
    "                'geometry': hole,\n",
    "                'is_hole': True,\n",
    "                'shp_length': hole.length,\n",
    "                'shp_area': hole.area\n",
    "            })\n",
    "        with rasterio.open(DEM_PATH) as dem:\n",
    "            dem_data = dem.read(1)\n",
    "            dem_transform = dem.transform\n",
    "        hole_basins_gdf = gpd.GeoDataFrame(hole_basins_data, crs=basins.crs)\n",
    "        all_basins = pd.concat([basins, hole_basins_gdf], ignore_index=True)\n",
    "        all_basins['is_hole'] = all_basins['is_hole'].fillna(False)\n",
    "        snapped_downspouts = process_downspouts(\n",
    "            DOWNSPOUTS_SHAPEFILE, all_basins, hole_basins_gdf,\n",
    "            dem_data, dem_transform, TEMP_SNAPPED_DOWNSPOUTS_OUTPUT\n",
    "        )\n",
    "        mid_segment_gdf = create_mid_segment_points(all_basins)\n",
    "        split_hole_basins_gdf, snapped_downspouts, max_basin_id = split_hole_basins_and_update_ids(\n",
    "            hole_basins_gdf, snapped_downspouts, mid_segment_gdf, max_basin_id\n",
    "        )\n",
    "        final_basins = all_basins[all_basins['is_hole'] == False]\n",
    "        final_basins = pd.concat([final_basins, split_hole_basins_gdf], ignore_index=True)\n",
    "        final_basins = ensure_numeric_basin_id(final_basins, 'basin_id')\n",
    "        lowest_basin_points_gdf = create_final_lowest_points(\n",
    "            lowest_segment_points,\n",
    "            snapped_downspouts,\n",
    "            final_basins,\n",
    "            basins_with_inlets_ids,\n",
    "            last_iteration\n",
    "        )\n",
    "        final_sinks_gdf = process_final_sinks(\n",
    "            SINKS_SHAPEFILE,\n",
    "            final_basins,\n",
    "            dem_data,\n",
    "            dem_transform,\n",
    "            lowest_basin_points_gdf,\n",
    "            snapped_downspouts,\n",
    "            STUDY_AREA_SHAPEFILE\n",
    "        )\n",
    "        print(\"\\nSaving final outputs...\")\n",
    "        print(\"Processing final basins...\")\n",
    "        final_basins = validate_geometries(final_basins)\n",
    "        if len(final_basins) > 0:\n",
    "            final_basins.to_file(f\"{FINAL_BASINS_OUTPUT_PREFIX}_{last_iteration}.shp\")\n",
    "            print(f\"Saved final basins to {FINAL_BASINS_OUTPUT_PREFIX}_{last_iteration}.shp\")\n",
    "        print(\"\\nSaving snapped downspouts...\")\n",
    "        snapped_downspouts.to_file(f\"{FINAL_SNAPPED_DOWNSPOUTS_OUTPUT_PREFIX}_{last_iteration}.shp\")\n",
    "        print(f\"Saved snapped downspouts to {FINAL_SNAPPED_DOWNSPOUTS_OUTPUT_PREFIX}_{last_iteration}.shp\")\n",
    "        print(\"\\nFinal Statistics:\")\n",
    "        print(f\"Total Basins: {len(final_basins)}\")\n",
    "        print(f\"Split Hole Basins: {len(split_hole_basins_gdf)}\")\n",
    "        print(f\"Snapped Downspouts: {len(snapped_downspouts)}\")\n",
    "        print(f\"Final Sinks: {len(final_sinks_gdf)}\")\n",
    "        print(f\"Total Iterations: {last_iteration + 1}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Critical error in main execution: {e}\")\n",
    "        print(f\"You can resume from iteration {iteration} using the checkpoint\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    max_iterations = -1\n",
    "    force_restart = True\n",
    "    main(max_iterations, force_restart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb99b5cd",
   "metadata": {},
   "source": [
    "# Algorithm 2: Inlet Watershed Delineation via Flow Tracing\n",
    "\n",
    "This section assigns each basin/depression to a **terminal inlet** by tracing surface connectivity. The output is a set of\n",
    "inlet-associated basins (and optional flow edges for visualization).\n",
    "\n",
    "**Inputs:** outputs from Algorithm 1 (final iteration), inlet layer, study area.  \n",
    "**Outputs:** merged basin layer per inlet (and optional flow-edge layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa72a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import LineString\n",
    "from shapely.ops import linemerge, unary_union\n",
    "\n",
    "# Important: Set the final iteration number from the previous script here\n",
    "LAST_ITERATION_NUM = 24\n",
    "\n",
    "# Input files\n",
    "BASINS_SHP = f\"final_basins_{LAST_ITERATION_NUM}.shp\"\n",
    "LOWEST_POINTS_SHP = f\"lowest_basin_points_{LAST_ITERATION_NUM}.shp\"\n",
    "SINKS_SHP = \"updated_sinks.shp\"\n",
    "INLETS_SHP = \"inlet.shp\"\n",
    "\n",
    "# Output files\n",
    "MERGED_BASINS_BY_INLET_SHP = \"basins_merged_by_inlet.shp\"\n",
    "ALL_EDGES_SHP = \"flow_paths_all.shp\"\n",
    "MERGED_EDGES_SHP = \"flow_paths_merged.shp\"\n",
    "\n",
    "# Set up basic logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "def build_flow_dict(lowest_points_gdf):\n",
    "    flow_dict = {}\n",
    "    for _, row in lowest_points_gdf.iterrows():\n",
    "        from_basin = row['basin_id']\n",
    "        to_basin = row.get('to_basin', -1)\n",
    "\n",
    "        if pd.isna(to_basin) or to_basin == -1:\n",
    "            flow_dict[from_basin] = -1\n",
    "        else:\n",
    "            flow_dict[from_basin] = int(to_basin)\n",
    "\n",
    "    return flow_dict\n",
    "\n",
    "\n",
    "def find_final_inlet_basin(basin_id, flow_dict, basins_with_inlets, memo, visited=None):\n",
    "    if basin_id in memo:\n",
    "        return memo[basin_id]\n",
    "\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "\n",
    "    if basin_id in visited:\n",
    "        memo[basin_id] = -1\n",
    "        return -1\n",
    "\n",
    "    visited.add(basin_id)\n",
    "\n",
    "    if basin_id in basins_with_inlets:\n",
    "        memo[basin_id] = basin_id\n",
    "        return basin_id\n",
    "\n",
    "    next_basin = flow_dict.get(basin_id, -1)\n",
    "    if next_basin == -1:\n",
    "        memo[basin_id] = -1\n",
    "        return -1\n",
    "\n",
    "    final_inlet = find_final_inlet_basin(next_basin, flow_dict, basins_with_inlets, memo, visited)\n",
    "    memo[basin_id] = final_inlet\n",
    "    return final_inlet\n",
    "\n",
    "\n",
    "def merge_basins_by_final_inlet(basins_gdf, lowest_points_gdf, inlets_gdf):\n",
    "    logging.info(\"Starting basin merging process based on final inlet.\")\n",
    "\n",
    "    try:\n",
    "        basins_with_inlets_gdf = gpd.sjoin(basins_gdf, inlets_gdf, how=\"inner\", predicate=\"contains\")\n",
    "        basins_with_inlets = set(basins_with_inlets_gdf['basin_id'].unique())\n",
    "        logging.info(f\"Found {len(basins_with_inlets)} basins that directly contain an inlet.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not perform spatial join for inlets. Ensure CRS match. Error: {e}\")\n",
    "        return gpd.GeoDataFrame()\n",
    "\n",
    "    flow_dict = build_flow_dict(lowest_points_gdf)\n",
    "\n",
    "    logging.info(\"Tracing flow paths for all basins...\")\n",
    "    memo = {}\n",
    "    basins_gdf['final_inlet_basin'] = basins_gdf['basin_id'].apply(\n",
    "        lambda bid: find_final_inlet_basin(bid, flow_dict, basins_with_inlets, memo)\n",
    "    )\n",
    "    logging.info(\"Flow tracing complete.\")\n",
    "\n",
    "    logging.info(\"Merging basin geometries based on final inlet.\")\n",
    "    grouped = basins_gdf.groupby('final_inlet_basin')\n",
    "\n",
    "    merged_basins = []\n",
    "    for inlet_id, group in grouped:\n",
    "        if inlet_id == -1:\n",
    "            for _, basin_row in group.iterrows():\n",
    "                merged_basins.append({\n",
    "                    'final_inlet_basin': -1,\n",
    "                    'original_id': basin_row['basin_id'],\n",
    "                    'geometry': basin_row['geometry']\n",
    "                })\n",
    "        else:\n",
    "            merged_geometry = unary_union(group['geometry'])\n",
    "            merged_basins.append({\n",
    "                'final_inlet_basin': inlet_id,\n",
    "                'original_id': None,\n",
    "                'geometry': merged_geometry\n",
    "            })\n",
    "\n",
    "    merged_basins_gdf = gpd.GeoDataFrame(merged_basins, crs=basins_gdf.crs)\n",
    "    logging.info(f\"Basin merging complete. Created {len(merged_basins_gdf)} final catchment areas.\")\n",
    "\n",
    "    return merged_basins_gdf\n",
    "\n",
    "\n",
    "def create_flow_edges(sinks_gdf, lowest_points_gdf, inlets_gdf, basins_gdf):\n",
    "    logging.info(\"Generating visual flow path edges.\")\n",
    "    edges = []\n",
    "    sinks_dict = {row['basin_id']: row.geometry for _, row in sinks_gdf.iterrows()}\n",
    "\n",
    "    for _, point in lowest_points_gdf.iterrows():\n",
    "        from_basin = point['basin_id']\n",
    "        to_basin = point.get('to_basin', -1)\n",
    "\n",
    "        start_geom = sinks_dict.get(from_basin)\n",
    "        if start_geom is None:\n",
    "            continue\n",
    "\n",
    "        edges.append({\n",
    "            'geometry': LineString([start_geom, point.geometry]),\n",
    "            'from_basin': from_basin,\n",
    "            'to_basin': from_basin,\n",
    "            'type': 'sink_to_spill'\n",
    "        })\n",
    "\n",
    "        if to_basin != -1 and pd.notna(to_basin):\n",
    "            to_basin = int(to_basin)\n",
    "            end_geom = sinks_dict.get(to_basin)\n",
    "            if end_geom:\n",
    "                edges.append({\n",
    "                    'geometry': LineString([point.geometry, end_geom]),\n",
    "                    'from_basin': from_basin,\n",
    "                    'to_basin': to_basin,\n",
    "                    'type': 'spill_to_sink'\n",
    "                })\n",
    "\n",
    "    edges_gdf = gpd.GeoDataFrame(edges, crs=sinks_gdf.crs)\n",
    "    logging.info(f\"Generated {len(edges_gdf)} flow path segments.\")\n",
    "    return edges_gdf\n",
    "\n",
    "\n",
    "def main():\n",
    "    # File Validation\n",
    "    input_files = [BASINS_SHP, LOWEST_POINTS_SHP, SINKS_SHP, INLETS_SHP]\n",
    "    for f in input_files:\n",
    "        if not os.path.exists(f):\n",
    "            logging.error(f\"Input file not found: {f}. Please check the path and iteration number.\")\n",
    "            return\n",
    "\n",
    "    # 1. Load Data\n",
    "    logging.info(\"Loading input shapefiles...\")\n",
    "    basins_gdf = gpd.read_file(BASINS_SHP)\n",
    "    lowest_points_gdf = gpd.read_file(LOWEST_POINTS_SHP)\n",
    "    sinks_gdf = gpd.read_file(SINKS_SHP)\n",
    "    inlets_gdf = gpd.read_file(INLETS_SHP)\n",
    "\n",
    "    # 2. Perform Primary Task: Merge Basins by Final Inlet\n",
    "    merged_basins_gdf = merge_basins_by_final_inlet(basins_gdf, lowest_points_gdf, inlets_gdf)\n",
    "\n",
    "    if not merged_basins_gdf.empty:\n",
    "        merged_basins_gdf.to_file(MERGED_BASINS_BY_INLET_SHP)\n",
    "        logging.info(f\"Successfully saved merged basins to {MERGED_BASINS_BY_INLET_SHP}\")\n",
    "\n",
    "    # 3. (Optional) Generate and Save Visual Flow Edges\n",
    "    flow_edges_gdf = create_flow_edges(sinks_gdf, lowest_points_gdf, inlets_gdf, basins_gdf)\n",
    "\n",
    "    if not flow_edges_gdf.empty:\n",
    "        flow_edges_gdf.to_file(ALL_EDGES_SHP)\n",
    "        logging.info(f\"Saved all flow path segments to {ALL_EDGES_SHP}\")\n",
    "\n",
    "        merged_lines = linemerge(unary_union(flow_edges_gdf.geometry))\n",
    "        merged_edges_gdf = gpd.GeoDataFrame(geometry=[merged_lines], crs=flow_edges_gdf.crs)\n",
    "        merged_edges_gdf.to_file(MERGED_EDGES_SHP)\n",
    "        logging.info(f\"Saved merged flow paths to {MERGED_EDGES_SHP}\")\n",
    "\n",
    "    logging.info(\"Processing complete.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a32468",
   "metadata": {},
   "source": [
    "# Watershed characterization plot\n",
    "\n",
    "This section creates summary plots to inspect watershed characteristics (e.g., area/imperviousness distributions and\n",
    "basic quality checks). The plots are intended for **quick diagnostics** before running depth mapping and the dynamic\n",
    "simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ec7f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "from typing import Dict, Any, Optional, Set, List\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker as mticker\n",
    "import numpy as np\n",
    "from scipy.spatial import Delaunay\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "\n",
    "# Logging Configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(module)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# Configuration: Input/Output File Paths and Parameters\n",
    "# --- Input Files (Update iteration number as needed) ---\n",
    "ITERATION_NUMBER = 24 # IMPORTANT: Change this to match your input files.\n",
    "\n",
    "BASINS_SHAPEFILE: str = f\"final_basins_{ITERATION_NUMBER}.shp\"\n",
    "LOWEST_POINTS_SHAPEFILE: str = f\"lowest_basin_points_{ITERATION_NUMBER}.shp\"\n",
    "SINKS_SHAPEFILE: str = \"updated_sinks.shp\"\n",
    "\n",
    "# --- Output File ---\n",
    "OUTPUT_CSV_FILE: str = \"watershed_geomorphology_stats.csv\"\n",
    "OUTPUT_3D_PLOT_FILE: str = \"watershed_3d_stats.png\"\n",
    "\n",
    "# Analysis Functions\n",
    "\n",
    "def build_flow_graph(lowest_points_gdf: gpd.GeoDataFrame) -> nx.DiGraph:\n",
    "    \"\"\"Constructs a directed graph representing basin-to-basin flow.\"\"\"\n",
    "    logging.info(\"Building flow network graph...\")\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add all unique basins as nodes first\n",
    "    all_basins = pd.concat([\n",
    "        lowest_points_gdf['basin_id'],\n",
    "        lowest_points_gdf['to_basin']\n",
    "    ]).dropna().unique()\n",
    "    G.add_nodes_from(all_basins)\n",
    "\n",
    "    # Add directed edges for flow\n",
    "    for _, row in lowest_points_gdf.iterrows():\n",
    "        from_basin = row['basin_id']\n",
    "        to_basin = row['to_basin']\n",
    "        # Only add edges for valid, connected basins (-1 signifies an outlet)\n",
    "        if pd.notna(from_basin) and pd.notna(to_basin) and to_basin != -1:\n",
    "            G.add_edge(from_basin, to_basin)\n",
    "\n",
    "    logging.info(f\"Graph built with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n",
    "    return G\n",
    "\n",
    "def get_strahler_order(graph: nx.DiGraph) -> Dict[Any, int]:\n",
    "    \"\"\"\n",
    "    Calculates the Strahler stream order for each node in the graph.\n",
    "    Note: In network terms, we are looking at predecessors (upstream nodes).\n",
    "    \"\"\"\n",
    "    orders = {}\n",
    "\n",
    "    # Find terminal nodes (headwaters), which have an in-degree of 0\n",
    "    terminals = [node for node, in_degree in graph.in_degree() if in_degree == 0]\n",
    "    for node in terminals:\n",
    "        orders[node] = 1\n",
    "\n",
    "    # Iteratively process nodes\n",
    "    nodes_to_process = list(graph.nodes())\n",
    "\n",
    "    while len(orders) < len(nodes_to_process):\n",
    "        num_ordered_before = len(orders)\n",
    "\n",
    "        for node in nodes_to_process:\n",
    "            if node in orders:\n",
    "                continue\n",
    "\n",
    "            predecessors = list(graph.predecessors(node))\n",
    "            if not predecessors: # Should be caught by terminal check, but as a safeguard\n",
    "                orders[node] = 1\n",
    "                continue\n",
    "\n",
    "            # Check if all predecessors have an order assigned\n",
    "            if all(p in orders for p in predecessors):\n",
    "                pred_orders = [orders[p] for p in predecessors]\n",
    "                max_order = max(pred_orders)\n",
    "\n",
    "                # If there are two or more predecessors with the max order, increment.\n",
    "                if pred_orders.count(max_order) >= 2:\n",
    "                    orders[node] = max_order + 1\n",
    "                else:\n",
    "                    orders[node] = max_order\n",
    "\n",
    "        # If no new nodes were ordered in a full pass, there might be a cycle or\n",
    "        # disconnected component\n",
    "        if len(orders) == num_ordered_before:\n",
    "            logging.warning(\"Stalled ordering process. Remaining nodes may be in cycles or disconnected.\")\n",
    "            # Assign a default order of -1 to remaining nodes\n",
    "            for node in nodes_to_process:\n",
    "                if node not in orders:\n",
    "                    orders[node] = -1\n",
    "            break\n",
    "\n",
    "    return orders\n",
    "\n",
    "def calculate_tree_width(graph: nx.DiGraph, root: Any) -> int:\n",
    "    \"\"\"\n",
    "    Calculates the width of a tree (max nodes at any depth level) using BFS.\n",
    "    The graph should be reversed (pointing from downstream to upstream).\n",
    "    \"\"\"\n",
    "    if root not in graph:\n",
    "        return 0\n",
    "\n",
    "    level_counts = {}\n",
    "    queue = [(root, 0)] # (node, level)\n",
    "    visited = {root}\n",
    "\n",
    "    while queue:\n",
    "        current_node, level = queue.pop(0)\n",
    "        level_counts[level] = level_counts.get(level, 0) + 1\n",
    "\n",
    "        for neighbor in graph.neighbors(current_node):\n",
    "            if neighbor not in visited:\n",
    "                visited.add(neighbor)\n",
    "                queue.append((neighbor, level + 1))\n",
    "\n",
    "    return max(level_counts.values()) if level_counts else 0\n",
    "\n",
    "\n",
    "def analyze_watersheds(basins_gdf: gpd.GeoDataFrame, lowest_points_gdf: gpd.GeoDataFrame, sinks_gdf: gpd.GeoDataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Performs a full geomorphological analysis of all watersheds.\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting full watershed analysis...\")\n",
    "\n",
    "    # Build the main flow graph\n",
    "    flow_graph = build_flow_graph(lowest_points_gdf)\n",
    "\n",
    "    # Identify all distinct watershed trees (weakly connected components)\n",
    "    watershed_subgraphs = [\n",
    "        flow_graph.subgraph(c) for c in nx.weakly_connected_components(flow_graph)\n",
    "    ]\n",
    "    logging.info(f\"Identified {len(watershed_subgraphs)} distinct watershed networks.\")\n",
    "\n",
    "    # Calculate Strahler order for the entire graph at once\n",
    "    strahler_orders = get_strahler_order(flow_graph)\n",
    "\n",
    "    # Prepare data for flow path length calculation\n",
    "    sinks_geom_dict = {row['basin_id']: row.geometry for _, row in sinks_gdf.iterrows()}\n",
    "    lowest_points_dict = {row['basin_id']: row.geometry for _, row in lowest_points_gdf.iterrows()}\n",
    "\n",
    "    analysis_results = []\n",
    "\n",
    "    for i, sub_graph in enumerate(watershed_subgraphs):\n",
    "        if sub_graph.number_of_nodes() == 0:\n",
    "            continue\n",
    "\n",
    "        # Find the outlet node for this watershed (node with out-degree of 0 in the\n",
    "        # subgraph)\n",
    "        outlet_node = None\n",
    "        for node in sub_graph.nodes():\n",
    "            if sub_graph.out_degree(node) == 0:\n",
    "                outlet_node = node\n",
    "                break\n",
    "\n",
    "        if outlet_node is None:\n",
    "            logging.warning(f\"Could not determine a single outlet for watershed {i}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # --- 1. Basic Metrics ---\n",
    "        node_count = sub_graph.number_of_nodes()\n",
    "        basin_ids_in_watershed = list(sub_graph.nodes())\n",
    "        total_area = basins_gdf[basins_gdf['basin_id'].isin(basin_ids_in_watershed)]['area'].sum()\n",
    "\n",
    "        # --- 2. Topological Metrics ---\n",
    "        # Reverse the graph to trace from outlet upstream\n",
    "        reversed_sub_graph = sub_graph.reverse(copy=True)\n",
    "        tree_depth = nx.dag_longest_path_length(reversed_sub_graph)\n",
    "        tree_width = calculate_tree_width(reversed_sub_graph, outlet_node)\n",
    "\n",
    "        # Get the Strahler order of the outlet node\n",
    "        watershed_strahler_order = strahler_orders.get(outlet_node, -1)\n",
    "\n",
    "        # --- 3. Geometric / Hydraulic Metrics ---\n",
    "        # This calculates the cumulative length of the straight-line edges connecting\n",
    "        # the sink and outlet points of the sub-basins.\n",
    "        cumulative_flow_path_length = 0.0\n",
    "        for u, v in sub_graph.edges():\n",
    "            from_sink = sinks_geom_dict.get(u)\n",
    "            from_lp = lowest_points_dict.get(u)\n",
    "            to_sink = sinks_geom_dict.get(v)\n",
    "\n",
    "            if from_sink and from_lp:\n",
    "                cumulative_flow_path_length += from_sink.distance(from_lp)\n",
    "            if from_lp and to_sink:\n",
    "                cumulative_flow_path_length += from_lp.distance(to_sink)\n",
    "\n",
    "        drainage_density = (cumulative_flow_path_length / total_area) if total_area > 0 else 0\n",
    "\n",
    "        analysis_results.append({\n",
    "            'outlet_id': outlet_node,\n",
    "            'node_count': node_count,\n",
    "            'total_area_sqm': total_area,\n",
    "            'tree_depth': tree_depth,\n",
    "            'tree_width': tree_width,\n",
    "            'strahler_order': watershed_strahler_order,\n",
    "            'flow_path_length_m': cumulative_flow_path_length,\n",
    "            'drainage_density': drainage_density\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(analysis_results)\n",
    "\n",
    "def plot_3d_watershed_stats(df: pd.DataFrame, coeffs: np.ndarray):\n",
    "    \"\"\"Creates and saves a 3D scatter plot with a best-fit surface and drop lines.\"\"\"\n",
    "    logging.info(\"Generating 3D watershed statistics plot...\")\n",
    "    if df.empty or len(df) < 6: # Need enough points for a 2nd degree polynomial\n",
    "        logging.warning(\"DataFrame has fewer than 6 points, skipping 3D plot.\")\n",
    "        return\n",
    "\n",
    "    plt.rcParams['font.family'] = 'Arial'\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 15))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 1.0))\n",
    "    ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 1.0))\n",
    "    ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 1.0))\n",
    "\n",
    "    ax.xaxis.line.set_color(\"black\")\n",
    "    ax.yaxis.line.set_color(\"black\")\n",
    "    ax.zaxis.line.set_color(\"black\")\n",
    "    ax.xaxis.line.set_linewidth(3)\n",
    "    ax.yaxis.line.set_linewidth(3)\n",
    "    ax.zaxis.line.set_linewidth(3)\n",
    "\n",
    "    ax.grid(True, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "    x_data = df['node_count']\n",
    "    y_data = df['tree_depth']\n",
    "    z_data = df['tree_width']\n",
    "\n",
    "    # --- Discrete Color Bar ---\n",
    "    order_values = sorted(df['strahler_order'].unique())\n",
    "    cmap = plt.get_cmap('viridis', len(order_values))\n",
    "    norm = BoundaryNorm(np.arange(min(order_values)-0.5, max(order_values)+1.5), cmap.N)\n",
    "\n",
    "    sc = ax.scatter(\n",
    "        x_data, y_data, z_data,\n",
    "        c=df['strahler_order'],\n",
    "        cmap=cmap, norm=norm,\n",
    "        s=80, alpha=0.9, edgecolor='black', linewidth=0.7\n",
    "    )\n",
    "\n",
    "    # --- Set Axis Limits ---\n",
    "    ax.set_xlim(160, 0) # Reversed as requested\n",
    "    ax.set_ylim(0, 30)\n",
    "    ax.set_zlim(0, 25)\n",
    "\n",
    "    # --- Drop Lines and Wall Projections ---\n",
    "    xlims = ax.get_xlim()\n",
    "    ylims = ax.get_ylim()\n",
    "    zlims = ax.get_zlim()\n",
    "\n",
    "    # Generate colors for each point directly from the colormap and norm\n",
    "    point_colors = cmap(norm(df['strahler_order']))\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        # Vertical drop line to the floor\n",
    "        ax.plot([x_data.iloc[i], x_data.iloc[i]], [y_data.iloc[i], y_data.iloc[i]], [z_data.iloc[i], zlims[0]],\n",
    "                 c='black', linestyle='--', linewidth=0.8, alpha=0.6)\n",
    "\n",
    "        # Projection dots on the walls with corresponding color\n",
    "        ax.scatter(xlims[0], y_data.iloc[i], z_data.iloc[i], c=[point_colors[i]], s=20, alpha=0.7) # YZ plane\n",
    "        ax.scatter(x_data.iloc[i], ylims[1], z_data.iloc[i], c=[point_colors[i]], s=20, alpha=0.7) # XZ plane\n",
    "        ax.scatter(x_data.iloc[i], y_data.iloc[i], zlims[0], c=[point_colors[i]], s=20, alpha=0.7) # XY plane (floor)\n",
    "\n",
    "\n",
    "    # --- Polynomial Best-Fit Surface that conforms to the data's shape ---\n",
    "    x_surf = np.linspace(0, 160, 50)\n",
    "    y_surf = np.linspace(0, 30, 50)\n",
    "    X_surf, Y_surf = np.meshgrid(x_surf, y_surf)\n",
    "\n",
    "    Z_surf = (coeffs[0] * X_surf**2 + coeffs[1] * Y_surf**2 + coeffs[2] * X_surf * Y_surf +\n",
    "              coeffs[3] * X_surf + coeffs[4] * Y_surf + coeffs[5])\n",
    "\n",
    "    points = np.c_[x_data, y_data]\n",
    "    tri = Delaunay(points)\n",
    "    p = np.c_[X_surf.ravel(), Y_surf.ravel()]\n",
    "    mask = tri.find_simplex(p) < 0\n",
    "    Z_surf.ravel()[mask] = np.nan\n",
    "\n",
    "    ax.plot_surface(X_surf, Y_surf, Z_surf, cmap='coolwarm', alpha=0.3, edgecolor='none')\n",
    "    ax.plot_wireframe(X_surf, Y_surf, Z_surf, color='black', linewidth=0.7, alpha=0.6)\n",
    "\n",
    "    # --- Axes Labels and Title ---\n",
    "    ax.set_xlabel('Number of Nodes', fontsize=28, labelpad=40)\n",
    "    ax.set_ylabel('Tree Depth (Longest Path)', fontsize=28, labelpad=40)\n",
    "    ax.set_zlabel('Tree Width (Max Nodes at Level)', fontsize=28, labelpad=40)\n",
    "    ax.set_title('Watershed Network Characteristics', fontsize=32, pad=20)\n",
    "\n",
    "    # --- Tick Font Size and Interval---\n",
    "    ax.tick_params(axis='x', labelsize=22)\n",
    "    ax.tick_params(axis='y', labelsize=22)\n",
    "    ax.tick_params(axis='z', labelsize=22)\n",
    "    ax.zaxis.set_major_locator(mticker.MultipleLocator(5))\n",
    "\n",
    "    # --- Color Bar ---\n",
    "    cbar = plt.colorbar(sc, ticks=order_values, pad=0.1)\n",
    "    cbar.set_label('Strahler Order', size=24, labelpad=20)\n",
    "    cbar.ax.tick_params(labelsize=20)\n",
    "\n",
    "    # --- Draw remaining box edges ---\n",
    "    ax.plot([xlims[0], xlims[1]], [ylims[0], ylims[0]], [zlims[0], zlims[0]], c='k', linewidth=3)\n",
    "    ax.plot([xlims[0], xlims[1]], [ylims[1], ylims[1]], [zlims[0], zlims[0]], c='k', linewidth=3)\n",
    "    ax.plot([xlims[0], xlims[0]], [ylims[0], ylims[1]], [zlims[0], zlims[0]], c='k', linewidth=3)\n",
    "    ax.plot([xlims[1], xlims[1]], [ylims[0], ylims[1]], [zlims[0], zlims[0]], c='k', linewidth=3)\n",
    "    ax.plot([xlims[0], xlims[0]], [ylims[0], ylims[0]], [zlims[0], zlims[1]], c='k', linewidth=3)\n",
    "    ax.plot([xlims[0], xlims[1]], [ylims[1], ylims[1]], [zlims[1], zlims[1]], c='k', linewidth=3)\n",
    "    ax.plot([xlims[0], xlims[0]], [ylims[0], ylims[1]], [zlims[1], zlims[1]], c='k', linewidth=3)\n",
    "\n",
    "\n",
    "    try:\n",
    "        plt.savefig(OUTPUT_3D_PLOT_FILE, dpi=450, bbox_inches='tight')\n",
    "        logging.info(f\"3D plot saved to {OUTPUT_3D_PLOT_FILE}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save 3D plot: {e}\")\n",
    "    plt.show()\n",
    "\n",
    "# Main Execution Block\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to load data, run the analysis, and print/save results.\n",
    "    \"\"\"\n",
    "    logging.info(\"--- Starting Watershed Geomorphology Analysis ---\")\n",
    "\n",
    "    # --- File Existence Check ---\n",
    "    required_files = {\n",
    "        \"Basins\": BASINS_SHAPEFILE,\n",
    "        \"Lowest Points\": LOWEST_POINTS_SHAPEFILE,\n",
    "        \"Sinks\": SINKS_SHAPEFILE\n",
    "    }\n",
    "\n",
    "    all_files_exist = True\n",
    "    for name, path in required_files.items():\n",
    "        if not os.path.exists(path):\n",
    "            logging.critical(f\"FATAL: Input file for {name} not found at '{path}'. Exiting.\")\n",
    "            all_files_exist = False\n",
    "    if not all_files_exist:\n",
    "        return\n",
    "\n",
    "    # --- Load Data ---\n",
    "    try:\n",
    "        basins_gdf = gpd.read_file(BASINS_SHAPEFILE)\n",
    "        lowest_points_gdf = gpd.read_file(LOWEST_POINTS_SHAPEFILE)\n",
    "        sinks_gdf = gpd.read_file(SINKS_SHAPEFILE)\n",
    "        logging.info(\"All input shapefiles loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"Fatal error loading input shapefiles: {e}. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # --- Run Analysis ---\n",
    "    results_df = analyze_watersheds(basins_gdf, lowest_points_gdf, sinks_gdf)\n",
    "\n",
    "    # --- Display and Save Results ---\n",
    "    if not results_df.empty:\n",
    "        logging.info(\"Analysis complete. Displaying results...\")\n",
    "        # Set pandas display options for better console output\n",
    "        pd.set_option('display.max_rows', 500)\n",
    "        pd.set_option('display.width', 1000)\n",
    "\n",
    "        # Rename the outlet ID column for clarity\n",
    "        results_df = results_df.rename(columns={'watershed_id': 'outlet_id'})\n",
    "\n",
    "        print(\"\\n--- Raw Watershed Geomorphology Statistics ---\")\n",
    "        print(results_df.to_string())\n",
    "\n",
    "        # Calculate and print summary statistics, excluding the ID column\n",
    "        print(\"\\n\\n--- Summary Statistics of All Watersheds ---\")\n",
    "        stats_df = results_df.drop(columns=['outlet_id'])\n",
    "        summary_stats = stats_df.describe().loc[['mean', '50%', 'std', 'min', 'max']]\n",
    "        summary_stats = summary_stats.rename(index={'50%': 'median'})\n",
    "        print(summary_stats.to_string())\n",
    "\n",
    "        # --- Fit model and print R-squared ---\n",
    "        coeffs = None\n",
    "        if len(results_df) >= 6:\n",
    "            x_data = results_df['node_count']\n",
    "            y_data = results_df['tree_depth']\n",
    "            z_data = results_df['tree_width']\n",
    "            # Create the design matrix for a 2nd degree polynomial: z = c0*x^2 + c1*y^2\n",
    "            # + c2*xy + c3*x + c4*y + c5\n",
    "            A = np.c_[x_data**2, y_data**2, x_data*y_data, x_data, y_data, np.ones(len(results_df))]\n",
    "            coeffs, _, _, _ = np.linalg.lstsq(A, z_data, rcond=None)\n",
    "\n",
    "            z_pred = A @ coeffs\n",
    "            ss_res = np.sum((z_data - z_pred)**2)\n",
    "            ss_tot = np.sum((z_data - np.mean(z_data))**2)\n",
    "            r2 = 1 - (ss_res / ss_tot)\n",
    "            print(f\"\\n--- Goodness of Fit for 3D Surface ---\\nR-squared (R²): {r2:.4f}\")\n",
    "\n",
    "            # Print the plane equation\n",
    "            print(\"\\n--- Best-Fit Plane Equation ---\")\n",
    "            print(f\"Tree Width = ({coeffs[0]:.4f} * NodeCount²) + ({coeffs[1]:.4f} * TreeDepth²) + ({coeffs[2]:.4f} * NodeCount * TreeDepth) + \"\n",
    "                  f\"({coeffs[3]:.4f} * NodeCount) + ({coeffs[4]:.4f} * TreeDepth) + ({coeffs[5]:.4f})\")\n",
    "\n",
    "        else:\n",
    "            logging.warning(\"Not enough data points to calculate a reliable best-fit surface.\")\n",
    "\n",
    "        # Export to CSV\n",
    "        try:\n",
    "            results_df.to_csv(OUTPUT_CSV_FILE, index=False)\n",
    "            logging.info(f\"Results successfully exported to {OUTPUT_CSV_FILE}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to export results to CSV: {e}\")\n",
    "\n",
    "        # Generate the 3D plot if a model was successfully fitted\n",
    "        if coeffs is not None:\n",
    "            plot_3d_watershed_stats(results_df, coeffs)\n",
    "\n",
    "    else:\n",
    "        logging.warning(\"Analysis did not produce any results.\")\n",
    "\n",
    "    logging.info(\"--- Script finished. ---\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3340fc",
   "metadata": {},
   "source": [
    "# Algorithm 3: Water Depth Mapping via Depression Filling\n",
    "\n",
    "This section computes spatial water-depth patterns by filling depressions to specified stages/levels.\n",
    "\n",
    "**Inputs:** depression polygons and elevation attributes/rasters.  \n",
    "**Outputs:** depth maps (and optional figures) that can be used for interpretation and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05624f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.features import rasterize\n",
    "\n",
    "# Configuration Block\n",
    "# All user-configurable settings are centralized here.\n",
    "\n",
    "# Set up basic logging to provide progress and error information.\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(module)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# File Paths and Parameters\n",
    "# Important: These paths should point to the outputs of the previous scripts.\n",
    "\n",
    "# The final iteration number from the iterative merging script.\n",
    "LAST_ITERATION_NUM: int = 24\n",
    "\n",
    "CONFIG = {\n",
    "    \"inputs\": {\n",
    "        \"dem\": \"DEM.tif\",\n",
    "        # Basin file from the previous iterative merging script.\n",
    "        \"basins\": f\"final_basins_{LAST_ITERATION_NUM}.shp\",\n",
    "        # Sinks file from the previous script, containing 'dZ' values.\n",
    "        \"sinks\": \"updated_sinks.shp\"\n",
    "    },\n",
    "    \"outputs\": {\n",
    "        \"output_dir\": \"water_depth_analysis\",\n",
    "        \"depth_raster\": \"water_depth_to_spill_point.tif\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        # The column in the basins shapefile that contains the unique basin ID.\n",
    "        \"basin_id_column\": \"basin_id\",\n",
    "        # Use `all_touched=True` to ensure that pixels touched by a polygon's\n",
    "        # edge are included in the rasterized mask.\n",
    "        \"rasterize_all_touched\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Core Hydrological Filling Functions\n",
    "\n",
    "def fill_single_basin_to_spill_elevation(\n",
    "    dem_array: np.ndarray,\n",
    "    basin_mask: np.ndarray,\n",
    "    spill_elevation_value: float\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates water depth within a single basin up to a given spill elevation.\n",
    "\n",
    "    Args:\n",
    "        dem_array: The full Digital Elevation Model as a NumPy array.\n",
    "        basin_mask: A boolean NumPy array where `True` indicates the pixels\n",
    "                    belonging to the current basin.\n",
    "        spill_elevation_value: The elevation to which the basin will be \"filled\".\n",
    "\n",
    "    Returns:\n",
    "        A NumPy array of the same shape as the DEM, containing the calculated\n",
    "        water depths for the specified basin.\n",
    "    \"\"\"\n",
    "    if not basin_mask.any():\n",
    "        return np.zeros_like(dem_array, dtype=np.float32)\n",
    "\n",
    "    # Create an empty array to hold the results for this basin.\n",
    "    basin_specific_water_depth = np.zeros_like(dem_array, dtype=np.float32)\n",
    "\n",
    "    # Get the DEM elevations only for the pixels within the current basin.\n",
    "    basin_dem_elevations = dem_array[basin_mask]\n",
    "\n",
    "    # Calculate water depth: spill elevation - ground elevation.\n",
    "    # np.maximum ensures that depth is never negative.\n",
    "    calculated_depths = np.maximum(0, spill_elevation_value - basin_dem_elevations)\n",
    "\n",
    "    # Place the calculated depths back into the full-sized array at the correct\n",
    "    # location.\n",
    "    basin_specific_water_depth[basin_mask] = calculated_depths\n",
    "    return basin_specific_water_depth\n",
    "\n",
    "\n",
    "def calculate_basin_water_depth_at_spill_points(config: Dict) -> None:\n",
    "    \"\"\"\n",
    "    Calculates and saves a raster representing water depth in basins.\n",
    "\n",
    "    This function orchestrates the entire process, from loading data to\n",
    "    calculating depths and saving the final output raster.\n",
    "    \"\"\"\n",
    "    dem_filepath = config['inputs']['dem']\n",
    "    basins_filepath = config['inputs']['basins']\n",
    "    sinks_filepath = config['inputs']['sinks']\n",
    "    output_dir = config['outputs']['output_dir']\n",
    "\n",
    "    logging.info(f\"Starting basin filling. DEM: '{dem_filepath}', Basins: '{basins_filepath}'\")\n",
    "\n",
    "    # --- 1. Load and Prepare Data ---\n",
    "    try:\n",
    "        with rasterio.open(dem_filepath) as dem_src:\n",
    "            dem_array = dem_src.read(1)\n",
    "            dem_profile = dem_src.profile\n",
    "            dem_transform = dem_src.transform\n",
    "            dem_crs = dem_src.crs\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"Failed to read DEM: '{dem_filepath}'. Error: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        basins_gdf = gpd.read_file(basins_filepath)\n",
    "        if basins_gdf.crs != dem_crs:\n",
    "            basins_gdf = basins_gdf.to_crs(dem_crs)\n",
    "        sinks_gdf = gpd.read_file(sinks_filepath)\n",
    "        if sinks_gdf.crs != dem_crs:\n",
    "            sinks_gdf = sinks_gdf.to_crs(dem_crs)\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"Failed to load/reproject shapefiles. Error: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Validate Input Data ---\n",
    "    basin_id_col = config['parameters']['basin_id_column']\n",
    "    if basin_id_col not in basins_gdf.columns:\n",
    "        logging.error(f\"Critical: Column '{basin_id_col}' not found in basins file '{basins_filepath}'.\")\n",
    "        return\n",
    "    required_sink_cols = ['basin_id', 'dZ', 'elev', 'geometry']\n",
    "    if not all(col in sinks_gdf.columns for col in required_sink_cols):\n",
    "        logging.error(f\"Critical: Sinks file must contain {required_sink_cols}.\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Rasterize Basins ---\n",
    "    logging.info(f\"Rasterizing basins using ID column: '{basin_id_col}'...\")\n",
    "    try:\n",
    "        # Ensure IDs are integers for consistent matching and rasterization values.\n",
    "        basins_gdf[basin_id_col] = pd.to_numeric(basins_gdf[basin_id_col], errors='coerce').dropna().astype(int)\n",
    "        sinks_gdf['basin_id'] = pd.to_numeric(sinks_gdf['basin_id'], errors='coerce').dropna().astype(int)\n",
    "\n",
    "        shapes_for_rasterize = [\n",
    "            (geom, int(b_id)) for geom, b_id in zip(basins_gdf.geometry, basins_gdf[basin_id_col])\n",
    "            if geom and pd.notna(b_id)\n",
    "        ]\n",
    "        if not shapes_for_rasterize:\n",
    "            logging.error(\"No valid basin geometries/IDs to rasterize.\")\n",
    "            return\n",
    "\n",
    "        basin_id_raster = rasterize(\n",
    "            shapes=shapes_for_rasterize,\n",
    "            out_shape=dem_array.shape,\n",
    "            transform=dem_transform,\n",
    "            dtype=rasterio.int32,\n",
    "            all_touched=config['parameters']['rasterize_all_touched'],\n",
    "            fill=0\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"Failed during data type conversion or rasterization: {e}\")\n",
    "        return\n",
    "    logging.info(\"Basin rasterization complete.\")\n",
    "\n",
    "    # --- 4. Prepare Spill Elevation Map ---\n",
    "    logging.info(\"Calculating spill elevations for each basin...\")\n",
    "    # The spill elevation is the sink's elevation plus the spill depth (dZ).\n",
    "    sinks_gdf['spill_elev'] = sinks_gdf['elev'] + sinks_gdf['dZ']\n",
    "\n",
    "    # Create a primary map from basin_id to its calculated spill elevation.\n",
    "    basin_spill_elevation_map = sinks_gdf.dropna(\n",
    "        subset=['spill_elev', 'basin_id']\n",
    "    ).set_index('basin_id')['spill_elev'].to_dict()\n",
    "\n",
    "    # Create a fallback map using just the sink elevation, in case dZ was NaN.\n",
    "    sink_elevations_direct_map = sinks_gdf.dropna(\n",
    "        subset=['elev', 'basin_id']\n",
    "    ).set_index('basin_id')['elev'].to_dict()\n",
    "\n",
    "    logging.info(f\"Spill elevation map created for {len(basin_spill_elevation_map)} basins.\")\n",
    "\n",
    "    # --- 5. Fill Basins and Calculate Depth ---\n",
    "    logging.info(\"Iterating through basins to calculate water depth...\")\n",
    "    final_cumulative_water_depth = np.zeros_like(dem_array, dtype=np.float32)\n",
    "    unique_raster_ids = np.unique(basin_id_raster[basin_id_raster != 0])\n",
    "\n",
    "    for basin_id in unique_raster_ids:\n",
    "        current_mask = (basin_id_raster == basin_id)\n",
    "\n",
    "        # Determine the spill elevation for the current basin.\n",
    "        spill_elev = basin_spill_elevation_map.get(basin_id)\n",
    "\n",
    "        # If primary spill elevation is missing (e.g., dZ was NaN), use fallback.\n",
    "        if spill_elev is None or pd.isna(spill_elev):\n",
    "            spill_elev = sink_elevations_direct_map.get(basin_id)\n",
    "            if spill_elev is not None:\n",
    "                logging.info(f\"Basin {basin_id}: Using fallback sink elevation {spill_elev:.2f} as spill level.\")\n",
    "            else:\n",
    "                logging.warning(f\"No spill elevation or fallback found for basin {basin_id}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "        if pd.isna(spill_elev):\n",
    "            logging.warning(f\"Spill elevation for basin {basin_id} is NaN. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Calculate depth for this single basin.\n",
    "        depth_this_basin = fill_single_basin_to_spill_elevation(dem_array, current_mask, spill_elev)\n",
    "\n",
    "        # Add the results to the cumulative depth raster.\n",
    "        final_cumulative_water_depth = np.maximum(final_cumulative_water_depth, depth_this_basin)\n",
    "\n",
    "    # --- 6. Save Final Output Raster ---\n",
    "    logging.info(\"Saving final water depth raster...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_raster_path = os.path.join(output_dir, config['outputs']['depth_raster'])\n",
    "\n",
    "    output_profile = dem_profile.copy()\n",
    "    output_profile.update(dtype=rasterio.float32, compress='lzw', nodata=0.0)\n",
    "\n",
    "    try:\n",
    "        with rasterio.open(output_raster_path, 'w', **output_profile) as dst:\n",
    "            dst.write(final_cumulative_water_depth.astype(rasterio.float32), 1)\n",
    "        logging.info(f\"Final water depth raster saved to: {output_raster_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save output raster: {e}\")\n",
    "\n",
    "    logging.info(\"Basin filling process completed.\")\n",
    "\n",
    "\n",
    "# Script Execution\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"Script started directly for basin water depth calculation.\")\n",
    "\n",
    "    # --- File Validation ---\n",
    "    all_files_ok = True\n",
    "    for key, path in CONFIG['inputs'].items():\n",
    "        if not os.path.exists(path):\n",
    "            logging.critical(f\"Input file for '{key}' not found at: '{path}'\")\n",
    "            all_files_ok = False\n",
    "\n",
    "    if all_files_ok:\n",
    "        logging.info(\"All input files found. Starting process...\")\n",
    "        calculate_basin_water_depth_at_spill_points(CONFIG)\n",
    "    else:\n",
    "        logging.error(\"One or more input files are missing. Processing aborted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78222707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import logging\n",
    "import re\n",
    "from typing import Tuple, Optional, List, Any\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "\n",
    "# Logging Configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(module)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# Configuration: Input/Output File Paths and Parameters\n",
    "\n",
    "# The final iteration number.\n",
    "LAST_ITERATION_NUM: int = 24\n",
    "\n",
    "CONFIG = {\n",
    "    \"inputs\": {\n",
    "        \"dem\": \"DEM.tif\",\n",
    "        \"basins\": f\"final_basins_{LAST_ITERATION_NUM}.shp\",\n",
    "        \"sinks\": \"updated_sinks.shp\"\n",
    "    },\n",
    "    \"outputs\": {\n",
    "        \"output_dir\": \"water_depth_analysis\",\n",
    "        \"depth_raster\": \"water_depth_to_spill_point.tif\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"basin_id_column\": \"basin_id\",\n",
    "        \"rasterize_all_touched\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Input File Paths\n",
    "BASIN_SHAPEFILE_DIRECTORY: str = \"\"\n",
    "BASIN_SHAPEFILE_PREFIX: str = \"final_basins_\"\n",
    "BASIN_ID_COLUMN: str = \"basin_id\"\n",
    "WATER_DEPTH_RASTER_PATH: str = os.path.join(CONFIG['outputs']['output_dir'], CONFIG['outputs']['depth_raster'])\n",
    "\n",
    "# Output File Names\n",
    "OUTPUT_BASINS_WITH_STATS_SHP: str = \"basins_with_water_stats.shp\"\n",
    "OUTPUT_BASINS_STATS_CSV: str = \"basins_water_stats.csv\"\n",
    "\n",
    "# Processing Parameters\n",
    "TARGET_CRS: Optional[str] = None\n",
    "\n",
    "\n",
    "# Helper Functions\n",
    "\n",
    "def find_latest_iteration_shp_file(\n",
    "    directory: str,\n",
    "    prefix: str,\n",
    "    extension: str = \".shp\"\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Finds the shapefile with the highest iteration number in its name.\n",
    "\n",
    "    Args:\n",
    "        directory: The directory to search within.\n",
    "        prefix: The common prefix of the filenames (e.g., \"final_basins_\").\n",
    "        extension: The file extension (e.g., \".shp\").\n",
    "\n",
    "    Returns:\n",
    "        The full path to the file with the highest iteration number, or None.\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(directory, f\"{prefix}*{extension}\")\n",
    "    files = glob.glob(pattern)\n",
    "\n",
    "    if not files:\n",
    "        logging.warning(f\"No files found matching pattern: {pattern}\")\n",
    "        return None\n",
    "\n",
    "    latest_file_path = None\n",
    "    max_iteration = -1\n",
    "\n",
    "    filename_pattern_regex = re.compile(rf\"{re.escape(prefix)}(\\d+){re.escape(extension)}\")\n",
    "\n",
    "    for f_path in files:\n",
    "        filename_only = os.path.basename(f_path)\n",
    "        match = filename_pattern_regex.match(filename_only)\n",
    "        if match:\n",
    "            try:\n",
    "                iteration_num = int(match.group(1))\n",
    "                if iteration_num > max_iteration:\n",
    "                    max_iteration = iteration_num\n",
    "                    latest_file_path = f_path\n",
    "            except ValueError:\n",
    "                logging.warning(f\"Could not parse iteration number from: {filename_only}\")\n",
    "                continue\n",
    "\n",
    "    if latest_file_path:\n",
    "        logging.info(f\"Found latest iteration file: {latest_file_path}\")\n",
    "    else:\n",
    "        logging.warning(f\"Could not determine latest file for prefix '{prefix}'.\")\n",
    "    return latest_file_path\n",
    "\n",
    "\n",
    "# Core Water Statistics Calculation Functions\n",
    "\n",
    "def calculate_water_stats_for_single_basin(\n",
    "    basin_geometry: Any,\n",
    "    water_depth_raster_dataset: rasterio.DatasetReader\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Calculates the total water volume and surface area for a single basin polygon.\n",
    "\n",
    "    Args:\n",
    "        basin_geometry: The Shapely geometry for the basin.\n",
    "        water_depth_raster_dataset: An opened Rasterio dataset for the water depth raster.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of (total_water_volume, total_water_surface_area).\n",
    "    \"\"\"\n",
    "    if basin_geometry is None or basin_geometry.is_empty or not basin_geometry.is_valid:\n",
    "        logging.warning(\"Input basin geometry is invalid. Returning zero for stats.\")\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    try:\n",
    "        masked_image_array, _ = mask(\n",
    "            dataset=water_depth_raster_dataset,\n",
    "            shapes=[basin_geometry],\n",
    "            crop=True,\n",
    "            nodata=0.0,\n",
    "            filled=True\n",
    "        )\n",
    "\n",
    "        water_depths_in_basin = masked_image_array[0]\n",
    "\n",
    "        pixels_with_water_mask = water_depths_in_basin > 1e-6\n",
    "\n",
    "        pixel_area = abs(water_depth_raster_dataset.transform.a * water_depth_raster_dataset.transform.e)\n",
    "\n",
    "        total_surface_area = np.sum(pixels_with_water_mask) * pixel_area\n",
    "\n",
    "        total_volume = np.sum(water_depths_in_basin[pixels_with_water_mask]) * pixel_area\n",
    "\n",
    "        return float(total_volume), float(total_surface_area)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing basin geometry for stats: {e}\", exc_info=True)\n",
    "        return 0.0, 0.0\n",
    "\n",
    "\n",
    "def process_all_basins_for_water_stats(\n",
    "    basins_shapefile_path: str,\n",
    "    water_depth_raster_path: str,\n",
    "    basin_id_col_name: str\n",
    ") -> Optional[gpd.GeoDataFrame]:\n",
    "    \"\"\"\n",
    "    Processes each basin from a shapefile to calculate water statistics.\n",
    "\n",
    "    Args:\n",
    "        basins_shapefile_path: Path to the input basin shapefile.\n",
    "        water_depth_raster_path: Path to the water depth raster.\n",
    "        basin_id_col_name: The name of the column that uniquely identifies each basin.\n",
    "\n",
    "    Returns:\n",
    "        A GeoDataFrame with 'water_volume' and 'water_area' columns, or None on failure.\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting water depth statistics calculation.\")\n",
    "    logging.info(f\"Using basins: '{basins_shapefile_path}' (ID column: '{basin_id_col_name}')\")\n",
    "    logging.info(f\"Using depth raster: '{water_depth_raster_path}'\")\n",
    "\n",
    "    try:\n",
    "        basins_gdf = gpd.read_file(basins_shapefile_path)\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"Failed to load basins shapefile: {e}\")\n",
    "        return None\n",
    "\n",
    "    if basin_id_col_name not in basins_gdf.columns:\n",
    "        logging.critical(f\"ID column '{basin_id_col_name}' not found in basins file.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        with rasterio.open(water_depth_raster_path) as depth_raster_src:\n",
    "            target_crs = TARGET_CRS if TARGET_CRS else depth_raster_src.crs\n",
    "\n",
    "            if basins_gdf.crs != target_crs:\n",
    "                logging.info(f\"Reprojecting basins to target CRS: '{target_crs}'.\")\n",
    "                basins_gdf = basins_gdf.to_crs(target_crs)\n",
    "\n",
    "            volumes, areas = [], []\n",
    "\n",
    "            logging.info(\"Processing each basin...\")\n",
    "            for _, basin_row in basins_gdf.iterrows():\n",
    "                volume, area = calculate_water_stats_for_single_basin(basin_row.geometry, depth_raster_src)\n",
    "                volumes.append(volume)\n",
    "                areas.append(area)\n",
    "\n",
    "            basins_gdf['water_volume'] = volumes\n",
    "            basins_gdf['water_area'] = areas\n",
    "            logging.info(\"Statistics calculation complete.\")\n",
    "\n",
    "            # --- Export Results ---\n",
    "            output_dir = os.path.dirname(basins_shapefile_path)\n",
    "            output_shp_path = os.path.join(output_dir, OUTPUT_BASINS_WITH_STATS_SHP)\n",
    "            output_csv_path = os.path.join(output_dir, OUTPUT_BASINS_STATS_CSV)\n",
    "\n",
    "            logging.info(f\"Exporting results to Shapefile: '{output_shp_path}'\")\n",
    "            basins_gdf.to_file(output_shp_path)\n",
    "\n",
    "            csv_summary = basins_gdf[[basin_id_col_name, 'water_volume', 'water_area']]\n",
    "            csv_summary.to_csv(output_csv_path, index=False)\n",
    "            logging.info(f\"Exporting summary to CSV: '{output_csv_path}'\")\n",
    "\n",
    "            # --- Print Summary ---\n",
    "            wet_basins = csv_summary[csv_summary['water_volume'] > 1e-6]\n",
    "            logging.info(\"\\n--- Summary Water Statistics ---\")\n",
    "            logging.info(f\"Total Basins Processed: {len(basins_gdf)}\")\n",
    "            logging.info(f\"Basins with Water: {len(wet_basins)}\")\n",
    "            logging.info(f\"Total Water Volume: {csv_summary['water_volume'].sum():,.2f} cubic units\")\n",
    "            logging.info(f\"Total Water Surface Area: {csv_summary['water_area'].sum():,.2f} square units\")\n",
    "            if not wet_basins.empty:\n",
    "                logging.info(f\"Mean Volume (wet basins): {wet_basins['water_volume'].mean():,.2f} cubic units\")\n",
    "            logging.info(\"---------------------------------\")\n",
    "\n",
    "            return basins_gdf\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logging.critical(f\"Water depth raster not found at '{water_depth_raster_path}'.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"An error occurred during processing: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "\n",
    "# Script Execution\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"Script started directly.\")\n",
    "\n",
    "    input_basins_path = find_latest_iteration_shp_file(\n",
    "        BASIN_SHAPEFILE_DIRECTORY,\n",
    "        BASIN_SHAPEFILE_PREFIX\n",
    "    )\n",
    "    basin_id_column = BASIN_ID_COLUMN\n",
    "\n",
    "    if not os.path.exists(WATER_DEPTH_RASTER_PATH):\n",
    "        logging.critical(f\"Water depth raster not found: '{WATER_DEPTH_RASTER_PATH}'\")\n",
    "        exit(1)\n",
    "    if not input_basins_path or not os.path.exists(input_basins_path):\n",
    "        logging.critical(f\"Basin shapefile could not be found: '{input_basins_path}'\")\n",
    "        exit(1)\n",
    "\n",
    "    process_all_basins_for_water_stats(\n",
    "        basins_shapefile_path=input_basins_path,\n",
    "        water_depth_raster_path=WATER_DEPTH_RASTER_PATH,\n",
    "        basin_id_col_name=basin_id_column\n",
    "    )\n",
    "\n",
    "    logging.info(\"Script finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c9c987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import time\n",
    "from collections import deque\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point, LineString, MultiLineString\n",
    "\n",
    "# Logging Configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(module)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# Configuration: Input/Output File Paths and Parameters\n",
    "\n",
    "# Input File Paths\n",
    "DEFAULT_EDGES_FILE: str = \"flow_paths_all.shp\"\n",
    "DEFAULT_BASINS_FILE: str = \"basins_with_water_stats.shp\"\n",
    "\n",
    "# Basin ID Column Configuration\n",
    "BASIN_ID_COLUMN_NAME: str = \"basin_id\"\n",
    "\n",
    "# Output File Name\n",
    "OUTPUT_FLOW_ACCUMULATION_SHP: str = \"basins_with_flow_accumulation.shp\"\n",
    "\n",
    "# Processing Parameters\n",
    "TARGET_CRS: Optional[str] = None\n",
    "\n",
    "\n",
    "# Data Loading and Graph Building Functions\n",
    "\n",
    "def load_and_prepare_data(\n",
    "    edges_filepath: str,\n",
    "    basins_filepath: str,\n",
    "    basin_id_col: str\n",
    ") -> Tuple[Optional[gpd.GeoDataFrame], Optional[gpd.GeoDataFrame]]:\n",
    "    \"\"\"\n",
    "    Loads and prepares the edges and basins shapefiles for analysis.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Loading data: Edges='{edges_filepath}', Basins='{basins_filepath}'\")\n",
    "    try:\n",
    "        edges_gdf = gpd.read_file(edges_filepath)\n",
    "        basins_gdf = gpd.read_file(basins_filepath)\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"Failed to load input shapefiles: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    if basin_id_col not in basins_gdf.columns:\n",
    "        logging.error(f\"Specified basin ID column '{basin_id_col}' not found in '{basins_filepath}'.\")\n",
    "        return None, None\n",
    "\n",
    "    # Standardize basin data\n",
    "    basins_gdf['geo_area'] = basins_gdf.geometry.area\n",
    "    for col in ['water_volume', 'water_area']:\n",
    "        if col in basins_gdf.columns:\n",
    "            basins_gdf[col] = pd.to_numeric(basins_gdf[col], errors='coerce').fillna(0.0)\n",
    "        else:\n",
    "            logging.warning(f\"Column '{col}' not found in basins. Initializing to 0.0.\")\n",
    "            basins_gdf[col] = 0.0\n",
    "\n",
    "    basins_gdf['area_accum'] = 0.0\n",
    "    basins_gdf['stor_accum'] = 0.0\n",
    "\n",
    "    # Keep only necessary columns to avoid clutter\n",
    "    required_cols = [basin_id_col, 'geo_area', 'water_volume', 'water_area', 'area_accum', 'stor_accum', 'geometry']\n",
    "    basins_gdf = basins_gdf[required_cols]\n",
    "\n",
    "    logging.info(\"Data loading and preparation complete.\")\n",
    "    return edges_gdf, basins_gdf\n",
    "\n",
    "\n",
    "def find_basin_containing_point(\n",
    "    point: Point,\n",
    "    basins_sindex: Any,\n",
    "    basins_geometries: gpd.GeoSeries\n",
    ") -> Optional[int]:\n",
    "    \"\"\"Finds the index of the basin containing a given point.\"\"\"\n",
    "    if point is None or point.is_empty:\n",
    "        return None\n",
    "\n",
    "    possible_matches_indices = list(basins_sindex.intersection(point.bounds))\n",
    "    if not possible_matches_indices:\n",
    "        return None\n",
    "\n",
    "    candidate_basins = basins_geometries.iloc[possible_matches_indices]\n",
    "    containing_mask = candidate_basins.contains(point)\n",
    "\n",
    "    if containing_mask.any():\n",
    "        return candidate_basins[containing_mask].index[0]\n",
    "    return None\n",
    "\n",
    "\n",
    "def build_flow_graph(\n",
    "    edges_gdf: gpd.GeoDataFrame,\n",
    "    basins_gdf: gpd.GeoDataFrame,\n",
    "    basin_id_col: str\n",
    ") -> nx.DiGraph:\n",
    "    \"\"\"Builds a directed graph representing flow paths between basins.\"\"\"\n",
    "    logging.info(\"Building flow direction graph from edges...\")\n",
    "    flow_graph = nx.DiGraph()\n",
    "\n",
    "    # Add all basins as nodes, using their DataFrame index as the node ID\n",
    "    for idx, row in basins_gdf.iterrows():\n",
    "        flow_graph.add_node(idx, id_val=row[basin_id_col])\n",
    "\n",
    "    basins_sindex = basins_gdf.sindex\n",
    "\n",
    "    for _, edge_row in edges_gdf.iterrows():\n",
    "        line = edge_row.geometry\n",
    "        if not isinstance(line, LineString) or line.is_empty:\n",
    "            continue\n",
    "\n",
    "        start_point = Point(line.coords[0])\n",
    "        end_point = Point(line.coords[-1])\n",
    "\n",
    "        start_basin_idx = find_basin_containing_point(start_point, basins_sindex, basins_gdf.geometry)\n",
    "        end_basin_idx = find_basin_containing_point(end_point, basins_sindex, basins_gdf.geometry)\n",
    "\n",
    "        if start_basin_idx is not None and end_basin_idx is not None and start_basin_idx != end_basin_idx:\n",
    "            flow_graph.add_edge(start_basin_idx, end_basin_idx)\n",
    "\n",
    "    logging.info(f\"Flow graph built with {flow_graph.number_of_nodes()} nodes and {flow_graph.number_of_edges()} edges.\")\n",
    "    return flow_graph\n",
    "\n",
    "\n",
    "# Flow Accumulation Calculation\n",
    "\n",
    "def calculate_flow_accumulation(\n",
    "    flow_graph: nx.DiGraph,\n",
    "    basins_gdf: gpd.GeoDataFrame\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Calculates cumulative area and storage for each basin using the flow graph.\n",
    "    \"\"\"\n",
    "    logging.info(\"Calculating flow accumulation.\")\n",
    "\n",
    "    # Initialize accumulators with each basin's local values\n",
    "    area_accumulator = basins_gdf['geo_area'].to_dict()\n",
    "    storage_accumulator = basins_gdf['water_volume'].to_dict()\n",
    "\n",
    "    if not nx.is_directed_acyclic_graph(flow_graph):\n",
    "        logging.warning(\"Graph contains cycles! Accumulation results may be incorrect for basins in cycles.\")\n",
    "        # Attempt to proceed by breaking cycles, though results are not guaranteed\n",
    "        cycles = list(nx.simple_cycles(flow_graph))\n",
    "        for cycle in cycles:\n",
    "            # Break each cycle by removing one edge\n",
    "            edge_to_remove = (cycle[0], cycle[1])\n",
    "            if flow_graph.has_edge(*edge_to_remove):\n",
    "                flow_graph.remove_edge(*edge_to_remove)\n",
    "                logging.info(f\"Removed edge {edge_to_remove} to break a cycle.\")\n",
    "\n",
    "    try:\n",
    "        # Process nodes in topological order (from upstream to downstream)\n",
    "        for node_idx in nx.topological_sort(flow_graph):\n",
    "            # For each downstream neighbor of the current node...\n",
    "            for successor_idx in flow_graph.successors(node_idx):\n",
    "                # ...add the current node's total accumulated value to it.\n",
    "                area_accumulator[successor_idx] += area_accumulator[node_idx]\n",
    "                storage_accumulator[successor_idx] += storage_accumulator[node_idx]\n",
    "    except nx.NetworkXUnfeasible:\n",
    "        logging.error(\"Topological sort failed even after attempting to break cycles. Accumulation aborted.\")\n",
    "        return basins_gdf # Return with only local values\n",
    "\n",
    "    # Update the GeoDataFrame with the final accumulated values\n",
    "    basins_gdf['area_accum'] = basins_gdf.index.map(area_accumulator)\n",
    "    basins_gdf['stor_accum'] = basins_gdf.index.map(storage_accumulator)\n",
    "\n",
    "    logging.info(\"Flow accumulation calculation complete.\")\n",
    "    return basins_gdf\n",
    "\n",
    "\n",
    "# Main Orchestration Function\n",
    "\n",
    "def run_flow_accumulation_analysis(\n",
    "    edges_input_file: str,\n",
    "    basins_input_file: str,\n",
    "    output_shapefile: str,\n",
    "    basin_id_column: str\n",
    "):\n",
    "    \"\"\"Main function to orchestrate the flow accumulation analysis.\"\"\"\n",
    "    start_time = time.time()\n",
    "    logging.info(\"--- Starting Flow Accumulation Analysis ---\")\n",
    "\n",
    "    edges_gdf, basins_gdf = load_and_prepare_data(edges_input_file, basins_input_file, basin_id_col=basin_id_column)\n",
    "    if basins_gdf is None:\n",
    "        logging.error(\"Failed to load or prepare data. Aborting.\")\n",
    "        return\n",
    "\n",
    "    flow_network = build_flow_graph(edges_gdf, basins_gdf, basin_id_col=basin_id_column)\n",
    "\n",
    "    if flow_network.number_of_edges() == 0:\n",
    "        logging.warning(\"Flow graph has no edges. Accumulation will only reflect local values.\")\n",
    "\n",
    "    results_gdf = calculate_flow_accumulation(flow_network, basins_gdf)\n",
    "\n",
    "    logging.info(f\"Exporting results to: {output_shapefile}\")\n",
    "    results_gdf.to_file(output_shapefile)\n",
    "\n",
    "    logging.info(\"\\n--- Flow Accumulation Summary ---\")\n",
    "    logging.info(f\"Total basins processed: {len(results_gdf)}\")\n",
    "    logging.info(f\"Max accumulated area: {results_gdf['area_accum'].max():,.2f} sq. units\")\n",
    "    logging.info(f\"Max accumulated storage: {results_gdf['stor_accum'].max():,.2f} cubic units\")\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    logging.info(f\"--- Analysis finished in {elapsed:.2f} seconds ---\")\n",
    "\n",
    "\n",
    "# Script Execution\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"Script started directly for flow accumulation.\")\n",
    "\n",
    "    basins_file = DEFAULT_BASINS_FILE\n",
    "    id_column = BASIN_ID_COLUMN_NAME\n",
    "\n",
    "    edges_file = DEFAULT_EDGES_FILE\n",
    "\n",
    "    if not os.path.exists(edges_file):\n",
    "        logging.critical(f\"Edges file not found: '{edges_file}'\")\n",
    "    elif not basins_file or not os.path.exists(basins_file):\n",
    "        logging.critical(f\"Basins file not found or determined: '{basins_file}'\")\n",
    "    else:\n",
    "        run_flow_accumulation_analysis(\n",
    "            edges_input_file=edges_file,\n",
    "            basins_input_file=basins_file,\n",
    "            output_shapefile=OUTPUT_FLOW_ACCUMULATION_SHP,\n",
    "            basin_id_column=id_column\n",
    "        )\n",
    "\n",
    "    logging.info(\"Script finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7932f2",
   "metadata": {},
   "source": [
    "# Algorithms 4–6: Drainage Tree, Initial Hydrograph, and Dynamic Fill–Spill–Merge Simulation\n",
    "\n",
    "- **Algorithm 4** builds a drainage-tree network from depression connectivity (nodes/edges).\n",
    "- **Algorithm 5** generates an initial hydrograph per depression for a rainfall event.\n",
    "- **Algorithm 6** performs a dynamic simulation where depressions **fill, spill, and merge** over time.\n",
    "\n",
    "These algorithms are typically run after Algorithms 1–3 have produced consistent basin/inlet assignments and\n",
    "diagnostic checks look reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ea0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Wedge, Circle, FancyArrowPatch\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.colors as colors\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.gridspec as gridspec\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Dict, Set, Tuple, Union\n",
    "import logging\n",
    "import matplotlib.patches as mpatches\n",
    "from shapely.geometry import Point, Polygon, MultiPoint, LineString, MultiPolygon\n",
    "from shapely.ops import nearest_points\n",
    "import matplotlib.cm as cm\n",
    "import traceback\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Optional, Set\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.cm as cm\n",
    "from scipy.spatial import ConvexHull\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# User inputs (edit as needed)\n",
    "# File Paths\n",
    "lowest_points_file = r\"lowest_basin_points_iter_24.shp\"\n",
    "basins_file = r\"basins_flow_accumulation.shp\"\n",
    "inlets_file = r\"inlet.shp\"\n",
    "study_area_file = r\"studyarea.shp\"\n",
    "impervious_tif = r\"TAMUCC_Imperviousness.tif\"\n",
    "dem_file = r\"DEM.tif\"\n",
    "sinks_file = r\"updated_sinks.shp\"\n",
    "rainfall_csv = r\"E:\\GIS_code\\weatherstation_plot\\export\\rain_2025-06-16_02-20-00_2025-06-16_12-30-00.csv\"\n",
    "\n",
    "# Script Parameters\n",
    "selected_inlets = ['7','444','100','472','486','493','484','2170','622','561','2461']\n",
    "simulation_time_min = 240\n",
    "timestep_min = 0.1\n",
    "rain_unit = 'in/hr'\n",
    "P2_USER_VALUE = 4.53 # P2 value used for Tc/Lag calculations\n",
    "\n",
    "# Model Coefficients (Base Values)\n",
    "N_IMPERV_BASE: float = 0.011\n",
    "N_PERV_BASE: float = 0.15\n",
    "INFILTRATION_RATE_CM_S_BASE: float = 0.000964667\n",
    "\n",
    "\n",
    "# Calibration Adjustments (Multiplicative Factors or Percentage Changes)\n",
    "# These parameters allow for sensitivity analysis or calibration by adjusting base\n",
    "# values.\n",
    "# A factor of 1.0 or percentage of 0.0 means no change from base values.\n",
    "\n",
    "# 1. Slope Adjustment (for Tc calculation):\n",
    "#    Multiplicative factor for the calculated land slope (S_tc) used in Tc calculations.\n",
    "#    e.g., 1.02 means increase slope by 2% (S_tc_adjusted = S_tc_original * 1.02).\n",
    "SLOPE_ADJUSTMENT_FACTOR: float = 1.00 # Example: 1.0 (no change)\n",
    "\n",
    "# 2. Depression Storage Adjustment:\n",
    "#    Multiplicative factor applied to the SUM of base depression storage, texture\n",
    "# storage, and interception.\n",
    "#    e.g., 1.1 means increase total initial storage by 10%.\n",
    "DEPRESSION_STORAGE_ADJUSTMENT_FACTOR: float = 1.00 # Example: 1.0 (no change)\n",
    "\n",
    "# 3. Infiltration Rate Adjustment:\n",
    "#    Multiplicative factor for the base infiltration rate (INFILTRATION_RATE_CM_S_BASE).\n",
    "#    e.g., 0.9 means decrease infiltration rate by 10% (infil_rate_adj = infil_rate_base\n",
    "# * 0.9).\n",
    "INFILTRATION_RATE_ADJUSTMENT_FACTOR: float = 1.00 # Example: 1.0 (no change)\n",
    "\n",
    "# 4. Manning's N Impervious Adjustment:\n",
    "#    Multiplicative factor for the base Manning's n for impervious surfaces\n",
    "# (N_IMPERV_BASE).\n",
    "#    e.g., 1.1 means N_imperv_adjusted = N_imperv_base * 1.1.\n",
    "N_IMPERV_ADJUSTMENT_FACTOR: float = 1.00 # Example: 1.0 (no change)\n",
    "\n",
    "# 5. Manning's N Pervious Adjustment:\n",
    "#    Multiplicative factor for the base Manning's n for pervious surfaces (N_PERV_BASE).\n",
    "#    e.g., 1.1 means N_perv_adjusted = N_perv_base * 1.1.\n",
    "N_PERV_ADJUSTMENT_FACTOR: float = 1.00    # Example: 1.0 (no change)\n",
    "\n",
    "# 6. Storage Depth Adjustments (Absolute Values in Meters)\n",
    "#    These parameters define a uniform depth of initial storage across surface types.\n",
    "#    This volume is added to the base depression storage from the shapefile's\n",
    "# 'water_volu' field.\n",
    "\n",
    "#    a) Texture Storage on Impervious Surfaces:\n",
    "#    Represents water held in the texture of impervious surfaces (e.g., pavement).\n",
    "TEXTURE_STORAGE_DEPTH_IMPERVIOUS_M: float = 0.003 # meters (e.g., 2.5 mm)\n",
    "\n",
    "#    b) Interception on Pervious Surfaces:\n",
    "#    Represents water intercepted by vegetation on pervious surfaces.\n",
    "INTERCEPTION_STORAGE_DEPTH_PERVIOUS_M: float = 0.002 # meters (e.g., 1.0 mm)\n",
    "\n",
    "\n",
    "# Derived Parameters (incorporating adjustments for use in simulation)\n",
    "# These parameters are calculated from base values and adjustment factors.\n",
    "N_IMPERV: float = N_IMPERV_BASE * N_IMPERV_ADJUSTMENT_FACTOR\n",
    "N_PERV: float = N_PERV_BASE * N_PERV_ADJUSTMENT_FACTOR\n",
    "INFILTRATION_RATE_CM_S: float = INFILTRATION_RATE_CM_S_BASE * INFILTRATION_RATE_ADJUSTMENT_FACTOR\n",
    "\n",
    "# Conversion Factors\n",
    "METERS_TO_FEET = 3.28084\n",
    "\n",
    "# Plotting/Layout Constants\n",
    "VERTICAL_SPACING = 1.5\n",
    "HORIZONTAL_SPACING = 1.5\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "try:\n",
    "    matplotlib.rcParams.update({'font.sans-serif': 'Arial', 'font.family': 'sans-serif'})\n",
    "    logging.info(\"Attempting to set plot font to Arial.\")\n",
    "except Exception as e:\n",
    "    logging.warning(f\"Could not set font to Arial, using default: {e}\")\n",
    "    \n",
    "# Geometric Helper Functions\n",
    "\n",
    "def prep(d, tkey, qkey):\n",
    "    # Prepares timeline and flow data, converting flow to m³/min\n",
    "    if d is None: return None, None\n",
    "    t = d.get(tkey); q = d.get(qkey)\n",
    "    if t is None or q is None: return None, None\n",
    "    if not isinstance(t, (list, np.ndarray)) or not isinstance(q, (list, np.ndarray)): return None, None\n",
    "    try:\n",
    "        ta = np.array(t, dtype=float)\n",
    "        qa = np.array(q, dtype=float)\n",
    "        if np.any(np.isnan(ta)) or np.any(np.isinf(ta)):\n",
    "            logging.warning(f\"prep: Found NaN/Inf in time array for key {tkey}. Returning None.\")\n",
    "            return None, None\n",
    "        if np.any(np.isnan(qa)) or np.any(np.isinf(qa)):\n",
    "            logging.warning(f\"prep: Found NaN/Inf in quantity array for key {qkey}. Returning None.\")\n",
    "            return None, None\n",
    "    except (ValueError, TypeError) as e:\n",
    "        logging.warning(f\"prep: Error converting data for keys {tkey}/{qkey}: {e}. Returning None.\")\n",
    "        return None, None\n",
    "    n = min(ta.size, qa.size);\n",
    "    if n == 0: return None, None\n",
    "    return ta[:n], qa[:n] * 60.0\n",
    "\n",
    "def safe_interp(x_new, x_old, y_old, default_val=0.0, label=\"\"):\n",
    "    # Interpolates y_old onto x_new timeline safely\n",
    "    try:\n",
    "        x_new = np.asarray(x_new, dtype=float)\n",
    "        x_old = np.asarray(x_old, dtype=float)\n",
    "        y_old = np.asarray(y_old, dtype=float)\n",
    "        if x_new.size == 0: return np.full_like(x_new, default_val, dtype=float)\n",
    "        if x_old.size == 0 or y_old.size == 0: return np.full(x_new.shape, default_val, dtype=float)\n",
    "        if x_old.size != y_old.size:\n",
    "            logging.warning(f\"safe_interp ({label}): x_old ({x_old.size}) and y_old ({y_old.size}) size mismatch. Returning default.\")\n",
    "            return np.full(x_new.shape, default_val, dtype=float)\n",
    "        if np.any(np.isnan(x_old)) or np.any(np.isinf(x_old)):\n",
    "            logging.warning(f\"safe_interp ({label}): NaN/Inf found in x_old. Returning default.\")\n",
    "            return np.full(x_new.shape, default_val, dtype=float)\n",
    "        if np.any(np.isnan(y_old)) or np.any(np.isinf(y_old)):\n",
    "            logging.warning(f\"safe_interp ({label}): NaN/Inf found in y_old. Replacing with {default_val} before interp.\")\n",
    "            y_old = np.nan_to_num(y_old, nan=default_val, posinf=default_val, neginf=default_val)\n",
    "        if not np.all(np.diff(x_old) >= 0):\n",
    "            sort_indices = np.argsort(x_old); x_old = x_old[sort_indices]; y_old = y_old[sort_indices]\n",
    "        unique_indices = np.unique(x_old, return_index=True)[1]\n",
    "        if len(unique_indices) < len(x_old):\n",
    "            x_old = x_old[unique_indices]; y_old = y_old[unique_indices]\n",
    "        if x_old.size < 2:\n",
    "            logging.warning(f\"safe_interp ({label}): Too few points ({x_old.size}) left after cleaning x_old. Cannot interp, returning default.\")\n",
    "            return np.full(x_new.shape, default_val, dtype=float)\n",
    "        interp_result = np.interp(x_new, x_old, y_old, left=default_val, right=default_val)\n",
    "        interp_result = np.nan_to_num(interp_result.astype(float), nan=default_val, posinf=default_val, neginf=default_val)\n",
    "        return interp_result\n",
    "    except Exception as e:\n",
    "        logging.error(f\"safe_interp ({label}) failed unexpectedly: {e}\")\n",
    "        try: return np.full(np.asarray(x_new).shape, default_val, dtype=float)\n",
    "        except: return np.array([default_val], dtype=float)\n",
    "\n",
    "def find_farthest_vertex(geometry: Union[Polygon, MultiPolygon], point: Point) -> Optional[Point]:\n",
    "    \"\"\"\n",
    "    Finds the vertex on the boundary of a Polygon or MultiPolygon that is farthest from the given point.\n",
    "    This version correctly handles both single and multi-part geometries.\n",
    "    \"\"\"\n",
    "    max_dist = -1.0\n",
    "    farthest_v = None\n",
    "    if not geometry or not geometry.is_valid or not point or not isinstance(point, Point):\n",
    "        logging.warning(\"Invalid input geometry for find_farthest_vertex.\")\n",
    "        return None\n",
    "\n",
    "    # Start Of Correction\n",
    "    # Create a list of polygons to process. This handles both single and multi-part\n",
    "    # inputs.\n",
    "    polygons_to_process = []\n",
    "    if isinstance(geometry, Polygon):\n",
    "        polygons_to_process.append(geometry)\n",
    "    elif isinstance(geometry, MultiPolygon):\n",
    "        polygons_to_process.extend(list(geometry.geoms))\n",
    "    else:\n",
    "        logging.warning(f\"Unsupported geometry type for find_farthest_vertex: {type(geometry)}\")\n",
    "        return None\n",
    "\n",
    "    all_coords = []\n",
    "    for poly in polygons_to_process:\n",
    "        if poly.exterior and len(poly.exterior.coords) > 0:\n",
    "            all_coords.extend(poly.exterior.coords)\n",
    "        for interior in poly.interiors:\n",
    "            if len(interior.coords) > 0:\n",
    "                all_coords.extend(interior.coords)\n",
    "    # End Of Correction\n",
    "    \n",
    "    if not all_coords:\n",
    "        logging.warning(f\"No coordinates extracted from geometry. Cannot find farthest vertex.\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Using a set to get unique vertex coordinates before creating points\n",
    "        unique_coords = set(all_coords)\n",
    "        vertex_points = [Point(xy) for xy in unique_coords]\n",
    "        if not vertex_points:\n",
    "            raise ValueError(\"No valid Point objects created from unique coordinates.\")\n",
    "    except Exception as geom_err:\n",
    "        logging.error(f\"Error creating Point objects from polygon vertices: {geom_err}\")\n",
    "        return None\n",
    "\n",
    "    # Iterate through the created vertex points to find the farthest one\n",
    "    for vertex_geom in vertex_points:\n",
    "        try:\n",
    "            dist = point.distance(vertex_geom)\n",
    "            if dist > max_dist:\n",
    "                max_dist = dist\n",
    "                farthest_v = vertex_geom\n",
    "        except Exception as dist_err:\n",
    "            logging.warning(f\"Error calculating distance for vertex {vertex_geom}: {dist_err}\")\n",
    "            continue\n",
    "            \n",
    "    if farthest_v is None:\n",
    "        logging.warning(\"Could not determine a farthest vertex after checking all points.\")\n",
    "        \n",
    "    return farthest_v\n",
    "\n",
    "def calculate_tc_parameters_farthest_path(basin_poly: Union[Polygon, MultiPolygon],\n",
    "                                           sink_pt: Point,\n",
    "                                           dem_file: str) -> Tuple[Optional[float], Optional[float]]:\n",
    "    if not isinstance(basin_poly, (Polygon, MultiPolygon)) or not isinstance(sink_pt, Point):\n",
    "        logging.warning(f\"Invalid input types for Tc parameters: poly={type(basin_poly)}, sink={type(sink_pt)}\")\n",
    "        return None, None\n",
    "    if not basin_poly.is_valid or not sink_pt.is_valid:\n",
    "        logging.warning(f\"Invalid geometry provided for Tc parameter calculation.\")\n",
    "        return None, None\n",
    "    try:\n",
    "        farthest_vertex = find_farthest_vertex(basin_poly, sink_pt)\n",
    "        if not farthest_vertex:\n",
    "            logging.warning(\"Could not determine farthest vertex for Tc path.\")\n",
    "            return None, None\n",
    "        L_tc_m = farthest_vertex.distance(sink_pt)\n",
    "        if L_tc_m < 0.1:\n",
    "            logging.debug(f\"Farthest vertex distance to sink < 0.1m ({L_tc_m:.3f}m). Cannot reliably calculate slope. Returning L={L_tc_m}, slope=None.\")\n",
    "            return L_tc_m, None\n",
    "        with rasterio.open(dem_file) as src:\n",
    "            coords_to_sample = [(farthest_vertex.x, farthest_vertex.y), (sink_pt.x, sink_pt.y)]\n",
    "            try:\n",
    "                elevations_raw = list(src.sample(coords_to_sample))\n",
    "                elevations = [val[0] for val in elevations_raw]\n",
    "            except Exception as sample_err:\n",
    "                logging.error(f\"DEM sampling failed for Tc path calculation: {sample_err}\")\n",
    "                return L_tc_m, None\n",
    "            if len(elevations) < 2:\n",
    "                logging.error(\"DEM sampling returned fewer than 2 elevations.\")\n",
    "                return L_tc_m, None\n",
    "            elev_farthest = elevations[0]\n",
    "            elev_sink = elevations[1]\n",
    "            nodata_val = src.nodata\n",
    "            valid_farthest = np.isfinite(elev_farthest) and (nodata_val is None or elev_farthest != nodata_val)\n",
    "            valid_sink = np.isfinite(elev_sink) and (nodata_val is None or elev_sink != nodata_val)\n",
    "            if not valid_farthest or not valid_sink:\n",
    "                logging.warning(f\"Invalid elevation data. Farthest: {elev_farthest} (type {type(elev_farthest)}), Sink: {elev_sink} (type {type(elev_sink)}), DEM NoData value: {nodata_val}. Cannot calculate slope.\")\n",
    "                return L_tc_m, None\n",
    "            elev_farthest = float(elev_farthest)\n",
    "            elev_sink = float(elev_sink)\n",
    "            delta_elev = abs(elev_farthest - elev_sink)\n",
    "            S_tc_dim = delta_elev / L_tc_m\n",
    "            S_tc_dim = max(S_tc_dim, 1e-5)\n",
    "            logging.debug(f\"Tc Params: L={L_tc_m:.2f}m, Elevs=[{elev_farthest:.3f}, {elev_sink:.3f}], S={S_tc_dim:.5f}\")\n",
    "            return L_tc_m, S_tc_dim\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error calculating Tc parameters from farthest path: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "def compute_tc_sheet_flow(n_dimensionless: float, L_ft: float, P2_in: float, S_ft_ft: float) -> Optional[float]:\n",
    "    tc_min = np.nan\n",
    "    try:\n",
    "        if not (isinstance(n_dimensionless, (int, float)) and n_dimensionless > 0 and\n",
    "                isinstance(L_ft, (int, float)) and L_ft > 0 and\n",
    "                isinstance(P2_in, (int, float)) and P2_in > 0 and\n",
    "                isinstance(S_ft_ft, (int, float)) and S_ft_ft > 0):\n",
    "            logging.debug(f\"[SheetFlow] Invalid input (n={n_dimensionless}, L={L_ft}, P2={P2_in}, S={S_ft_ft}).\")\n",
    "            return None\n",
    "        tc_hr = (0.007 * (n_dimensionless * L_ft)**0.8) / (P2_in**0.5 * S_ft_ft**0.4)\n",
    "        tc_min = tc_hr * 60.0\n",
    "        return tc_min\n",
    "    except (ValueError, OverflowError, RuntimeWarning) as e:\n",
    "        logging.error(f\"[SheetFlow] Math error: {e}\", exc_info=False)\n",
    "        return None\n",
    "\n",
    "def compute_mannings_n(imperv_area, total_area):\n",
    "    if total_area <= 1e-6:\n",
    "        return N_PERV\n",
    "    imperv_area = float(imperv_area); total_area = float(total_area)\n",
    "    imperv_ratio = max(0.0, min(1.0, imperv_area / total_area))\n",
    "    n_total = imperv_ratio * N_IMPERV + (1 - imperv_ratio) * N_PERV\n",
    "    return n_total\n",
    "\n",
    "def get_rainfall_start_time(rainfall_csv_path: str) -> Optional[pd.Timestamp]:\n",
    "    \"\"\"\n",
    "    Reads a rainfall CSV file and returns the earliest timestamp found.\n",
    "    This serves as the absolute start time for the simulation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # We only need the 'time' column to find the minimum (start) time.\n",
    "        df = pd.read_csv(rainfall_csv_path, usecols=['time'])\n",
    "        if df.empty or 'time' not in df.columns:\n",
    "            logging.warning(f\"Cannot find 'time' column or data in {rainfall_csv_path} to determine simulation start time.\")\n",
    "            return None\n",
    "        # Convert to datetime objects and return the earliest one.\n",
    "        return pd.to_datetime(df['time']).min()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not determine start time from rainfall CSV '{rainfall_csv_path}': {e}\")\n",
    "        return None\n",
    "# 1) Data Classes for Drainage Network\n",
    "@dataclass\n",
    "class DrainageNode:\n",
    "    basin_id: str\n",
    "    area: float\n",
    "    impervious_area: float\n",
    "    pervious_area: float\n",
    "    water_volume: float\n",
    "    effective_depth: float\n",
    "    storage_capacity: float\n",
    "    children: List['DrainageNode']\n",
    "    parent: Optional['DrainageNode'] = None\n",
    "    is_inlet: bool = False\n",
    "    runoff_data: Optional[Dict] = None\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.basin_id)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, DrainageNode):\n",
    "            return False\n",
    "        return self.basin_id == other.basin_id\n",
    "\n",
    "    def add_child(self, child: 'DrainageNode'):\n",
    "        self.children.append(child)\n",
    "        child.parent = self\n",
    "\n",
    "    def get_subtree_stats(self) -> tuple[float, float, float]:\n",
    "        total_area = self.area\n",
    "        total_storage_capacity = self.storage_capacity\n",
    "        for child in self.children:\n",
    "            c_area, c_cap, _ = child.get_subtree_stats()\n",
    "            total_area += c_area\n",
    "            total_storage_capacity += c_cap\n",
    "        avg_eff_depth = total_storage_capacity / total_area if total_area > 1e-9 else 0\n",
    "        return total_area, total_storage_capacity, avg_eff_depth\n",
    "\n",
    "@dataclass\n",
    "class BasinState:\n",
    "    basin_id: str\n",
    "    current_area: float\n",
    "    impervious_area: float\n",
    "    pervious_area: float\n",
    "    max_volume: float\n",
    "    current_volume: float\n",
    "    effective_depth: float\n",
    "    alpha: float\n",
    "    infiltration_rate: float = 0.0\n",
    "    infiltrated_volume: float = 0.0\n",
    "    parent_id: Optional[str] = None\n",
    "    merged_from: List[str] = field(default_factory=list)\n",
    "    is_merged: bool = False\n",
    "    spilled_volume: float = 0.0\n",
    "    runoff_state: Dict = field(default_factory=dict)\n",
    "    lag_time: float = 0.0\n",
    "    response_start_time: float = 0.0\n",
    "    initial_transit_water: float = 0.0\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if not isinstance(self.merged_from, list) or not self.merged_from:\n",
    "            self.merged_from = [self.basin_id]\n",
    "        if self.runoff_state is None:\n",
    "            self.runoff_state = {}\n",
    "\n",
    "# 2) DrainageForest: Building the Drainage Network\n",
    "class DrainageForest:\n",
    "    def __init__(self):\n",
    "        self.inlet_roots: Dict[str, DrainageNode] = {}\n",
    "        self.all_nodes: Dict[str, DrainageNode] = {}\n",
    "\n",
    "    def calculate_effective_areas(self, basins_gdf: gpd.GeoDataFrame, impervious_tif: str) -> Dict[str, tuple]:\n",
    "        \"\"\"\n",
    "        Calculates impervious and pervious areas for each basin.\n",
    "        MODIFIED: Applies a 98% factor to the calculated impervious area, reclassifying\n",
    "        the remaining 2% of the original impervious area as pervious.\n",
    "        \"\"\"\n",
    "        effective_areas = {}\n",
    "        try:\n",
    "            with rasterio.open(impervious_tif) as src:\n",
    "                if basins_gdf.crs != src.crs:\n",
    "                    logging.info(f\"Reprojecting basins GDF from {basins_gdf.crs} to raster CRS {src.crs}\")\n",
    "                    basins_gdf_proj = basins_gdf.to_crs(src.crs)\n",
    "                else:\n",
    "                    basins_gdf_proj = basins_gdf\n",
    "\n",
    "                logging.info(f\"Calculating effective areas using CRS: {src.crs}\")\n",
    "                for idx, basin_polygon_feature in basins_gdf_proj.iterrows():\n",
    "                    original_basin_data = basins_gdf.loc[idx]\n",
    "                    basin_id_str = str(original_basin_data['basin_id'])\n",
    "                    tot_area_m2 = float(original_basin_data['area'])\n",
    "\n",
    "                    imp_area_raster_calc_m2 = 0.0\n",
    "                    try:\n",
    "                        geom = [basin_polygon_feature.geometry]\n",
    "                        out_image, out_transform = mask(src, geom, crop=True, nodata=src.nodata)\n",
    "\n",
    "                        if out_image.size > 0:\n",
    "                            if out_image.ndim == 3 and out_image.shape[0] == 1:\n",
    "                                out_image = out_image.squeeze(axis=0)\n",
    "\n",
    "                            valid_mask = (out_image != src.nodata) & (~np.isnan(out_image))\n",
    "                            imperv_pix = np.sum(out_image[valid_mask] == 1)\n",
    "                            perv_pix = np.sum(out_image[valid_mask] == 2)\n",
    "                            total_valid_pix = imperv_pix + perv_pix\n",
    "\n",
    "                            if total_valid_pix > 0:\n",
    "                                imperv_ratio_from_raster = imperv_pix / total_valid_pix\n",
    "                                imp_area_raster_calc_m2 = tot_area_m2 * imperv_ratio_from_raster\n",
    "\n",
    "                        # NEW LOGIC: Apply 98% factor to impervious area\n",
    "                        # The original pervious area is the total area minus the raster-\n",
    "                        # calculated impervious area.\n",
    "                        original_perv_area_m2 = tot_area_m2 - imp_area_raster_calc_m2\n",
    "\n",
    "                        # The amount of the original impervious area to be reclassified\n",
    "                        # as pervious (2%).\n",
    "                        reclassified_area_m2 = imp_area_raster_calc_m2 * 0.02\n",
    "\n",
    "                        # The new final impervious area is 98% of the original\n",
    "                        # calculation.\n",
    "                        final_impervious_area_m2 = imp_area_raster_calc_m2 * 0.98\n",
    "\n",
    "                        # The new final pervious area is the original pervious area plus\n",
    "                        # the reclassified amount.\n",
    "                        final_pervious_area_m2 = original_perv_area_m2 + reclassified_area_m2\n",
    "                        # End Of New Logic\n",
    "\n",
    "                        # Store the final calculated values.\n",
    "                        effective_areas[basin_id_str] = (max(0.0, final_impervious_area_m2), max(0.0, final_pervious_area_m2))\n",
    "\n",
    "                    except (ValueError, IndexError) as ve:\n",
    "                        logging.error(f\"Geometry or mask error for basin {basin_id_str}: {ve}\")\n",
    "                        effective_areas[basin_id_str] = (0.0, tot_area_m2)\n",
    "                    except Exception as e_mask:\n",
    "                        logging.error(f\"Error processing basin {basin_id_str} for effective area (masking part): {e_mask}\")\n",
    "                        traceback.print_exc()\n",
    "                        effective_areas[basin_id_str] = (0.0, tot_area_m2)\n",
    "\n",
    "        except rasterio.RasterioIOError as e_rio:\n",
    "            logging.error(f\"Error opening impervious raster {impervious_tif}: {e_rio}\")\n",
    "            raise\n",
    "        except Exception as e_outer:\n",
    "            logging.error(f\"An unexpected error occurred during effective area calculation: {e_outer}\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "        return effective_areas\n",
    "\n",
    "    def build_forest(self,\n",
    "                     lowest_points_gdf: gpd.GeoDataFrame,\n",
    "                     basins_gdf: gpd.GeoDataFrame,\n",
    "                     inlets_gdf: gpd.GeoDataFrame,\n",
    "                     study_area_gdf: gpd.GeoDataFrame,\n",
    "                     impervious_tif: str\n",
    "                     ) -> None:\n",
    "        logging.info(\"Building drainage forest...\")\n",
    "        target_crs = basins_gdf.crs\n",
    "        if target_crs is None:\n",
    "            logging.warning(\"Basins GDF has no CRS defined. Assuming compatibility or check inputs.\")\n",
    "            \n",
    "        if target_crs:\n",
    "            logging.info(f\"Using target CRS for forest building: {target_crs}\")\n",
    "            try:\n",
    "                if lowest_points_gdf.crs != target_crs: lowest_points_gdf = lowest_points_gdf.to_crs(target_crs)\n",
    "                if inlets_gdf.crs != target_crs: inlets_gdf = inlets_gdf.to_crs(target_crs)\n",
    "                if study_area_gdf.crs != target_crs: study_area_gdf = study_area_gdf.to_crs(target_crs)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error reprojecting input GDFs to {target_crs}: {e}\")\n",
    "                return\n",
    "        else:\n",
    "            logging.warning(\"Cannot reproject layers as target CRS is unknown. Assuming CRS match.\")\n",
    "\n",
    "        eff_areas = self.calculate_effective_areas(basins_gdf, impervious_tif)\n",
    "\n",
    "        try:\n",
    "            study_area_geom = study_area_gdf.geometry.unary_union\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error creating study area union: {e}\")\n",
    "            return\n",
    "\n",
    "        if not inlets_gdf.has_sindex: inlets_gdf.sindex\n",
    "        possible_inlets_idx = list(inlets_gdf.sindex.intersection(study_area_geom.bounds))\n",
    "        possible_inlets = inlets_gdf.iloc[possible_inlets_idx]\n",
    "        inlets_within = possible_inlets[possible_inlets.intersects(study_area_geom)]\n",
    "        logging.info(f\"Found {len(inlets_within)} inlets within study area geometry.\")\n",
    "\n",
    "        inlet_basins = set()\n",
    "        if not basins_gdf.has_sindex: basins_gdf.sindex\n",
    "        for idx, inlet in inlets_within.iterrows():\n",
    "            possible_matches_idx = list(basins_gdf.sindex.intersection(inlet.geometry.bounds))\n",
    "            if not possible_matches_idx: continue\n",
    "            possible_matches = basins_gdf.iloc[possible_matches_idx]\n",
    "            containing_basins = possible_matches[possible_matches.contains(inlet.geometry)]\n",
    "            if not containing_basins.empty:\n",
    "                basin_id = str(containing_basins.iloc[0]['basin_id'])\n",
    "                inlet_basins.add(basin_id)\n",
    "            else:\n",
    "                logging.warning(f\"Inlet {idx} at {inlet.geometry.coords[0]} not strictly contained within any basin polygon.\")\n",
    "        logging.info(f\"Identified {len(inlet_basins)} unique inlet basins: {sorted(list(inlet_basins))}\")\n",
    "\n",
    "        G = nx.DiGraph()\n",
    "        basins_gdf['basin_id_str'] = basins_gdf['basin_id'].astype(str)\n",
    "        lowest_points_gdf['basin_id_str'] = lowest_points_gdf['basin_id'].astype(str)\n",
    "        lowest_points_gdf['to_basin_str'] = lowest_points_gdf['to_basin'].astype(str)\n",
    "        \n",
    "        valid_basin_ids = set(basins_gdf['basin_id_str'])\n",
    "        valid_conn = lowest_points_gdf[lowest_points_gdf['basin_id_str'].isin(valid_basin_ids) & lowest_points_gdf['to_basin_str'].isin(valid_basin_ids)]\n",
    "        logging.info(f\"Found {len(valid_conn)} valid connections between existing basins.\")\n",
    "        for _, row in valid_conn.iterrows():\n",
    "            G.add_edge(row['basin_id_str'], row['to_basin_str'])\n",
    "        logging.info(f\"Connectivity graph built with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n",
    "\n",
    "        min_area_node_init = 1e-3\n",
    "        min_volume_node_init = 1e-5\n",
    "\n",
    "        nodes_to_process = valid_basin_ids\n",
    "        logging.info(f\"Attempting to create DrainageNode objects for {len(nodes_to_process)} basins...\")\n",
    "        nodes_created_count = 0\n",
    "        for b_id_str in nodes_to_process:\n",
    "            bdata_rows = basins_gdf[basins_gdf['basin_id_str'] == b_id_str]\n",
    "            if bdata_rows.empty:\n",
    "                logging.warning(f\"Basin data unexpectedly not found for basin_id '{b_id_str}'. Skipping node creation.\")\n",
    "                continue\n",
    "            bdata = bdata_rows.iloc[0]\n",
    "            \n",
    "            # Get base area and effective area info\n",
    "            original_area_val = float(bdata['area'])\n",
    "            a = max(min_area_node_init, original_area_val)\n",
    "            impA, pervA = eff_areas.get(b_id_str, (0, a if a > 1e-9 else 0.0))\n",
    "\n",
    "            # Calculate Storage Capacity\n",
    "            base_storage_volume_from_input: float\n",
    "            try:\n",
    "                base_storage_volume_from_input = float(bdata['water_volu'])\n",
    "            except (KeyError, ValueError, TypeError) as e:\n",
    "                logging.warning(f\"Basin {b_id_str}: Missing or invalid 'water_volu'. Defaulting to min_volume_node_init. Error: {e}\")\n",
    "                base_storage_volume_from_input = min_volume_node_init\n",
    "\n",
    "            # NEW: Calculate additional storage from impervious texture and pervious\n",
    "            # interception\n",
    "            # These parameters are now defined in the global USER INPUT section.\n",
    "\n",
    "            # Volume from impervious texture storage\n",
    "            volume_from_impervious_texture = impA * TEXTURE_STORAGE_DEPTH_IMPERVIOUS_M\n",
    "            logging.debug(f\"Basin {b_id_str}: Impervious texture volume (from {impA:.2f} m²): {volume_from_impervious_texture:.4e} m³\")\n",
    "\n",
    "            # Volume from pervious interception storage\n",
    "            volume_from_pervious_interception = pervA * INTERCEPTION_STORAGE_DEPTH_PERVIOUS_M\n",
    "            logging.debug(f\"Basin {b_id_str}: Pervious interception volume (from {pervA:.2f} m²): {volume_from_pervious_interception:.4e} m³\")\n",
    "\n",
    "            # Combine base storage with the two new additional storage components\n",
    "            combined_initial_storage_volume = (base_storage_volume_from_input +\n",
    "                                               volume_from_impervious_texture +\n",
    "                                               volume_from_pervious_interception)\n",
    "            # End Of New Logic\n",
    "            \n",
    "            # Apply adjustments and final enforcement\n",
    "            pre_factor_storage_volume = max(min_volume_node_init, combined_initial_storage_volume)\n",
    "            storage_after_factor_application = pre_factor_storage_volume * DEPRESSION_STORAGE_ADJUSTMENT_FACTOR\n",
    "            final_storage_capacity_for_node = max(min_volume_node_init, storage_after_factor_application)\n",
    "            \n",
    "            eff_depth = final_storage_capacity_for_node / a if a > 1e-9 else 0.0\n",
    "            \n",
    "            # Create the node with all calculated values\n",
    "            node = DrainageNode(\n",
    "                basin_id=b_id_str, \n",
    "                area=a,\n",
    "                impervious_area=impA, \n",
    "                pervious_area=pervA, \n",
    "                water_volume=final_storage_capacity_for_node,\n",
    "                effective_depth=eff_depth,\n",
    "                storage_capacity=final_storage_capacity_for_node,\n",
    "                children=[],\n",
    "                is_inlet=(b_id_str in inlet_basins), \n",
    "                runoff_data=None\n",
    "            )\n",
    "            self.all_nodes[b_id_str] = node\n",
    "            nodes_created_count += 1\n",
    "        logging.info(f\"Created {nodes_created_count} DrainageNode objects with potentially adjusted parameters.\")\n",
    "\n",
    "        valid_inlet_roots = {}\n",
    "        for b_id_str in inlet_basins:\n",
    "            if b_id_str in self.all_nodes:\n",
    "                valid_inlet_roots[b_id_str] = self.all_nodes[b_id_str]\n",
    "            else:\n",
    "                logging.warning(f\"Identified inlet basin {b_id_str} not found in created nodes. Skipping as root.\")\n",
    "        self.inlet_roots = valid_inlet_roots\n",
    "        logging.info(f\"Identified {len(self.inlet_roots)} valid inlet roots: {list(self.inlet_roots.keys())}\")\n",
    "\n",
    "        processed_for_tree = set()\n",
    "        for i_id in self.inlet_roots:\n",
    "            if i_id in G:\n",
    "                self.build_upstream_tree(G, i_id, processed_for_tree)\n",
    "            else:\n",
    "                logging.warning(f\"Inlet root {i_id} not found in the connectivity graph G. Tree might be isolated.\")\n",
    "                processed_for_tree.add(i_id) \n",
    "\n",
    "        init_count = len(self.all_nodes)\n",
    "        self.remove_disconnected_nodes()\n",
    "        final_count = len(self.all_nodes)\n",
    "        if init_count != final_count:\n",
    "            logging.info(f\"Removed {init_count - final_count} disconnected nodes (not upstream of any inlet).\")\n",
    "        logging.info(f\"Final forest has {len(self.all_nodes)} nodes across {len(self.inlet_roots)} trees.\")\n",
    "\n",
    "        for i_id, root in self.inlet_roots.items():\n",
    "            try:\n",
    "                ar, cap, dpth = root.get_subtree_stats()\n",
    "                n_count = self.count_tree_nodes(root)\n",
    "                logging.info(f\"Tree {i_id}: {n_count} nodes, Area={ar:.2f} m², Total Capacity={cap:.2f} m³, Avg Depth={dpth:.3f} m\")\n",
    "            except Exception as stat_err:\n",
    "                logging.error(f\"Error getting stats for tree {i_id}: {stat_err}\")\n",
    "                \n",
    "    def build_upstream_tree(self, G: nx.DiGraph, current_id: str, processed: Set[str]):\n",
    "        if current_id in processed or current_id not in self.all_nodes:\n",
    "            return\n",
    "        processed.add(current_id)\n",
    "        c_node = self.all_nodes[current_id]\n",
    "        try:\n",
    "            preds = list(G.predecessors(current_id))\n",
    "        except nx.NetworkXError:\n",
    "            preds = []\n",
    "        for p_id in preds:\n",
    "            if p_id in self.all_nodes:\n",
    "                p_node = self.all_nodes[p_id]\n",
    "                if p_node not in c_node.children and c_node != p_node.parent: # Check c_node != p_node.parent as well\n",
    "                    if p_node.parent is None:\n",
    "                        c_node.add_child(p_node)\n",
    "                        self.build_upstream_tree(G, p_id, processed)\n",
    "                    elif p_node.parent == c_node: # Already correctly parented by a previous (opposite direction) link in data?\n",
    "                        self.build_upstream_tree(G, p_id, processed) # Still need to process its children\n",
    "                    else: # p_node already has a DIFFERENT parent\n",
    "                        logging.warning(f\"Node {p_id} already has parent {p_node.parent.basin_id if p_node.parent else 'Unknown'}, cannot add as child to {current_id}. Check connectivity data for potential conflicts or multiple downstream paths not handled by tree structure.\")\n",
    "                elif p_node in c_node.children: # Already a child, just recurse\n",
    "                    self.build_upstream_tree(G, p_id, processed)\n",
    "            else:\n",
    "                logging.warning(f\"Predecessor node {p_id} for {current_id} not found in all_nodes. Connection ignored.\")\n",
    "\n",
    "\n",
    "    def remove_disconnected_nodes(self):\n",
    "        if not self.inlet_roots:\n",
    "            logging.warning(\"No inlet roots found, cannot determine connected nodes. Skipping removal.\")\n",
    "            return\n",
    "        connected = set()\n",
    "        for root in self.inlet_roots.values():\n",
    "            self._mark_connected(root, connected)\n",
    "        all_node_ids = set(self.all_nodes.keys())\n",
    "        disconnected_ids = all_node_ids - connected\n",
    "        if disconnected_ids:\n",
    "            logging.info(f\"Removing {len(disconnected_ids)} nodes not connected upstream from any inlet root.\")\n",
    "            for d_id in disconnected_ids:\n",
    "                if d_id in self.all_nodes:\n",
    "                    node_to_remove = self.all_nodes[d_id]\n",
    "                    if node_to_remove.parent and node_to_remove in node_to_remove.parent.children:\n",
    "                        try:\n",
    "                            node_to_remove.parent.children.remove(node_to_remove)\n",
    "                        except ValueError: pass \n",
    "                    del self.all_nodes[d_id]\n",
    "        else:\n",
    "            logging.info(\"No disconnected nodes found.\")\n",
    "\n",
    "    def _mark_connected(self, node: Optional[DrainageNode], connected: Set[str]):\n",
    "        if node is None or node.basin_id in connected:\n",
    "            return\n",
    "        connected.add(node.basin_id)\n",
    "        for child in node.children:\n",
    "            self._mark_connected(child, connected)\n",
    "\n",
    "    def count_tree_nodes(self, node: Optional[DrainageNode]) -> int:\n",
    "        if node is None: return 0\n",
    "        count = 1\n",
    "        for child in node.children:\n",
    "            count += self.count_tree_nodes(child)\n",
    "        return count\n",
    "# 3) Runoff Response Functions\n",
    "def calculate_average_intensity(rainfall_csv, rain_unit):\n",
    "    try:\n",
    "        df = pd.read_csv(rainfall_csv)\n",
    "        if 'time' not in df.columns or 'Rain' not in df.columns:\n",
    "            logging.error(f\"Rainfall CSV {rainfall_csv} must contain 'time' and 'Rain' columns.\")\n",
    "            return 0.0\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "        df = df.sort_values('time')\n",
    "        duration_sec = (df['time'].max() - df['time'].min()).total_seconds()\n",
    "        if duration_sec <= 0:\n",
    "            logging.warning(\"Invalid duration (<= 0) in rainfall data. Cannot calculate average intensity.\")\n",
    "            return 0.0\n",
    "        avg_rate = df['Rain'].mean(skipna=True)\n",
    "        if pd.isna(avg_rate):\n",
    "            logging.warning(\"Could not calculate average rain rate (all NaN?).\")\n",
    "            return 0.0\n",
    "        if rain_unit == 'cm/hr':    avg_intensity_m_s = (avg_rate * 0.01) / 3600.0\n",
    "        elif rain_unit == 'mm/hr': avg_intensity_m_s = (avg_rate * 0.001) / 3600.0\n",
    "        elif rain_unit == 'in/hr': avg_intensity_m_s = (avg_rate * 0.0254) / 3600.0\n",
    "        else:\n",
    "            logging.warning(f\"Unsupported rain unit: {rain_unit}. Cannot calculate average intensity.\")\n",
    "            avg_intensity_m_s = 0.0\n",
    "        logging.info(f\"Calculated average rainfall intensity: {avg_intensity_m_s:.6e} m/s (from {avg_rate:.2f} {rain_unit}) - FOR INFO ONLY\")\n",
    "        return avg_intensity_m_s\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Rainfall CSV file not found: {rainfall_csv}.\")\n",
    "        return 0.0\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading or processing rainfall CSV {rainfall_csv}: {e}.\")\n",
    "        return 0.0\n",
    "\n",
    "def build_SCS_UH(T_p, D, dt_sec):\n",
    "    time_base_factor = 5.0\n",
    "    T_base_sec = time_base_factor * T_p\n",
    "    x_tab = np.array([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.2, 2.4, 2.6, 2.8, 3.0, 3.2, 3.4, 3.6, 3.8, 4.0, 4.5, 5.0])\n",
    "    Q_tab = np.array([0.000, 0.030, 0.100, 0.190, 0.310, 0.470, 0.660, 0.820, 0.930, 0.990, 1.000, 0.990, 0.930, 0.860, 0.780, 0.680, 0.560, 0.460, 0.390, 0.330, 0.280, 0.207, 0.147, 0.107, 0.077, 0.055, 0.040, 0.029, 0.021, 0.015, 0.011, 0.005, 0.000])\n",
    "    t_tab = x_tab * T_p\n",
    "    n_steps_uh = int(np.ceil(T_base_sec / dt_sec)) + 1\n",
    "    t_array_uh = np.linspace(0, T_base_sec, n_steps_uh)\n",
    "    UH_ordinates = np.interp(t_array_uh, t_tab, Q_tab, left=0, right=0)\n",
    "    total_area = np.sum(UH_ordinates) * dt_sec\n",
    "    if total_area > 1e-9:\n",
    "        UH_normalized = UH_ordinates / total_area\n",
    "    else:\n",
    "        logging.warning(f\"Unit hydrograph area near zero (Tp={T_p:.2f}s, dt={dt_sec:.2f}s). Creating single pulse UH.\")\n",
    "        UH_normalized = np.zeros_like(t_array_uh)\n",
    "        if len(UH_normalized) > 0: UH_normalized[0] = 1.0 / dt_sec\n",
    "    return t_array_uh, UH_normalized\n",
    "\n",
    "def read_rainfall_csv(rainfall_csv, sim_time_min, timestep_min, rain_unit='cm/hr', start_time_offset=0.0):\n",
    "    try:\n",
    "        df = pd.read_csv(rainfall_csv)\n",
    "        if 'time' not in df.columns or 'Rain' not in df.columns:\n",
    "            logging.error(f\"Rainfall CSV {rainfall_csv} must contain 'time' and 'Rain' columns.\")\n",
    "            dt_sec = timestep_min * 60.0; timeline_min = np.arange(start_time_offset, start_time_offset + sim_time_min + timestep_min, timestep_min); rain_depth_m = np.zeros_like(timeline_min)\n",
    "            return timeline_min, rain_depth_m, dt_sec\n",
    "        df['time'] = pd.to_datetime(df['time']); df = df.sort_values('time')\n",
    "        start_t = df['time'].min(); df['time_min_from_start'] = (df['time'] - start_t).dt.total_seconds() / 60.0\n",
    "        end_time = start_time_offset + sim_time_min;\n",
    "        timeline_min = np.arange(start_time_offset, end_time + timestep_min, timestep_min)\n",
    "        df = df.set_index('time_min_from_start')\n",
    "        rain_series_orig_units = df['Rain'].reindex(timeline_min, method='ffill').fillna(0)\n",
    "        if rain_unit == 'cm/hr': rain_m_s = (rain_series_orig_units * 0.01) / 3600.0\n",
    "        elif rain_unit == 'mm/hr': rain_m_s = (rain_series_orig_units * 0.001) / 3600.0\n",
    "        elif rain_unit == 'in/hr': rain_m_s = (rain_series_orig_units * 0.0254) / 3600.0\n",
    "        else:\n",
    "            logging.error(f\"Unsupported rain unit: {rain_unit}. Returning zero rainfall.\");\n",
    "            dt_sec = timestep_min * 60.0; rain_depth_m = np.zeros_like(timeline_min)\n",
    "            return timeline_min, rain_depth_m, dt_sec\n",
    "        dt_sec = timestep_min * 60.0;\n",
    "        rain_depth_m = rain_m_s.to_numpy() * dt_sec\n",
    "        return timeline_min, rain_depth_m, dt_sec\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Rainfall CSV file not found: {rainfall_csv}.\");\n",
    "        dt_sec = timestep_min * 60.0; timeline_min = np.arange(start_time_offset, start_time_offset + sim_time_min + timestep_min, timestep_min); rain_depth_m = np.zeros_like(timeline_min)\n",
    "        return timeline_min, rain_depth_m, dt_sec\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading rainfall CSV {rainfall_csv}: {e}.\");\n",
    "        dt_sec = timestep_min * 60.0; timeline_min = np.arange(start_time_offset, start_time_offset + sim_time_min + timestep_min, timestep_min); rain_depth_m = np.zeros_like(timeline_min)\n",
    "        return timeline_min, rain_depth_m, dt_sec\n",
    "\n",
    "def calculate_infiltration(runoff_vol_step, perv_area, total_area, dt_sec):\n",
    "    if total_area <= 1e-9 or perv_area <= 1e-9: return 0.0\n",
    "    infiltration_rate_m_s = INFILTRATION_RATE_CM_S * 0.01\n",
    "    max_potential_infiltration_vol = perv_area * infiltration_rate_m_s * dt_sec\n",
    "    perv_ratio = perv_area / total_area\n",
    "    water_on_pervious = runoff_vol_step * perv_ratio\n",
    "    infiltration_vol = np.minimum(water_on_pervious, max_potential_infiltration_vol)\n",
    "    infiltration_vol = max(0.0, infiltration_vol)\n",
    "    return infiltration_vol\n",
    "\n",
    "# 4) Runoff Processor Class - Incorporates Sheet Flow Tc\n",
    "class RunoffProcessor:\n",
    "    \"\"\"Handles calculation of runoff hydrographs for individual and merged basins.\"\"\"\n",
    "    def __init__(self,\n",
    "                 forest: 'DrainageForest',\n",
    "                 basins_gdf: gpd.GeoDataFrame,\n",
    "                 sinks_gdf: gpd.GeoDataFrame,\n",
    "                 lowest_points_file: str,\n",
    "                 dem_file: str,\n",
    "                 P2_in: float):\n",
    "        self.forest = forest\n",
    "        self.basins_gdf = basins_gdf\n",
    "        self.sinks_gdf = sinks_gdf\n",
    "        self.lowest_points_file = lowest_points_file\n",
    "        self.dem_file = dem_file\n",
    "        self.P2_in = P2_in # This is P2_USER_VALUE from global scope\n",
    "        self.basin_runoff_data: Dict[str, Dict] = {}\n",
    "        self.lowest_points_gdf: Optional[gpd.GeoDataFrame] = None\n",
    "        self.dem_crs = None\n",
    "        self.rainfall_csv: Optional[str] = None\n",
    "        self.rain_unit: Optional[str] = None\n",
    "\n",
    "        logging.info(\"--- Initializing RunoffProcessor ---\")\n",
    "        logging.info(f\"Using P2 = {self.P2_in} inches for Tc/Lag calculations.\")\n",
    "        try:\n",
    "            with rasterio.open(self.dem_file) as src:\n",
    "                self.dem_crs = src.crs\n",
    "                if self.dem_crs is None: raise ValueError(\"DEM CRS could not be read.\")\n",
    "                logging.info(f\"DEM CRS read: {self.dem_crs.to_string()}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed opening DEM '{self.dem_file}': {e}\"); raise\n",
    "        try:\n",
    "            logging.info(f\"Loading lowest points file: {self.lowest_points_file}\")\n",
    "            lp_gdf = gpd.read_file(self.lowest_points_file)\n",
    "            self.lowest_points_gdf = lp_gdf.to_crs(self.dem_crs)\n",
    "            logging.info(f\"Lowest points reprojected to {self.dem_crs}\")\n",
    "            required_lp_cols = ['basin_id', 'to_basin', 'geometry']\n",
    "            for col in required_lp_cols:\n",
    "                if col not in self.lowest_points_gdf.columns: raise KeyError(f\"Lowest points missing required column: '{col}'\")\n",
    "            if 'basin_id_str' not in self.lowest_points_gdf.columns: self.lowest_points_gdf['basin_id_str'] = self.lowest_points_gdf['basin_id'].astype(str)\n",
    "            if 'to_basin_str' not in self.lowest_points_gdf.columns: self.lowest_points_gdf['to_basin_str'] = self.lowest_points_gdf['to_basin'].astype(str)\n",
    "            logging.info(\"Processed lowest points file successfully.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed loading/processing lowest points file {self.lowest_points_file}: {e}\"); raise\n",
    "        try:\n",
    "            logging.info(\"Checking/Reprojecting sinks GDF...\")\n",
    "            if self.sinks_gdf is None or not isinstance(self.sinks_gdf, gpd.GeoDataFrame): raise ValueError(\"Sinks GDF invalid.\")\n",
    "            if self.sinks_gdf.crs != self.dem_crs: self.sinks_gdf = self.sinks_gdf.to_crs(self.dem_crs)\n",
    "            required_sink_cols = ['basin_id', 'geometry']\n",
    "            for col in required_sink_cols:\n",
    "                if col not in self.sinks_gdf.columns: raise KeyError(f\"Sinks file missing required column: '{col}'\")\n",
    "            if 'basin_id_str' not in self.sinks_gdf.columns:\n",
    "                if 'basin_id' in self.sinks_gdf.columns: self.sinks_gdf['basin_id_str'] = self.sinks_gdf['basin_id'].astype(str)\n",
    "                else: raise KeyError(\"Sinks GDF missing required 'basin_id' column.\")\n",
    "            logging.info(\"Sinks GDF processed successfully.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed processing/reprojecting sinks GDF: {e}\"); raise\n",
    "        logging.info(\"--- RunoffProcessor Initialization Complete ---\")\n",
    "\n",
    "\n",
    "    def _get_geometry_for_basin(self, basin_id_str: str, get_sink: bool = False) -> Optional[Union[Polygon, MultiPolygon, Point]]:\n",
    "        \"\"\"Dynamically retrieves and projects basin or sink geometry from the input GDFs.\"\"\"\n",
    "        if not self.dem_crs:\n",
    "            logging.error(\"DEM CRS not set in RunoffProcessor, cannot get geometry.\")\n",
    "            return None\n",
    "        original_id = basin_id_str.split('+')[0]\n",
    "        gdf_to_use = self.sinks_gdf if get_sink else self.basins_gdf\n",
    "        lookup_id = original_id\n",
    "        if gdf_to_use is None:\n",
    "            logging.error(f\"Source GeoDataFrame for {'sinks' if get_sink else 'basins'} is None.\")\n",
    "            return None\n",
    "        if 'basin_id_str' not in gdf_to_use.columns:\n",
    "            if 'basin_id' in gdf_to_use.columns:\n",
    "                gdf_to_use['basin_id_str'] = gdf_to_use['basin_id'].astype(str)\n",
    "            else:\n",
    "                logging.error(f\"'basin_id_str' column missing and cannot be created in {'sinks' if get_sink else 'basins'} GDF.\")\n",
    "                return None\n",
    "        row = gdf_to_use[gdf_to_use['basin_id_str'] == lookup_id]\n",
    "        if row.empty:\n",
    "            logging.warning(f\"{'Sink' if get_sink else 'Basin'} geometry not found for original ID '{lookup_id}' in source GDF.\")\n",
    "            return None\n",
    "        try:\n",
    "            geom = row.iloc[0]['geometry']\n",
    "            if gdf_to_use.crs != self.dem_crs:\n",
    "                projected_geom = gpd.GeoSeries([geom], crs=gdf_to_use.crs).to_crs(self.dem_crs).iloc[0]\n",
    "            else:\n",
    "                projected_geom = geom\n",
    "            if not projected_geom.is_valid:\n",
    "                projected_geom = projected_geom.buffer(0)\n",
    "                if not projected_geom.is_valid:\n",
    "                    logging.warning(f\"Invalid {'sink' if get_sink else 'basin'} geometry for {lookup_id} after buffer(0)..\")\n",
    "                    return None\n",
    "            if get_sink and not isinstance(projected_geom, Point):\n",
    "                logging.warning(f\"Expected Point for sink {lookup_id}, found {type(projected_geom)}. Returning None.\")\n",
    "                return None\n",
    "            if not get_sink and not isinstance(projected_geom, (Polygon, MultiPolygon)):\n",
    "                logging.warning(f\"Expected Polygon/MultiPolygon for basin {lookup_id}, found {type(projected_geom)}. Returning None.\")\n",
    "                return None\n",
    "            return projected_geom\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error getting/projecting/validating geometry for {'sink' if get_sink else 'basin'} {lookup_id}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def _collect_basin_ids(self, node: Optional['DrainageNode'], basin_ids_set: set):\n",
    "        if not node or node.basin_id in basin_ids_set: return\n",
    "        basin_ids_set.add(node.basin_id)\n",
    "        for child in node.children: self._collect_basin_ids(child, basin_ids_set)\n",
    "\n",
    "    def calculate_runoff_for_all_basins(self, rainfall_csv: str, sim_time_min: float,\n",
    "                                         timestep_min: float, rain_unit: str = 'cm/hr',\n",
    "                                         inlet_ids: Optional[List[str]] = None):\n",
    "        logging.info(\"Calculating runoff responses for relevant basins...\")\n",
    "        if not self.dem_crs: logging.error(\"DEM CRS not set.\"); return\n",
    "        self.rainfall_csv = rainfall_csv; self.rain_unit = rain_unit\n",
    "        relevant_basins = set()\n",
    "        if inlet_ids:\n",
    "            logging.info(f\"Calculating runoff for trees associated with inlets: {inlet_ids}\")\n",
    "            for inlet_id in inlet_ids:\n",
    "                inlet_id_str = str(inlet_id)\n",
    "                if inlet_id_str in self.forest.inlet_roots: self._collect_basin_ids(self.forest.inlet_roots[inlet_id_str], relevant_basins)\n",
    "                else: logging.warning(f\"Specified inlet ID {inlet_id_str} not found in forest roots.\")\n",
    "            logging.info(f\"Processing {len(relevant_basins)} basins in selected tree(s).\")\n",
    "        else: relevant_basins = set(self.forest.all_nodes.keys()); logging.info(f\"Processing all {len(relevant_basins)} basins.\")\n",
    "        if not relevant_basins: logging.error(\"No relevant basins found.\"); return\n",
    "        processed_count = 0; failed_basins = set(); skipped_basins = set()\n",
    "        total_basins_to_process = len(relevant_basins)\n",
    "        logging.info(f\"Starting runoff calculation loop for {total_basins_to_process} basins.\")\n",
    "        for i, basin_id in enumerate(relevant_basins):\n",
    "            node = self.forest.all_nodes.get(basin_id)\n",
    "            if not node:\n",
    "                logging.warning(f\"Basin ID {basin_id} node not found in forest. Skipping runoff calc.\")\n",
    "                skipped_basins.add(basin_id); continue\n",
    "            if basin_id in self.basin_runoff_data:\n",
    "                logging.debug(f\"Runoff data already exists for basin {basin_id}. Skipping.\"); processed_count += 1; continue\n",
    "            logging.debug(f\"Attempting runoff calculation for basin {basin_id} ({i+1}/{total_basins_to_process})...\")\n",
    "            runoff_data = self.calculate_basin_runoff(\n",
    "                basin_id=basin_id, basin_area=node.area, imperv_area=node.impervious_area, perv_area=node.pervious_area,\n",
    "                rainfall_csv=rainfall_csv, sim_time_min=sim_time_min, timestep_min=timestep_min,\n",
    "                rain_unit=rain_unit, P2_in=self.P2_in\n",
    "            )\n",
    "            if runoff_data:\n",
    "                required_keys_for_merge = [\n",
    "                    'convolution_timeline_min', 'direct_flow_m3_s', 'infiltration_vol_step',\n",
    "                    'max_potential_infiltration_vol_step', 'timeline_min',\n",
    "                    'actual_direct_runoff_vol_step', 'impervious_area', 'pervious_area',\n",
    "                    'basin_area', 'dt_sec' ]\n",
    "                missing_keys = [k for k in required_keys_for_merge if k not in runoff_data]\n",
    "                if missing_keys:\n",
    "                    logging.error(f\"Runoff calculation for {basin_id} SUCCEEDED but is MISSING keys needed later: {missing_keys}. Discarding.\")\n",
    "                    failed_basins.add(basin_id)\n",
    "                else:\n",
    "                    self.basin_runoff_data[basin_id] = runoff_data; processed_count += 1\n",
    "                    logging.debug(f\"Successfully calculated runoff for basin {basin_id}.\")\n",
    "            else:\n",
    "                logging.warning(f\"Runoff calculation recorded as FAILED for basin {basin_id}.\")\n",
    "                failed_basins.add(basin_id)\n",
    "            if (i + 1) % 100 == 0: logging.info(f\"Processed runoff calculations for {i+1}/{total_basins_to_process} relevant basins...\")\n",
    "        logging.info(\"Runoff calculation loop finished.\")\n",
    "        logging.info(f\"Successfully generated runoff data for {processed_count} basins.\")\n",
    "        if skipped_basins: logging.warning(f\"Skipped runoff calculation for {len(skipped_basins)} basins (missing node): {sorted(list(skipped_basins))}\")\n",
    "        if failed_basins: logging.error(f\"Runoff calculation FAILED for {len(failed_basins)} basins: {sorted(list(failed_basins))}\")\n",
    "        if failed_basins or skipped_basins: logging.warning(\"Some relevant basins were skipped or failed runoff calculation. Simulation may halt.\")\n",
    "\n",
    "    def calculate_basin_runoff(self, basin_id: str, basin_area: float, imperv_area: float, perv_area: float,\n",
    "                                 rainfall_csv: str, sim_time_min: float, timestep_min: float, rain_unit: str,\n",
    "                                 P2_in: float) -> Optional[dict]:\n",
    "        logging.debug(f\"--- Starting Runoff Calculation for Basin: {basin_id} ---\")\n",
    "        try:\n",
    "            # 1. Get Geometry\n",
    "            basin_poly = self._get_geometry_for_basin(basin_id, get_sink=False)\n",
    "            sink_pt = self._get_geometry_for_basin(basin_id, get_sink=True)\n",
    "            if basin_poly is None or sink_pt is None:\n",
    "                raise ValueError(\"Failed to retrieve valid basin or sink geometry.\")\n",
    "\n",
    "            # 2. Calculate Manning's n\n",
    "            total_effective_area_for_n = imperv_area + perv_area\n",
    "            n_total = N_PERV\n",
    "            if total_effective_area_for_n > 1e-6:\n",
    "                n_total = compute_mannings_n(imperv_area, total_effective_area_for_n)\n",
    "            logging.debug(f\"Basin {basin_id}: n_total={n_total:.4f} used for Tc calc.\")\n",
    "\n",
    "            # 3. Compute Time of Concentration (Tc)\n",
    "            tc_sec = 300.0\n",
    "            L_tc_m_final, S_tc_dim_final = None, None\n",
    "            try:\n",
    "                L_tc_m, S_tc_dim_original = calculate_tc_parameters_farthest_path(basin_poly, sink_pt, self.dem_file)\n",
    "                L_tc_m_final = L_tc_m\n",
    "                S_tc_dim_adjusted = S_tc_dim_original\n",
    "                if S_tc_dim_original is not None:\n",
    "                    S_tc_dim_adjusted = S_tc_dim_original * SLOPE_ADJUSTMENT_FACTOR\n",
    "                    S_tc_dim_adjusted = max(S_tc_dim_adjusted, 1e-5)\n",
    "                    S_tc_dim_final = S_tc_dim_adjusted\n",
    "                if L_tc_m is not None and S_tc_dim_adjusted is not None:\n",
    "                    L_tc_ft = L_tc_m * METERS_TO_FEET\n",
    "                    S_tc_ftft = S_tc_dim_adjusted\n",
    "                    calculated_tc_min = compute_tc_sheet_flow(n_total, L_tc_ft, P2_in, S_tc_ftft)\n",
    "                    if calculated_tc_min is not None and calculated_tc_min > 0:\n",
    "                        tc_sec = calculated_tc_min * 60.0\n",
    "                    else:\n",
    "                        logging.warning(f\"[Tc Calc {basin_id}] Sheet flow calc failed. Using default Tc.\")\n",
    "                else:\n",
    "                    logging.warning(f\"[Tc Calc {basin_id}] L_tc_m or S_tc_dim_adjusted is None. Using default Tc.\")\n",
    "            except Exception as tc_err:\n",
    "                logging.warning(f\"Error during Tc calculation for {basin_id}: {tc_err}. Using default Tc.\")\n",
    "            \n",
    "            tc_sec = max(tc_sec, 60.0)\n",
    "            T_p = tc_sec\n",
    "            T_base_sec = 5.0 * T_p\n",
    "            logging.debug(f\"Basin {basin_id}: Tc={tc_sec:.1f}s, Tp={T_p:.1f}s, Tbase={T_base_sec:.1f}s\")\n",
    "\n",
    "            # 4. Read Rainfall Data\n",
    "            extended_sim_time_min = sim_time_min + (T_base_sec / 60.0)\n",
    "            timeline_min, rain_depth_m_step, dt_sec_from_rain = read_rainfall_csv(\n",
    "                rainfall_csv, extended_sim_time_min, timestep_min, rain_unit\n",
    "            )\n",
    "            if dt_sec_from_rain <= 0 or len(timeline_min) == 0:\n",
    "                raise ValueError(\"Rainfall reading failed or returned empty/invalid data.\")\n",
    "            dt_sec = dt_sec_from_rain\n",
    "\n",
    "            # 5. Build SCS Unit Hydrograph\n",
    "            t_uh_sec, UH_norm = build_SCS_UH(T_p, T_base_sec, dt_sec)\n",
    "            if len(UH_norm) == 0:\n",
    "                raise ValueError(\"Unit Hydrograph generation failed.\")\n",
    "\n",
    "            # 6. Calculate Simplified Excess & Actual Infiltration\n",
    "            input_rate_m_s = INFILTRATION_RATE_CM_S * 0.01\n",
    "            threshold_rate_m_s = 0.02 * input_rate_m_s\n",
    "            initial_effective_rate_m_s = 0.0\n",
    "            if basin_area > 1e-9 and input_rate_m_s > 1e-12:\n",
    "                initial_effective_rate_m_s = (input_rate_m_s * perv_area / basin_area)\n",
    "            perv_area_for_infiltration = perv_area\n",
    "            adjusted_infiltration_flag = False\n",
    "            if initial_effective_rate_m_s < threshold_rate_m_s and threshold_rate_m_s > 1e-12:\n",
    "                adjusted_infiltration_flag = True\n",
    "                if input_rate_m_s > 1e-12:\n",
    "                    equivalent_perv_area = (threshold_rate_m_s * basin_area) / input_rate_m_s\n",
    "                    perv_area_for_infiltration = min(basin_area, max(0.0, equivalent_perv_area))\n",
    "                else: # Should not happen if threshold_rate_m_s > 1e-12 unless input_rate_m_s is zero\n",
    "                    adjusted_infiltration_flag = False\n",
    "            \n",
    "            rain_rate_m_s = np.divide(rain_depth_m_step, dt_sec, out=np.zeros_like(rain_depth_m_step), where=dt_sec!=0)\n",
    "            simplified_excess_rate_m_s = np.maximum(0, rain_rate_m_s - effective_max_infil_rate_m_s)\n",
    "            simplified_excess_depth_m_step = simplified_excess_rate_m_s * dt_sec\n",
    "            total_rainfall_vol_step = rain_depth_m_step * basin_area\n",
    "            infiltration_vol_step = np.array([\n",
    "                calculate_infiltration(rv, perv_area_for_infiltration, basin_area, dt_sec)\n",
    "                for rv in total_rainfall_vol_step\n",
    "            ])\n",
    "            actual_direct_runoff_vol_step = np.maximum(0, total_rainfall_vol_step - infiltration_vol_step)\n",
    "            max_potential_infiltration_vol_step_val = perv_area_for_infiltration * input_rate_m_s * dt_sec if perv_area_for_infiltration > 1e-6 else 0.0\n",
    "            max_potential_infiltration_vol_step = np.full_like(total_rainfall_vol_step, max_potential_infiltration_vol_step_val)\n",
    "            deficit_rate_m_s = np.maximum(0, effective_max_infil_rate_m_s - rain_rate_m_s)\n",
    "            infiltration_deficit_vol_step = deficit_rate_m_s * basin_area * dt_sec\n",
    "\n",
    "            # 7. Convolve with UH using SIMPLIFIED Excess Depth\n",
    "            len_rain = len(simplified_excess_depth_m_step); len_uh = len(UH_norm)\n",
    "            direct_flow_m3_s = np.array([]); direct_flow_vol_step_conv = np.array([]); convolution_timeline_min = np.array([])\n",
    "            if len_rain == 0 or len_uh == 0:\n",
    "                num_steps_out = len(timeline_min) + len_uh - 1 if len(timeline_min) > 0 and len_uh > 0 else len(timeline_min)\n",
    "                direct_flow_m3_s = np.zeros(num_steps_out); direct_flow_vol_step_conv = np.zeros(num_steps_out)\n",
    "                if num_steps_out > 0 and len(timeline_min) > 0:\n",
    "                    convolution_timeline_min = timeline_min[0] + np.arange(num_steps_out) * timestep_min\n",
    "                else:\n",
    "                    convolution_timeline_min = np.arange(num_steps_out) * timestep_min\n",
    "            else:\n",
    "                convolved_discharge_depth = np.convolve(simplified_excess_depth_m_step, UH_norm * dt_sec)\n",
    "                num_conv_steps = len(convolved_discharge_depth)\n",
    "                if len(timeline_min) > 0:\n",
    "                    convolution_timeline_min = timeline_min[0] + np.arange(num_conv_steps) * timestep_min\n",
    "                else:\n",
    "                    convolution_timeline_min = np.arange(num_conv_steps) * timestep_min\n",
    "                direct_flow_vol_step_conv = convolved_discharge_depth * basin_area\n",
    "                direct_flow_m3_s = np.divide(direct_flow_vol_step_conv, dt_sec, out=np.zeros_like(direct_flow_vol_step_conv), where=dt_sec!=0)\n",
    "            \n",
    "            # MODIFICATION START: Inlet basin self-abstraction\n",
    "            node_obj = self.forest.all_nodes.get(basin_id)\n",
    "            if node_obj and node_obj.is_inlet: # Check if it's an inlet node\n",
    "                node_storage_capacity_for_abstraction = node_obj.storage_capacity\n",
    "                logging.debug(f\"Basin {basin_id} is an inlet node. Applying self-storage abstraction (Vmax={node_storage_capacity_for_abstraction:.3f} m³).\")\n",
    "                if node_storage_capacity_for_abstraction > 1e-9:\n",
    "                    abstracted_direct_flow_vol_step = np.copy(direct_flow_vol_step_conv)\n",
    "                    remaining_storage_to_fill = node_storage_capacity_for_abstraction\n",
    "                    for k_step in range(len(abstracted_direct_flow_vol_step)):\n",
    "                        if remaining_storage_to_fill <= 1e-9:\n",
    "                            break \n",
    "                        volume_in_this_step = abstracted_direct_flow_vol_step[k_step]\n",
    "                        can_be_stored = min(volume_in_this_step, remaining_storage_to_fill)\n",
    "                        abstracted_direct_flow_vol_step[k_step] = volume_in_this_step - can_be_stored\n",
    "                        remaining_storage_to_fill -= can_be_stored\n",
    "                    \n",
    "                    original_total_vol_before_abs = np.sum(direct_flow_vol_step_conv)\n",
    "                    direct_flow_vol_step_conv = abstracted_direct_flow_vol_step # Update with abstracted values\n",
    "                    direct_flow_m3_s = np.divide(direct_flow_vol_step_conv, dt_sec, out=np.zeros_like(direct_flow_vol_step_conv), where=dt_sec!=0) # Recalculate rate\n",
    "                    new_total_vol_after_abs = np.sum(direct_flow_vol_step_conv)\n",
    "                    logging.debug(f\"Basin {basin_id} (Inlet): Self-storage abstraction reduced hydrograph volume from {original_total_vol_before_abs:.3f} m³ to {new_total_vol_after_abs:.3f} m³.\")\n",
    "            # Modification End\n",
    "\n",
    "            # 8. Store Results\n",
    "            len_conv_time = len(convolution_timeline_min); len_input_time = len(timeline_min)\n",
    "            basin_runoff_data = {\n",
    "                'basin_id': basin_id, 'basin_area': basin_area,\n",
    "                'impervious_area': imperv_area, 'pervious_area': perv_area,\n",
    "                'perv_area_used_for_infiltration': perv_area_for_infiltration,\n",
    "                'adjusted_infiltration_flag': adjusted_infiltration_flag,\n",
    "                'L_tc_m': L_tc_m_final, 'S_tc_dim': S_tc_dim_final, 'n_total': n_total, 'tc_sec': tc_sec, 'P2_in_used': P2_in,\n",
    "                'timestep_min': timestep_min, 'dt_sec': dt_sec,\n",
    "                'timeline_min': timeline_min[:len_input_time].tolist(),\n",
    "                'rain_depth_m_step': rain_depth_m_step[:len_input_time].tolist(),\n",
    "                'total_rainfall_vol_step': total_rainfall_vol_step[:len_input_time].tolist(),\n",
    "                'cum_rainfall_vol': np.cumsum(total_rainfall_vol_step[:len_input_time]).tolist(),\n",
    "                'total_rainfall_vol': np.sum(total_rainfall_vol_step[:len_input_time]),\n",
    "                'max_potential_infiltration_vol_step': max_potential_infiltration_vol_step[:len_input_time].tolist(),\n",
    "                'infiltration_vol_step': infiltration_vol_step[:len_input_time].tolist(),\n",
    "                'cum_infiltration_vol': np.cumsum(infiltration_vol_step[:len_input_time]).tolist(),\n",
    "                'total_infiltration_vol': np.sum(infiltration_vol_step[:len_input_time]),\n",
    "                'actual_direct_runoff_vol_step': actual_direct_runoff_vol_step[:len_input_time].tolist(),\n",
    "                'simplified_excess_depth_m_step': simplified_excess_depth_m_step[:len_input_time].tolist(),\n",
    "                'infiltration_deficit_vol_step': infiltration_deficit_vol_step[:len_input_time].tolist(),\n",
    "                'convolution_timeline_min': convolution_timeline_min[:len_conv_time].tolist(),\n",
    "                'direct_flow_m3_s': direct_flow_m3_s[:len_conv_time].tolist(), # Uses potentially modified hydrograph\n",
    "                'direct_flow_vol_step': direct_flow_vol_step_conv[:len_conv_time].tolist(), # Uses potentially modified hydrograph\n",
    "                'cum_direct_runoff_vol': np.cumsum(direct_flow_vol_step_conv[:len_conv_time]).tolist(), # Uses potentially modified hydrograph\n",
    "                'total_direct_runoff_vol': np.sum(direct_flow_vol_step_conv[:len_conv_time]), # Uses potentially modified hydrograph\n",
    "                't_uh_sec': t_uh_sec.tolist(), 'UH_norm': UH_norm.tolist()\n",
    "            }\n",
    "            logging.debug(f\"--- Finished Runoff Calculation for Basin: {basin_id} ---\")\n",
    "            return basin_runoff_data\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"!!! Failed Runoff Calculation for Basin {basin_id}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def calculate_merged_basin_runoff(self,\n",
    "                                      upstream_id: str,\n",
    "                                      downstream_id: str,\n",
    "                                      current_time_sec: float,\n",
    "                                      timestep_min: float,\n",
    "                                      sim_time_min: float,\n",
    "                                      rainfall_csv: str, # Though not directly used, kept for signature consistency if planned\n",
    "                                      rain_unit: str,    # Though not directly used, kept for signature consistency if planned\n",
    "                                      P2_in: float) -> Optional[dict]:\n",
    "        \"\"\"\n",
    "        Calculates the runoff hydrograph for a newly merged basin resulting from\n",
    "        an upstream basin spilling into a downstream basin.\n",
    "\n",
    "        This method lags and routes the upstream hydrograph, calculates run-on\n",
    "        infiltration into the downstream area's available deficit, and combines\n",
    "        the hydrographs. The results are stored in the processor's basin_runoff_data.\n",
    "\n",
    "        Args:\n",
    "            upstream_id: The ID of the basin spilling (source of additional flow).\n",
    "            downstream_id: The ID of the basin receiving the spill (base basin).\n",
    "            current_time_sec: The simulation time (in seconds) at which the merge occurs.\n",
    "            timestep_min: The simulation timestep in minutes.\n",
    "            sim_time_min: The total simulation duration in minutes.\n",
    "            rainfall_csv: Path to the rainfall CSV (may be used by helpers or future enhancements).\n",
    "            rain_unit: Unit of rainfall (may be used by helpers or future enhancements).\n",
    "            P2_in: The 2-year, 24-hour rainfall depth in inches, used for Tc/lag calculations.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing the merged runoff data, or None if the calculation fails.\n",
    "        \"\"\"\n",
    "        logging.info(f\"MERGE EVENT: Basin '{upstream_id}' merging into '{downstream_id}' at t={current_time_sec/60.0:.2f} min.\")\n",
    "        merged_id = f\"{downstream_id}+{upstream_id}\" # Define merged ID early for logging\n",
    "\n",
    "        # Determine dt_sec (simulation timestep in seconds)\n",
    "        dt_sec = None\n",
    "        if timestep_min is not None and timestep_min > 0:\n",
    "            dt_sec = timestep_min * 60.0\n",
    "        else: # Fallback if timestep_min is not directly provided\n",
    "            dn_data_temp = self.basin_runoff_data.get(downstream_id)\n",
    "            up_data_temp = self.basin_runoff_data.get(upstream_id)\n",
    "            if dn_data_temp and 'dt_sec' in dn_data_temp and dn_data_temp['dt_sec'] > 0:\n",
    "                dt_sec = dn_data_temp['dt_sec']\n",
    "            elif up_data_temp and 'dt_sec' in up_data_temp and up_data_temp['dt_sec'] > 0:\n",
    "                dt_sec = up_data_temp['dt_sec']\n",
    "\n",
    "        if dt_sec is None or dt_sec <= 0:\n",
    "            logging.error(f\"Cannot determine valid dt_sec for merge {upstream_id}->{downstream_id}. Aborting merge calculation.\")\n",
    "            return None\n",
    "            \n",
    "        merge_time_min = current_time_sec / 60.0\n",
    "\n",
    "        try:\n",
    "            # 1. Retrieve runoff data for component basins\n",
    "            up_data = self.basin_runoff_data.get(upstream_id)\n",
    "            dn_data = self.basin_runoff_data.get(downstream_id)\n",
    "            if up_data is None or dn_data is None:\n",
    "                missing_basin = 'upstream' if up_data is None else 'downstream'\n",
    "                raise ValueError(f\"Merge failed: Missing base runoff data for {missing_basin} basin ('{upstream_id if missing_basin=='upstream' else downstream_id}').\")\n",
    "\n",
    "            # 2. Check for essential keys in component basin data\n",
    "            required_keys = [\n",
    "                'convolution_timeline_min', 'direct_flow_m3_s', 'timeline_min',\n",
    "                'basin_area', 'pervious_area', 'impervious_area',\n",
    "                'max_potential_infiltration_vol_step', 'infiltration_vol_step',\n",
    "                'infiltration_deficit_vol_step', 'dt_sec'\n",
    "            ]\n",
    "            missing_up_keys = [key for key in required_keys if key not in up_data]\n",
    "            missing_dn_keys = [key for key in required_keys if key not in dn_data]\n",
    "            if missing_up_keys:\n",
    "                logging.warning(f\"Upstream basin ({upstream_id}) runoff data missing required keys for merge: {missing_up_keys}. Proceeding with defaults where possible.\")\n",
    "            if missing_dn_keys:\n",
    "                logging.warning(f\"Downstream basin ({downstream_id}) runoff data missing required keys for merge: {missing_dn_keys}. Proceeding with defaults where possible.\")\n",
    "\n",
    "            # 3. Calculate Lag Time for routing upstream flow to downstream sink\n",
    "            lag_sec = 5.0 * 60.0  # Default lag time (e.g., 5 minutes) if detailed calculation fails\n",
    "            try:\n",
    "                # Determine the actual outlet point of the upstream (potentially merged)\n",
    "                # basin\n",
    "                # The upstream_id might be a compound ID like \"X+Y\", so we need the\n",
    "                # outlet of \"Y\"\n",
    "                outlet_component_id_of_upstream = upstream_id.split('+')[-1]\n",
    "                \n",
    "                # Ensure basin_id_str column exists, create if not (should be handled\n",
    "                # during GDF loading)\n",
    "                if 'basin_id_str' not in self.lowest_points_gdf.columns and 'basin_id' in self.lowest_points_gdf.columns:\n",
    "                    self.lowest_points_gdf['basin_id_str'] = self.lowest_points_gdf['basin_id'].astype(str)\n",
    "\n",
    "                out_row_df = self.lowest_points_gdf[self.lowest_points_gdf['basin_id_str'] == outlet_component_id_of_upstream]\n",
    "                if out_row_df.empty:\n",
    "                    raise ValueError(f\"Lowest point data not found for upstream component outlet: '{outlet_component_id_of_upstream}'.\")\n",
    "                \n",
    "                outlet_pt = out_row_df.iloc[0].geometry\n",
    "                if not isinstance(outlet_pt, Point) or not outlet_pt.is_valid:\n",
    "                    raise ValueError(f\"Invalid outlet geometry for upstream component: '{outlet_component_id_of_upstream}'.\")\n",
    "\n",
    "                # Get sink point of the downstream basin (its ultimate outlet for Tc\n",
    "                # purposes)\n",
    "                sink_pt_downstream = self._get_geometry_for_basin(downstream_id, get_sink=True)\n",
    "                if sink_pt_downstream is None:\n",
    "                    raise ValueError(f\"Sink point geometry not found or invalid for downstream basin: '{downstream_id}'.\")\n",
    "\n",
    "                # Sample elevations from DEM\n",
    "                with rasterio.open(self.dem_file) as src:\n",
    "                    nodata_val = src.nodata\n",
    "                    elev_out_list = list(src.sample([(outlet_pt.x, outlet_pt.y)]))\n",
    "                    elev_snk_list = list(src.sample([(sink_pt_downstream.x, sink_pt_downstream.y)]))\n",
    "\n",
    "                    elev_out = float(elev_out_list[0][0]) if elev_out_list and len(elev_out_list[0]) > 0 and (nodata_val is None or abs(float(elev_out_list[0][0]) - nodata_val) > 1e-6) else None\n",
    "                    elev_snk = float(elev_snk_list[0][0]) if elev_snk_list and len(elev_snk_list[0]) > 0 and (nodata_val is None or abs(float(elev_snk_list[0][0]) - nodata_val) > 1e-6) else None\n",
    "                    \n",
    "                    if elev_out is None: raise ValueError(f\"Outlet elevation is invalid or NoData for '{outlet_component_id_of_upstream}'.\")\n",
    "                    if elev_snk is None: raise ValueError(f\"Sink elevation is invalid or NoData for '{downstream_id}'.\")\n",
    "\n",
    "                L_m = outlet_pt.distance(sink_pt_downstream)\n",
    "                slope_val = abs(elev_out - elev_snk) / max(L_m, 1e-6) # Avoid division by zero for L_m\n",
    "                slope_val = max(slope_val, 1e-5) # Ensure a minimum slope\n",
    "\n",
    "                # Manning's n for the downstream basin (or the path between upstream\n",
    "                # outlet and downstream sink)\n",
    "                # This uses the properties of the downstream basin for simplicity here.\n",
    "                # Global N_IMPERV, N_PERV are already adjusted by their factors.\n",
    "                dn_imperv_area = dn_data.get('impervious_area', 0.0)\n",
    "                dn_perv_area = dn_data.get('pervious_area', 0.0)\n",
    "                dn_total_effective_area = dn_imperv_area + dn_perv_area\n",
    "                n_downstream_effective = compute_mannings_n(dn_imperv_area, dn_total_effective_area)\n",
    "                \n",
    "                # Global SLOPE_ADJUSTMENT_FACTOR is applied here\n",
    "                adjusted_slope_for_tc = slope_val * SLOPE_ADJUSTMENT_FACTOR\n",
    "                adjusted_slope_for_tc = max(adjusted_slope_for_tc, 1e-5)\n",
    "\n",
    "\n",
    "                # Calculate Tc for the reach, which serves as the lag time\n",
    "                # METERS_TO_FEET is a global constant\n",
    "                calculated_lag_tc_min = compute_tc_sheet_flow(n_downstream_effective, L_m * METERS_TO_FEET, P2_in, adjusted_slope_for_tc)\n",
    "                \n",
    "                if calculated_lag_tc_min is not None and calculated_lag_tc_min > 0:\n",
    "                    lag_sec = max(5.0, calculated_lag_tc_min * 60.0) # Ensure a minimum lag of 5 seconds\n",
    "                else:\n",
    "                    logging.warning(f\"Lag calculation using sheet flow returned invalid Tc ({calculated_lag_tc_min}). Using default lag {lag_sec/60.0:.2f} min.\")\n",
    "            except Exception as e_lag:\n",
    "                logging.error(f\"Lag calculation failed for {upstream_id}->{downstream_id}: {e_lag}. Using default lag {lag_sec/60.0:.2f} min.\")\n",
    "                traceback.print_exc()\n",
    "\n",
    "            # 4. Prepare Timelines and Unified Timeline for Merged Hydrograph\n",
    "            t_up_conv = np.array(up_data.get('convolution_timeline_min', []), dtype=float)\n",
    "            t_dn_conv = np.array(dn_data.get('convolution_timeline_min', []), dtype=float)\n",
    "            t_up_in = np.array(up_data.get('timeline_min', []), dtype=float) # For deficits/infiltration\n",
    "            t_dn_in = np.array(dn_data.get('timeline_min', []), dtype=float) # For deficits/infiltration\n",
    "\n",
    "            if not all(arr.size > 0 for arr in [t_up_conv, t_dn_conv, t_up_in, t_dn_in]):\n",
    "                raise ValueError(f\"Merge failed: One or more essential timelines are empty for '{upstream_id}' or '{downstream_id}'.\")\n",
    "\n",
    "            # Unified timeline extending to cover the full simulation plus any lag\n",
    "            # effects\n",
    "            start_time_unified = min(t_dn_conv[0], t_up_conv[0]) # Should ideally be 0 or simulation start\n",
    "            max_conv_time_components = max(t_dn_conv[-1] if t_dn_conv.size > 0 else 0, t_up_conv[-1] if t_up_conv.size > 0 else 0)\n",
    "            end_time_unified = max(sim_time_min, max_conv_time_components + (lag_sec / 60.0))\n",
    "            \n",
    "            unified_timeline = np.arange(start_time_unified, end_time_unified + timestep_min, timestep_min)\n",
    "            if unified_timeline.size == 0:\n",
    "                raise ValueError(\"Merge failed: Unified timeline generation resulted in an empty array.\")\n",
    "            \n",
    "            merge_idx_on_unified = np.searchsorted(unified_timeline, merge_time_min, side='left')\n",
    "\n",
    "            # 5. Interpolate and Prepare Key Hydrograph Arrays onto Unified Timeline\n",
    "            q_up_orig_m3s = np.array(up_data.get('direct_flow_m3_s', []), dtype=float)\n",
    "            q_dn_orig_m3s = np.array(dn_data.get('direct_flow_m3_s', []), dtype=float)\n",
    "\n",
    "            # Downstream flow interpolated on unified timeline\n",
    "            dn_q_interpolated_m3s = safe_interp(unified_timeline, t_dn_conv, q_dn_orig_m3s, label=f\"Downstream Q {downstream_id}\")\n",
    "\n",
    "            # Upstream flow that will be lagged and potentially spill\n",
    "            spill_start_idx_on_up_timeline = np.searchsorted(t_up_conv, merge_time_min, side='left')\n",
    "            \n",
    "            # Prepare lagged upstream flow\n",
    "            if q_up_orig_m3s.size == 0 or spill_start_idx_on_up_timeline >= len(t_up_conv):\n",
    "                spill_times_lagged, spill_q_lagged = np.array([]), np.array([]) # No flow to lag\n",
    "            else:\n",
    "                spill_times_lagged = t_up_conv[spill_start_idx_on_up_timeline:] + (lag_sec / 60.0) # Lagged time\n",
    "                spill_q_lagged = q_up_orig_m3s[spill_start_idx_on_up_timeline:]\n",
    "            \n",
    "            up_q_lagged_interpolated_m3s = safe_interp(unified_timeline, spill_times_lagged, spill_q_lagged, label=f\"Lagged Upstream Q {upstream_id}\")\n",
    "            up_vol_step_lagged_m3 = up_q_lagged_interpolated_m3s * dt_sec\n",
    "\n",
    "            # Prepare downstream deficit for run-on calculation\n",
    "            dn_deficit_vol_step_orig = np.array(dn_data.get('infiltration_deficit_vol_step', []), dtype=float)\n",
    "            aligned_dn_deficit_vol_step = safe_interp(unified_timeline, t_dn_in, dn_deficit_vol_step_orig, label=f\"Aligned Downstream Deficit {downstream_id}\")\n",
    "\n",
    "            # Other arrays for weighted averages or combined properties\n",
    "            up_deficit_vol_step_orig = np.array(up_data.get('infiltration_deficit_vol_step', []), dtype=float)\n",
    "            up_deficit_interpolated_for_weighting = safe_interp(unified_timeline, t_up_in, up_deficit_vol_step_orig, label=f\"Upstream Deficit {upstream_id}\")\n",
    "            \n",
    "            dn_infiltration_vol_step_orig = np.array(dn_data.get('infiltration_vol_step', []), dtype=float)\n",
    "            dn_infiltration_interpolated = safe_interp(unified_timeline, t_dn_in, dn_infiltration_vol_step_orig, label=f\"Downstream Infiltration {downstream_id}\")\n",
    "\n",
    "            up_max_potential_infil_orig = np.array(up_data.get('max_potential_infiltration_vol_step', []), dtype=float)\n",
    "            dn_max_potential_infil_orig = np.array(dn_data.get('max_potential_infiltration_vol_step', []), dtype=float)\n",
    "            up_max_potential_infil_interpolated = safe_interp(unified_timeline, t_up_in, up_max_potential_infil_orig, label=f\"Upstream Max Potential Infil {upstream_id}\")\n",
    "            dn_max_potential_infil_interpolated = safe_interp(unified_timeline, t_dn_in, dn_max_potential_infil_orig, label=f\"Downstream Max Potential Infil {downstream_id}\")\n",
    "            merged_max_potential_infiltration_step_m3 = dn_max_potential_infil_interpolated + up_max_potential_infil_interpolated\n",
    "\n",
    "            # 6. Calculate Final Run-on Infiltration onto Downstream Area\n",
    "            runon_inf_vol_step_merged_m3 = np.zeros_like(unified_timeline, dtype=float)\n",
    "            # Arrival index on unified timeline (when lagged upstream flow starts\n",
    "            # arriving)\n",
    "            arrival_idx_on_unified = np.searchsorted(unified_timeline, merge_time_min + (lag_sec / 60.0), side='left')\n",
    "            \n",
    "            # Iterate from arrival time to the end of the relevant period\n",
    "            # common_len_for_runon determines how far we can reliably calculate run-on\n",
    "            common_len_for_runon = min(len(aligned_dn_deficit_vol_step), len(up_vol_step_lagged_m3))\n",
    "            for k_idx in range(arrival_idx_on_unified, common_len_for_runon):\n",
    "                downstream_deficit_at_k = max(0.0, aligned_dn_deficit_vol_step[k_idx])\n",
    "                incoming_lagged_vol_at_k = up_vol_step_lagged_m3[k_idx]\n",
    "                runon_infiltration_at_k = min(downstream_deficit_at_k, incoming_lagged_vol_at_k)\n",
    "                runon_inf_vol_step_merged_m3[k_idx] = max(0.0, runon_infiltration_at_k)\n",
    "            \n",
    "            total_calculated_runon_m3 = np.sum(runon_inf_vol_step_merged_m3)\n",
    "\n",
    "            # 7. Calculate Adjusted Upstream Flow (after run-on infiltration) and Combined Flow\n",
    "            adj_up_vol_step_m3 = np.maximum(0.0, up_vol_step_lagged_m3 - runon_inf_vol_step_merged_m3)\n",
    "            adj_up_q_m3s = np.divide(adj_up_vol_step_m3, dt_sec, out=np.zeros_like(adj_up_vol_step_m3), where=dt_sec != 0)\n",
    "            \n",
    "            # Combined flow: downstream's own flow plus the adjusted (post-infiltration)\n",
    "            # lagged upstream flow\n",
    "            # Before merge time, it's just downstream. After, it's downstream + adjusted\n",
    "            # lagged upstream.\n",
    "            # Note: This logic might need refinement if downstream flow should also be\n",
    "            # affected by the merge timing itself.\n",
    "            # Current assumption: downstream hydrograph continues, and lagged upstream\n",
    "            # is added on top *after* its arrival.\n",
    "            combined_q_m3s = np.where(unified_timeline < merge_time_min, # More accurately, < arrival_time_min for adding upstream\n",
    "                                      dn_q_interpolated_m3s,\n",
    "                                      dn_q_interpolated_m3s + adj_up_q_m3s)\n",
    "            combined_vol_step_m3 = combined_q_m3s * dt_sec\n",
    "\n",
    "            # 8. Calculate NEW Weighted Average Deficit for the Merged Basin (for storage/later use if needed)\n",
    "            # Deficit in downstream area is reduced by the run-on infiltration it\n",
    "            # received.\n",
    "            dn_deficit_post_runon_m3 = np.maximum(0.0, aligned_dn_deficit_vol_step - runon_inf_vol_step_merged_m3)\n",
    "            \n",
    "            area_up = up_data.get('basin_area', 0.0)\n",
    "            area_dn = dn_data.get('basin_area', 0.0)\n",
    "            total_area_merged = area_up + area_dn\n",
    "            \n",
    "            merged_weighted_deficit_vol_step_m3 = np.zeros_like(unified_timeline, dtype=float)\n",
    "            if total_area_merged > 1e-9:\n",
    "                weight_dn = area_dn / total_area_merged\n",
    "                weight_up = area_up / total_area_merged\n",
    "                \n",
    "                # Before merge, use a weighted average of individual deficits\n",
    "                merged_weighted_deficit_vol_step_m3[:merge_idx_on_unified] = (\n",
    "                    safe_interp(unified_timeline[:merge_idx_on_unified], t_dn_in, dn_deficit_vol_step_orig, label=\"PreMerge Deficit DN\") * weight_dn +\n",
    "                    up_deficit_interpolated_for_weighting[:merge_idx_on_unified] * weight_up\n",
    "                )\n",
    "                # After merge, the \"downstream component\" deficit is the adjusted one,\n",
    "                # weighted by its area.\n",
    "                # Upstream component's deficit is conceptually \"used up\" or transferred.\n",
    "                merged_weighted_deficit_vol_step_m3[merge_idx_on_unified:] = dn_deficit_post_runon_m3[merge_idx_on_unified:] * weight_dn # Or simply dn_deficit_post_runon if it represents the whole new entity\n",
    "            else:\n",
    "                logging.warning(f\"Merged area is zero for {upstream_id}->{downstream_id}. Deficit will be zero.\")\n",
    "            merged_weighted_deficit_vol_step_m3 = np.maximum(0.0, merged_weighted_deficit_vol_step_m3)\n",
    "\n",
    "            # 9. Store Results for the Merged Basin\n",
    "            cum_combined_vol_m3 = np.cumsum(combined_vol_step_m3) if combined_vol_step_m3.size > 0 else np.array([0.0])\n",
    "            total_combined_vol_final_m3 = cum_combined_vol_m3[-1] if cum_combined_vol_m3.size > 0 else 0.0\n",
    "\n",
    "            # Total actual infiltration for the merged entity per step\n",
    "            # This is downstream's original infiltration plus the new run-on\n",
    "            # infiltration it received\n",
    "            min_len_actual_infiltration = min(len(dn_infiltration_interpolated), len(runon_inf_vol_step_merged_m3))\n",
    "            total_actual_infiltration_step_m3 = np.zeros_like(unified_timeline, dtype=float)\n",
    "            total_actual_infiltration_step_m3[:min_len_actual_infiltration] = (\n",
    "                dn_infiltration_interpolated[:min_len_actual_infiltration] +\n",
    "                runon_inf_vol_step_merged_m3[:min_len_actual_infiltration]\n",
    "            )\n",
    "            # If lengths differ, pad the shorter one for remaining steps if necessary\n",
    "            # (though safe_interp should align to unified_timeline)\n",
    "\n",
    "            final_imperv_area_merged = up_data.get('impervious_area', 0.0) + dn_data.get('impervious_area', 0.0)\n",
    "            final_perv_area_merged = up_data.get('pervious_area', 0.0) + dn_data.get('pervious_area', 0.0)\n",
    "            final_timeline_len = len(unified_timeline)\n",
    "\n",
    "            merged_data = {\n",
    "                'basin_id': merged_id,\n",
    "                'merged_from_ids': [downstream_id, upstream_id], # Convention: [receiving, spilling]\n",
    "                'basin_area': total_area_merged,\n",
    "                'impervious_area': final_imperv_area_merged,\n",
    "                'pervious_area': final_perv_area_merged,\n",
    "                'timestep_min': timestep_min,\n",
    "                'dt_sec': dt_sec,\n",
    "                'merge_time_min': merge_time_min,\n",
    "                'lag_used_sec': lag_sec,\n",
    "                'convolution_timeline_min': unified_timeline[:final_timeline_len],\n",
    "                'direct_flow_m3_s': combined_q_m3s[:final_timeline_len],\n",
    "                'direct_flow_vol_step': combined_vol_step_m3[:final_timeline_len],\n",
    "                'cum_direct_runoff_vol': cum_combined_vol_m3[:final_timeline_len],\n",
    "                'total_direct_runoff_vol': total_combined_vol_final_m3,\n",
    "                'max_potential_infiltration_vol_step': merged_max_potential_infiltration_step_m3[:final_timeline_len],\n",
    "                'infiltration_deficit_vol_step': merged_weighted_deficit_vol_step_m3[:final_timeline_len],\n",
    "                'infiltration_vol_step': total_actual_infiltration_step_m3[:final_timeline_len], # Total infiltration for the merged entity\n",
    "                'runon_infiltration_vol_step': runon_inf_vol_step_merged_m3[:final_timeline_len], # Specifically the run-on part\n",
    "                'total_runon_infiltration_calculated': total_calculated_runon_m3,\n",
    "                'timeline_min': unified_timeline[:final_timeline_len], # Input timeline reference\n",
    "                'component_down_flow_m3s': dn_q_interpolated_m3s[:final_timeline_len],\n",
    "                'original_lagged_flow_m3s': up_q_lagged_interpolated_m3s[:final_timeline_len], # Before run-on infiltration\n",
    "                'adjusted_upstream_lagged_flow_m3s': adj_up_q_m3s[:final_timeline_len], # After run-on infiltration\n",
    "            }\n",
    "\n",
    "            # 10. Clean up NaN/Inf and Convert arrays to lists for storage\n",
    "            final_merged_data_cleaned = {}\n",
    "            for key, value in merged_data.items():\n",
    "                if isinstance(value, np.ndarray):\n",
    "                    if np.issubdtype(value.dtype, np.number): # Check if numeric array\n",
    "                        value_clean = np.nan_to_num(value, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                        final_merged_data_cleaned[key] = value_clean.tolist()\n",
    "                    else: # For non-numeric arrays (e.g., object arrays, though not expected here for timeseries)\n",
    "                        final_merged_data_cleaned[key] = value.tolist()\n",
    "                elif isinstance(value, (int, float, np.number)): # Handles Python scalars and NumPy scalars\n",
    "                    if np.isnan(value) or np.isinf(value):\n",
    "                        final_merged_data_cleaned[key] = 0.0\n",
    "                    else:\n",
    "                        final_merged_data_cleaned[key] = float(value)\n",
    "                elif isinstance(value, list) and key == 'merged_from_ids': # Ensure IDs are strings\n",
    "                    final_merged_data_cleaned[key] = [str(item) for item in value]\n",
    "                else:\n",
    "                    final_merged_data_cleaned[key] = value\n",
    "            \n",
    "            # 11. Store the processed data for the new merged ID\n",
    "            self.basin_runoff_data[merged_id] = final_merged_data_cleaned\n",
    "            logging.info(f\"Successfully calculated and stored merged runoff data for '{merged_id}'.\")\n",
    "            return final_merged_data_cleaned\n",
    "\n",
    "        except KeyError as ke:\n",
    "            logging.error(f\"Merge calculation failed for {merged_id} due to missing key: {ke}. Upstream: {upstream_id}, Downstream: {downstream_id}.\")\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "        except ValueError as ve:\n",
    "            logging.error(f\"Merge calculation failed for {merged_id} due to value error: {ve}. Upstream: {upstream_id}, Downstream: {downstream_id}.\")\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error during merge calculation for {merged_id}: {e}. Upstream: {upstream_id}, Downstream: {downstream_id}.\")\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def get_volume_in_timespan(self, basin_id: str, start_time_min: float, end_time_min: float) -> float:\n",
    "        \"\"\" Calculates volume using interpolation on the cumulative hydrograph. \"\"\"\n",
    "        if basin_id not in self.basin_runoff_data: return 0.0\n",
    "        data = self.basin_runoff_data[basin_id]\n",
    "        timeline = data.get('convolution_timeline_min'); cum_vol = data.get('cum_direct_runoff_vol')\n",
    "        if timeline is None or cum_vol is None: logging.warning(f\"Missing timeline/cum_vol for {basin_id}\"); return 0.0\n",
    "        timeline = np.array(timeline); cum_vol = np.array(cum_vol)\n",
    "        min_len = min(len(timeline), len(cum_vol))\n",
    "        if min_len == 0: return 0.0\n",
    "        timeline = timeline[:min_len]; cum_vol = cum_vol[:min_len]\n",
    "        try:\n",
    "            vol_at_start = np.interp(start_time_min, timeline, cum_vol, left=0.0, right=cum_vol[-1] if len(cum_vol)>0 else 0.0)\n",
    "            vol_at_end = np.interp(end_time_min, timeline, cum_vol, left=0.0, right=cum_vol[-1] if len(cum_vol)>0 else 0.0)\n",
    "            interval_volume = vol_at_end - vol_at_start\n",
    "            return float(max(0.0, interval_volume))\n",
    "        except Exception as e: logging.error(f\"Error interpolating volume for {basin_id}: {e}\"); return 0.0\n",
    "\n",
    "\n",
    "    # get_flow_at_time Method (Remains the same)\n",
    "    def get_flow_at_time(self, basin_id: str, time_min: float) -> float:\n",
    "        \"\"\" Gets the calculated flow rate (m3/s) at a specific time using interpolation. \"\"\"\n",
    "        # (Same as original/previous versions)\n",
    "        if basin_id not in self.basin_runoff_data: return 0.0\n",
    "        data = self.basin_runoff_data[basin_id]\n",
    "        timeline = data.get('convolution_timeline_min'); flow = data.get('direct_flow_m3_s')\n",
    "        if timeline is None or flow is None: return 0.0\n",
    "        timeline = np.array(timeline); flow = np.array(flow)\n",
    "        min_len = min(len(timeline), len(flow))\n",
    "        if min_len == 0: return 0.0\n",
    "        timeline = timeline[:min_len]; flow = flow[:min_len]\n",
    "        try:\n",
    "            flow_at_time = np.interp(time_min, timeline, flow, left=0.0, right=0.0)\n",
    "            return float(flow_at_time)\n",
    "        except Exception as e: logging.error(f\"Interpolation error in get_flow_at_time for {basin_id} @ {time_min}: {e}\"); return 0.0\n",
    "\n",
    "# 5) Integrated Simulation Class\n",
    "\n",
    "@dataclass\n",
    "class IntegratedSimulation:\n",
    "    forest: 'DrainageForest'\n",
    "    runoff_processor: 'RunoffProcessor'\n",
    "    selected_inlet_ids: List[str]\n",
    "    pending_runon: Dict[str, Tuple[float, int, float]] = field(default_factory=dict)\n",
    "    active_states: Dict[str, BasinState] = field(default_factory=dict)\n",
    "    merged_states_history: List[Dict] = field(default_factory=list)\n",
    "    time_series: List[Dict] = field(default_factory=list)\n",
    "    water_balance: List[Dict] = field(default_factory=list)\n",
    "    tree_snapshots: List[Tuple[float, Dict]] = field(default_factory=list)\n",
    "    original_basin_ids_in_sim: Set[str] = field(default_factory=set)\n",
    "    simulation_time_min: Optional[float] = None\n",
    "    timestep_min: Optional[float] = None\n",
    "    dt_sec: Optional[float] = None\n",
    "    halted_prematurely: bool = False\n",
    "    halt_reason: str = \"\"\n",
    "    previous_total_storage: float = 0.0\n",
    "    initialization_successful: bool = False\n",
    "    connected_original_basin_ids: Set[str] = field(default_factory=set)\n",
    "    connected_total_area: float = 0.0\n",
    "    connected_impervious_area: float = 0.0\n",
    "    connected_pervious_area: float = 0.0\n",
    "    \n",
    "    RUNON_DISTRIBUTION_STEPS: int = 5\n",
    "\n",
    "    def __post_init__(self):\n",
    "        logging.info(\"--- Initializing Integrated Simulator ---\")\n",
    "        self.selected_inlet_ids = [str(iid) for iid in self.selected_inlet_ids]\n",
    "        self.initialization_successful = self._initialize_basin_states()\n",
    "        if self.initialization_successful:\n",
    "            logging.info(f\"Simulation initialized with {len(self.active_states)} active states.\")\n",
    "            logging.info(f\"Tracking {len(self.original_basin_ids_in_sim)} original basins.\")\n",
    "            if self.simulation_time_min is not None and self.timestep_min is not None:\n",
    "                logging.info(f\"Derived Sim Time: {self.simulation_time_min} min, Timestep: {self.timestep_min} min ({self.dt_sec} sec)\")\n",
    "                logging.info(f\"Run-on infiltration will be distributed over {self.RUNON_DISTRIBUTION_STEPS} steps starting after lag time.\")\n",
    "            else: logging.warning(\"Could not derive simulation time/timestep during init.\")\n",
    "            logging.info(f\"Selected Inlet Trees Rooted At: {self.selected_inlet_ids}\")\n",
    "            logging.info(\"--- Integrated Simulation Initialization Complete ---\")\n",
    "        else: logging.error(\"--- Integrated Simulation Initialization FAILED ---\")\n",
    "\n",
    "    def _initialize_basin_states(self) -> bool:\n",
    "        # Clear all state from previous runs\n",
    "        self.active_states.clear(); self.time_series.clear(); self.water_balance.clear()\n",
    "        self.tree_snapshots.clear(); self.previous_total_storage = 0.0; self.original_basin_ids_in_sim.clear()\n",
    "        self.pending_runon.clear()\n",
    "        # NEW: Reset connected area trackers\n",
    "        self.connected_original_basin_ids.clear()\n",
    "        self.connected_total_area = 0.0\n",
    "        self.connected_impervious_area = 0.0\n",
    "        self.connected_pervious_area = 0.0\n",
    "\n",
    "        nodes_to_initialize = set()\n",
    "        for inlet_id in self.selected_inlet_ids:\n",
    "            if inlet_id in self.forest.inlet_roots:\n",
    "                root_node = self.forest.inlet_roots[inlet_id]; queue = [root_node]; visited_init = set()\n",
    "                while queue:\n",
    "                    current_node = queue.pop(0)\n",
    "                    if current_node and current_node.basin_id not in visited_init:\n",
    "                        visited_init.add(current_node.basin_id)\n",
    "                        if current_node.basin_id in self.forest.all_nodes:\n",
    "                            nodes_to_initialize.add(current_node.basin_id); self.original_basin_ids_in_sim.add(current_node.basin_id)\n",
    "                            for child in current_node.children:\n",
    "                                if child: queue.append(child)\n",
    "                        else: logging.warning(f\"Node {current_node.basin_id} not in forest.all_nodes during init.\")\n",
    "            else: logging.warning(f\"Selected inlet {inlet_id} not found in forest roots.\")\n",
    "        if not nodes_to_initialize: logging.error(\"No valid nodes found for selected inlets.\"); return False\n",
    "        logging.info(f\"Initializing states for {len(nodes_to_initialize)} nodes (from {len(self.original_basin_ids_in_sim)} unique basins).\")\n",
    "        first_valid_runoff_data = None\n",
    "        for node_id in self.original_basin_ids_in_sim:\n",
    "                if node_id in self.runoff_processor.basin_runoff_data: first_valid_runoff_data = self.runoff_processor.basin_runoff_data[node_id]; break\n",
    "        if not first_valid_runoff_data: logging.error(\"Cannot initialize: No runoff data found.\"); return False\n",
    "        global simulation_time_min; self.simulation_time_min = simulation_time_min\n",
    "        self.timestep_min = first_valid_runoff_data.get('timestep_min'); self.dt_sec = first_valid_runoff_data.get('dt_sec')\n",
    "        if self.timestep_min is None or self.dt_sec is None or self.dt_sec <= 0 or self.simulation_time_min is None: logging.error(f\"Invalid simulation parameters derived.\"); return False\n",
    "        logging.info(f\"Using Timestep: {self.timestep_min:.4f} min ({self.dt_sec:.2f} sec), Sim Duration: {self.simulation_time_min} min.\")\n",
    "        initialized_count = 0\n",
    "        for node_id in nodes_to_initialize:\n",
    "            node = self.forest.all_nodes.get(node_id);\n",
    "            if not node: logging.warning(f\"Node object {node_id} not found.\"); continue\n",
    "            runoff_data = self.runoff_processor.basin_runoff_data.get(node_id)\n",
    "            if not runoff_data:\n",
    "                logging.warning(f\"Runoff data missing for {node_id}. Skipping & removing.\");\n",
    "                if node_id in self.original_basin_ids_in_sim: self.original_basin_ids_in_sim.remove(node_id)\n",
    "                continue\n",
    "            if abs(runoff_data.get('dt_sec', self.dt_sec) - self.dt_sec) > 1e-6: logging.warning(f\"Inconsistent dt_sec for {node_id}.\")\n",
    "            original_tc_sec = runoff_data.get('tc_sec', 300.0)\n",
    "            parent_id_in_sim = node.parent.basin_id if node.parent and node.parent.basin_id in nodes_to_initialize else None\n",
    "            initial_state = BasinState(\n",
    "                basin_id=node.basin_id, current_area=node.area, impervious_area=node.impervious_area, pervious_area=node.pervious_area,\n",
    "                max_volume=node.storage_capacity, current_volume=0.0, effective_depth=node.effective_depth, alpha=1.0,\n",
    "                parent_id=parent_id_in_sim, merged_from=[node.basin_id], is_merged=False, spilled_volume=0.0,\n",
    "                runoff_state=runoff_data, lag_time=original_tc_sec, response_start_time=0.0, infiltrated_volume=0.0, initial_transit_water=0.0\n",
    "            )\n",
    "            self.active_states[node.basin_id] = initial_state; initialized_count += 1\n",
    "        if initialized_count == 0: logging.error(\"Initialization failed: Zero states created.\"); return False\n",
    "        self._log_state_and_balance(0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
    "        self.tree_snapshots.append((0.0, self._capture_tree_state()))\n",
    "        return True\n",
    "\n",
    "    def _add_newly_connected_areas(self, original_ids_to_connect: List[str]):\n",
    "        \"\"\"\n",
    "        Adds the area of original basins to the running total if they haven't been added before.\n",
    "        \"\"\"\n",
    "        for orig_id in original_ids_to_connect:\n",
    "            if orig_id not in self.connected_original_basin_ids:\n",
    "                # Find the node in the original forest to get its properties\n",
    "                node = self.forest.all_nodes.get(orig_id)\n",
    "                if node:\n",
    "                    self.connected_total_area += node.area\n",
    "                    self.connected_impervious_area += node.impervious_area\n",
    "                    self.connected_pervious_area += node.pervious_area\n",
    "                    self.connected_original_basin_ids.add(orig_id)\n",
    "                    logging.info(f\"Area of basin {orig_id} is now hydrologically connected.\")\n",
    "                else:\n",
    "                    logging.warning(f\"Could not find original node {orig_id} in forest to add its area.\")\n",
    "                    \n",
    "    # _capture_tree_state method\n",
    "    def _capture_tree_state(self) -> Dict:\n",
    "        snapshot: Dict[str, Dict] = {};\n",
    "        for b_id, st in self.active_states.items():\n",
    "            data: Dict[str, Union[float, bool, List[str], List[float]]] = {'area': st.current_area, 'impervious_area': st.impervious_area, 'pervious_area': st.pervious_area, 'current_volume': st.current_volume, 'max_volume': st.max_volume, 'infiltrated_volume': st.infiltrated_volume, 'is_merged': st.is_merged, 'effective_depth': st.effective_depth, 'parent_id': st.parent_id, 'spilled_volume': st.spilled_volume, 'merged_from': list(st.merged_from), 'is_inlet': any(orig in st.merged_from for orig in self.selected_inlet_ids), 'lag_time': st.lag_time}\n",
    "            rs = st.runoff_state or {}; data['convolution_timeline_min'] = rs.get('convolution_timeline_min', []); data['runon_infiltration_vol_step'] = rs.get('runon_infiltration_vol_step', [])\n",
    "            snapshot[b_id] = data\n",
    "        return snapshot\n",
    "\n",
    "    # _calculate_total_storage method\n",
    "    def _calculate_total_storage(self) -> float:\n",
    "        total_storage = sum(st.current_volume for st in self.active_states.values() if st and not st.is_merged)\n",
    "        return total_storage\n",
    "\n",
    "    # _log_state_and_balance method\n",
    "    def _log_state_and_balance(self, current_time_min: float, step_rainfall: float, step_initial_infiltration: float,\n",
    "                                 step_runon_infiltration: float, step_runoff_gen: float, step_discharge_vol: float):\n",
    "        storage_end_of_step = self._calculate_total_storage()\n",
    "        storage_change = storage_end_of_step - self.previous_total_storage\n",
    "        total_step_infiltration = step_initial_infiltration + step_runon_infiltration\n",
    "        expected_storage_change = step_runoff_gen - (step_discharge_vol + step_runon_infiltration)\n",
    "        internal_mass_balance_error = storage_change - expected_storage_change\n",
    "        overall_mass_balance_error = step_rainfall - total_step_infiltration - step_discharge_vol - storage_change\n",
    "        current_outlet_flow_m3_s = step_discharge_vol / self.dt_sec if self.dt_sec is not None and self.dt_sec > 0 else 0.0\n",
    "        \n",
    "        self.time_series.append({\n",
    "            'time': current_time_min, \n",
    "            'inlet_total_area': self.connected_total_area, \n",
    "            'inlet_impervious_area': self.connected_impervious_area, \n",
    "            'inlet_pervious_area': self.connected_pervious_area, \n",
    "            'inlet_effective_area': self.connected_impervious_area + self.connected_pervious_area, \n",
    "            'inlet_volume_rate': current_outlet_flow_m3_s, \n",
    "            'current_storage': storage_end_of_step\n",
    "        })\n",
    "        \n",
    "        self.water_balance.append({'time': current_time_min, 'total_rainfall': step_rainfall, 'initial_infiltration': step_initial_infiltration, 'runon_infiltration': step_runon_infiltration, 'total_infiltration': total_step_infiltration, 'active_runoff_generated': step_runoff_gen, 'inlet_discharge': step_discharge_vol, 'total_storage': storage_end_of_step, 'storage_change': storage_change, 'internal_mass_balance_error': internal_mass_balance_error, 'overall_mass_balance_error': overall_mass_balance_error})\n",
    "        self.previous_total_storage = storage_end_of_step\n",
    "        rel_error_threshold = 0.01; abs_error_threshold = 1e-4; check_value = abs(internal_mass_balance_error)\n",
    "        input_check = abs(step_runoff_gen) if abs(step_runoff_gen) > 1e-9 else abs(storage_change)\n",
    "        if check_value > abs_error_threshold:\n",
    "            is_relative_error_significant = (input_check > 1e-9 and (check_value / input_check) > rel_error_threshold)\n",
    "            if check_value > abs_error_threshold * 5 or is_relative_error_significant : logging.warning(f\"Internal Mass Balance Error at t={current_time_min:.2f} min: {internal_mass_balance_error: .6f} m³ (RunoffGen={step_runoff_gen:.4f}, RunonInf={step_runon_infiltration:.4f}, Disch={step_discharge_vol:.4f}, dS={storage_change:.4f})\")\n",
    "\n",
    "    def calculate_eia_by_arrival_at_inlet(self) -> Optional[pd.DataFrame]:\n",
    "            \"\"\"\n",
    "            Recalculates timeseries for Total, Impervious, and Pervious connected areas\n",
    "            based on the time runoff from each source basin physically arrives at the\n",
    "            final inlet basin. This is a post-simulation analysis.\n",
    "            CORRECTED: Now correctly handles no-spill scenarios, resulting in zero\n",
    "            area if no runoff is ever generated.\n",
    "            \"\"\"\n",
    "            logging.info(\"Recalculating all connected areas based on time of arrival at the final inlet basin...\")\n",
    "            if not self.time_series or not self.original_basin_ids_in_sim:\n",
    "                logging.error(\"Cannot calculate area by arrival: initial simulation data is missing.\")\n",
    "                return None\n",
    "\n",
    "            # This dictionary will store: {original_basin_id: arrival_time_at_inlet_min}\n",
    "            arrival_times = {}\n",
    "\n",
    "            # --- Step 1: For every original basin, find when its own runoff hydrograph\n",
    "            # starts ---\n",
    "            first_runoff_times = {}\n",
    "            for basin_id in self.original_basin_ids_in_sim:\n",
    "                runoff_data = self.runoff_processor.basin_runoff_data.get(basin_id)\n",
    "                if not runoff_data: continue\n",
    "\n",
    "                flow = np.array(runoff_data.get('direct_flow_m3_s', []))\n",
    "                times = np.array(runoff_data.get('convolution_timeline_min', []))\n",
    "\n",
    "                # Only consider basins that actually produced a meaningful amount of\n",
    "                # runoff\n",
    "                if flow.size > 0 and times.size > 0 and np.sum(flow) > 1e-9:\n",
    "                    first_flow_idx_list = np.where(flow > 1e-9)[0]\n",
    "                    if first_flow_idx_list.size > 0:\n",
    "                        first_runoff_times[basin_id] = times[first_flow_idx_list[0]]\n",
    "\n",
    "            # If no basins produced any runoff at all, we can stop and return zeros.\n",
    "            if not first_runoff_times:\n",
    "                logging.warning(\"No runoff was generated in any basin. Connected area will be zero for the entire simulation.\")\n",
    "                timeline = np.array([d['time'] for d in self.time_series])\n",
    "                return pd.DataFrame({\n",
    "                    'time': timeline,\n",
    "                    'total_area_by_arrival': np.zeros_like(timeline),\n",
    "                    'impervious_area_by_arrival': np.zeros_like(timeline),\n",
    "                    'pervious_area_by_arrival': np.zeros_like(timeline)\n",
    "                })\n",
    "\n",
    "            # --- Step 2: For each basin that DID produce runoff, find its total travel\n",
    "            # time to the inlet ---\n",
    "            for basin_id, start_runoff_time in first_runoff_times.items():\n",
    "                total_lag_min = 0.0\n",
    "                node = self.forest.all_nodes.get(basin_id)\n",
    "                \n",
    "                # Trace path from this basin up to the root of its tree\n",
    "                path_node = node\n",
    "                while path_node and path_node.parent:\n",
    "                    parent_id = path_node.parent.basin_id\n",
    "                    \n",
    "                    # Find the merge event where this path segment was the upstream\n",
    "                    # component\n",
    "                    for merge in self.merged_states_history:\n",
    "                        up_id = merge.get('upstream_id')\n",
    "                        down_id = merge.get('downstream_id')\n",
    "                        \n",
    "                        if not (up_id and down_id): continue\n",
    "\n",
    "                        # Find the original components of the upstream and downstream\n",
    "                        # parts of the merge\n",
    "                        up_data = self.runoff_processor.basin_runoff_data.get(up_id, {})\n",
    "                        down_data = self.runoff_processor.basin_runoff_data.get(down_id, {})\n",
    "                        up_components = up_data.get('merged_from', [up_id])\n",
    "                        down_components = down_data.get('merged_from', [down_id])\n",
    "\n",
    "                        # If our current node is in the upstream part and its parent is\n",
    "                        # in the downstream part...\n",
    "                        if path_node.basin_id in up_components and parent_id in down_components:\n",
    "                            # ...add the lag time for this step and break from the merge\n",
    "                            # history loop.\n",
    "                            total_lag_min += merge.get('lag_calculated_sec', 0.0) / 60.0\n",
    "                            break\n",
    "                            \n",
    "                    path_node = path_node.parent\n",
    "\n",
    "                # The final arrival time is when its own runoff starts, plus all travel\n",
    "                # delays.\n",
    "                arrival_times[basin_id] = start_runoff_time + total_lag_min\n",
    "\n",
    "            # --- Step 3: Build the new area timeseries using the calculated arrival\n",
    "            # times ---\n",
    "            timeline = np.array([d['time'] for d in self.time_series])\n",
    "            total_area_series = np.zeros_like(timeline, dtype=float)\n",
    "            imperv_area_series = np.zeros_like(timeline, dtype=float)\n",
    "            perv_area_series = np.zeros_like(timeline, dtype=float)\n",
    "\n",
    "            for i, t in enumerate(timeline):\n",
    "                current_total = 0.0\n",
    "                current_imperv = 0.0\n",
    "                current_perv = 0.0\n",
    "                for basin_id, arrival_time in arrival_times.items():\n",
    "                    if t >= arrival_time:\n",
    "                        node = self.forest.all_nodes.get(basin_id)\n",
    "                        if node:\n",
    "                            current_total += node.area\n",
    "                            current_imperv += node.impervious_area\n",
    "                            current_perv += node.pervious_area\n",
    "                total_area_series[i] = current_total\n",
    "                imperv_area_series[i] = current_imperv\n",
    "                perv_area_series[i] = current_perv\n",
    "\n",
    "            return pd.DataFrame({\n",
    "                'time': timeline, \n",
    "                'total_area_by_arrival': total_area_series,\n",
    "                'impervious_area_by_arrival': imperv_area_series,\n",
    "                'pervious_area_by_arrival': perv_area_series\n",
    "            })\n",
    "\n",
    "    # _merge_basins_lag_route method (MODIFIED TO STORE PENDING RUNON INFO)\n",
    "    def _merge_basins_lag_route(self, up_id: str, down_id: str) -> bool:\n",
    "        \"\"\"Merges basins and UPDATES the dynamically tracked connected area.\"\"\"\n",
    "        up_state = self.active_states.get(up_id); down_state = self.active_states.get(down_id)\n",
    "        if not up_state or not down_state: logging.error(f\"Merge Error: State not found for {up_id} or {down_id}.\"); return False\n",
    "        if up_state.is_merged or down_state.is_merged: logging.warning(f\"Merge attempt skipped: {up_id} or {down_id} already merged.\"); return False\n",
    "        \n",
    "        # NEW: Update connected areas BEFORE the merge calculation\n",
    "        # The upstream state's original basins are now connected.\n",
    "        self._add_newly_connected_areas(up_state.merged_from)\n",
    "        # The downstream state's original basins are also now officially connected.\n",
    "        self._add_newly_connected_areas(down_state.merged_from)\n",
    "        # End New\n",
    "\n",
    "        current_time_sec = self.time_series[-1]['time'] * 60.0 if self.time_series else 0.0\n",
    "        current_merge_time_min = current_time_sec / 60.0\n",
    "\n",
    "        upstream_volume_to_transfer = min(up_state.current_volume, up_state.max_volume)\n",
    "        initial_merged_volume = upstream_volume_to_transfer + down_state.current_volume\n",
    "\n",
    "        rainfall_csv_path = getattr(self.runoff_processor, 'rainfall_csv', None)\n",
    "        rain_unit_val = getattr(self.runoff_processor, 'rain_unit', None)\n",
    "        p2_val = getattr(self.runoff_processor, 'P2_in', None)\n",
    "        if rainfall_csv_path is None or rain_unit_val is None or p2_val is None:\n",
    "            logging.error(\"Merge Error: Missing params in RunoffProcessor.\"); self.halt_reason = \"Missing params for merge.\"; return False\n",
    "\n",
    "        merged_runoff_data = self.runoff_processor.calculate_merged_basin_runoff(\n",
    "            upstream_id=up_id, downstream_id=down_id, current_time_sec=current_time_sec, timestep_min=self.timestep_min,\n",
    "            sim_time_min=self.simulation_time_min, rainfall_csv=rainfall_csv_path, rain_unit=rain_unit_val, P2_in=p2_val\n",
    "        )\n",
    "\n",
    "        if merged_runoff_data is None: self.halt_reason = f\"Merge hydrograph calc failed for {up_id}->{down_id}\"; logging.critical(f\"CRITICAL HALT: {self.halt_reason}\"); return False\n",
    "\n",
    "        merged_id = merged_runoff_data['basin_id']\n",
    "\n",
    "        new_area = up_state.current_area + down_state.current_area\n",
    "        new_max_volume = up_state.max_volume + down_state.max_volume\n",
    "        new_imperv_area = up_state.impervious_area + down_state.impervious_area\n",
    "        new_perv_area = up_state.pervious_area + down_state.pervious_area\n",
    "        new_merged_state = BasinState(\n",
    "            basin_id=merged_id, current_area=new_area, impervious_area=new_imperv_area, pervious_area=new_perv_area,\n",
    "            max_volume=new_max_volume, current_volume=initial_merged_volume, effective_depth=new_max_volume / new_area if new_area > 1e-6 else 0,\n",
    "            alpha=1.0, parent_id=down_state.parent_id, merged_from=sorted(list(set(down_state.merged_from + up_state.merged_from))),\n",
    "            is_merged=False, spilled_volume=0.0, runoff_state=merged_runoff_data, lag_time=merged_runoff_data.get('lag_used_sec', 0.0),\n",
    "            response_start_time=current_time_sec, infiltrated_volume=(up_state.infiltrated_volume + down_state.infiltrated_volume), initial_transit_water=0.0\n",
    "        )\n",
    "\n",
    "        up_state.is_merged = True; down_state.is_merged = True\n",
    "        self.active_states[merged_id] = new_merged_state\n",
    "        if up_id in self.active_states: del self.active_states[up_id]\n",
    "        if down_id in self.active_states: del self.active_states[down_id]\n",
    "\n",
    "        children_to_update = [cid for cid, cstate in self.active_states.items() if cstate.parent_id == up_id or cstate.parent_id == down_id]\n",
    "        for child_id in children_to_update:\n",
    "                if child_id in self.active_states: self.active_states[child_id].parent_id = merged_id\n",
    "\n",
    "        calculated_lag_sec = merged_runoff_data.get('lag_used_sec', 0.0)\n",
    "        self.merged_states_history.append({'time_min': current_merge_time_min, 'upstream_id': up_id, 'downstream_id': down_id, 'merged_id': merged_id, 'lag_calculated_sec': calculated_lag_sec})\n",
    "\n",
    "        total_runon_for_this_merge = merged_runoff_data.get('total_runon_infiltration_calculated', 0.0)\n",
    "        if isinstance(total_runon_for_this_merge, (int, float)) and total_runon_for_this_merge > 1e-9:\n",
    "            arrival_time_min = current_merge_time_min + (calculated_lag_sec / 60.0)\n",
    "            self.pending_runon[merged_id] = (total_runon_for_this_merge, self.RUNON_DISTRIBUTION_STEPS, arrival_time_min)\n",
    "            logging.debug(f\"  Stored {total_runon_for_this_merge:.6f} m³ run-on from {merged_id} to be distributed over {self.RUNON_DISTRIBUTION_STEPS} steps starting after t={arrival_time_min:.2f} min.\")\n",
    "        return True\n",
    "\n",
    "    # RECOMMENDED VERSION for IntegratedSimulation\n",
    "    def run_simulation(self):\n",
    "        \"\"\"Runs the fill-and-spill simulation. Uses TOTAL run-on stored from merge events,\n",
    "            logged immediately in the merge timestep for mass balance.\n",
    "            (No diagnostic print/debug lines).\"\"\"\n",
    "        if not self.initialization_successful: logging.error(\"Sim init failed.\"); return self.time_series, self.water_balance, self.tree_snapshots\n",
    "        logging.info(\"--- Starting Integrated Simulation Run ---\")\n",
    "        if (self.simulation_time_min is None or self.timestep_min is None or self.timestep_min <= 0 or self.dt_sec is None or self.dt_sec <= 0):\n",
    "            logging.error(\"Invalid sim time/step.\"); self.halted_prematurely = True; self.halt_reason = \"Invalid time/step.\"; return self.time_series, self.water_balance, self.tree_snapshots\n",
    "        timeline_mins = np.arange(0, self.simulation_time_min + self.timestep_min, self.timestep_min); timeline_mins = timeline_mins[timeline_mins <= self.simulation_time_min + 1e-9]; num_steps = len(timeline_mins) - 1\n",
    "        if num_steps <= 0: logging.error(\"Zero steps.\"); self.halted_prematurely = True; self.halt_reason = \"Zero steps.\"; return self.time_series, self.water_balance, self.tree_snapshots\n",
    "        logging.info(f\"Total simulation steps: {num_steps}\")\n",
    "\n",
    "        # Main loop\n",
    "        for i in range(num_steps):\n",
    "            current_time_min = timeline_mins[i]; next_time_min = timeline_mins[i+1]; current_time_sec = current_time_min * 60.0\n",
    "            logging.info(f\"--- Sim Step {i+1}/{num_steps} (Time: {current_time_min:.2f} -> {next_time_min:.2f} min) ---\")\n",
    "\n",
    "            # 1: Rainfall and initial infiltration\n",
    "            step_rain_vol = 0.0; step_init_inf = 0.0\n",
    "            for oid in self.original_basin_ids_in_sim:\n",
    "                rd = self.runoff_processor.basin_runoff_data.get(oid);\n",
    "                if not rd: continue\n",
    "                times = np.array(rd.get('timeline_min', [])); rain_arr = np.array(rd.get('total_rainfall_vol_step', [])); inf_arr = np.array(rd.get('infiltration_vol_step', []))\n",
    "                if times.size == 0 or rain_arr.size != times.size or inf_arr.size != times.size: continue\n",
    "                si = int(np.searchsorted(times, current_time_min - 1e-9, side='left')); ei = int(np.searchsorted(times, next_time_min - 1e-9, side='left'))\n",
    "                if si < ei and ei <= len(rain_arr): step_rain_vol += np.sum(rain_arr[si:ei]); step_init_inf += np.sum(inf_arr[si:ei])\n",
    "\n",
    "            # 2: Runoff inflow to storage\n",
    "            step_runoff_gen = 0.0\n",
    "            active_ids_before_inflow = list(self.active_states.keys())\n",
    "            for bid in active_ids_before_inflow:\n",
    "                if bid not in self.active_states: continue\n",
    "                state = self.active_states[bid]; vol_in = self.runoff_processor.get_volume_in_timespan(bid, current_time_min, next_time_min)\n",
    "                state.current_volume += vol_in; step_runoff_gen += vol_in\n",
    "\n",
    "            # 3: Merges at current time\n",
    "            merged_ids_this_step = [] # Track NEWLY created merge IDs this step\n",
    "            merge_occurred = False; check_for_merges = True; merge_iteration = 0; max_merge_iterations = len(self.active_states) + 5\n",
    "            while check_for_merges and merge_iteration < max_merge_iterations:\n",
    "                    merge_iteration += 1; found_merge_in_iteration = False\n",
    "                    active_ids_before_iteration = list(self.active_states.keys())\n",
    "                    for bid in active_ids_before_iteration:\n",
    "                        if bid not in self.active_states: continue\n",
    "                        state = self.active_states[bid]\n",
    "                        if state.current_volume > state.max_volume + 1e-9 and state.parent_id:\n",
    "                            parent_id = state.parent_id\n",
    "                            if parent_id in self.active_states:\n",
    "                                success = self._merge_basins_lag_route(bid, parent_id)\n",
    "                                if success:\n",
    "                                    merge_occurred = True; found_merge_in_iteration = True; merged_id_created = f\"{parent_id}+{bid}\"\n",
    "                                    if merged_id_created in self.active_states:\n",
    "                                        if merged_id_created not in merged_ids_this_step: merged_ids_this_step.append(merged_id_created)\n",
    "                                    else: self.halted_prematurely = True; self.halt_reason = f\"Merged state {merged_id_created} missing\"; break\n",
    "                                else: self.halted_prematurely = True; self.halt_reason = f\"Merge failed {bid}->{parent_id}\"; break\n",
    "                    if self.halted_prematurely: break\n",
    "                    if not found_merge_in_iteration: check_for_merges = False\n",
    "            if merge_iteration >= max_merge_iterations: self.halted_prematurely = True; self.halt_reason = \"Max merge iterations reached.\"\n",
    "            if self.halted_prematurely: break\n",
    "            if merge_occurred: self.tree_snapshots.append((current_time_sec, self._capture_tree_state()))\n",
    "\n",
    "            # 4: Run-on infiltration calculation (Summing Stored TOTALS)\n",
    "            step_runon_inf = 0.0\n",
    "            if merged_ids_this_step: # Only process NEW merges created in this step\n",
    "                for mid in merged_ids_this_step:\n",
    "                    ms = self.active_states.get(mid)\n",
    "                    if not ms or not ms.runoff_state:\n",
    "                        logging.warning(f\"State/runoff_state missing for {mid} when summing total runon.\")\n",
    "                        continue\n",
    "                    # Get the TOTAL run-on calculated and stored during the merge\n",
    "                    total_runon_for_this_merge = ms.runoff_state.get('total_runon_infiltration_calculated', 0.0) # Default to 0\n",
    "                    if not isinstance(total_runon_for_this_merge, (int, float)):\n",
    "                        logging.warning(f\"Invalid type stored total runon for {mid}.\")\n",
    "                        total_runon_for_this_merge = 0.0\n",
    "                    step_runon_inf += total_runon_for_this_merge # Add the total lump sum\n",
    "\n",
    "            # 5: Discharge from outlet basins\n",
    "            step_discharge = 0.0\n",
    "            active_ids_after_merge = list(self.active_states.keys())\n",
    "            for bid in active_ids_after_merge:\n",
    "                    if bid not in self.active_states: continue\n",
    "                    state = self.active_states[bid]; is_outlet_path = any(orig_inlet in state.merged_from for orig_inlet in self.selected_inlet_ids)\n",
    "                    if is_outlet_path:\n",
    "                        discharge_potential_vol = self.runoff_processor.get_volume_in_timespan(bid, current_time_min, next_time_min)\n",
    "                        actual_discharge_vol = min(discharge_potential_vol, state.current_volume); actual_discharge_vol = max(0.0, actual_discharge_vol)\n",
    "                        state.current_volume -= actual_discharge_vol; step_discharge += actual_discharge_vol\n",
    "\n",
    "            # 6: Log state and mass balance\n",
    "            self._log_state_and_balance(\n",
    "                next_time_min, step_rain_vol, step_init_inf,\n",
    "                step_runon_inf, # Log the total \"lump sum\" calculated this step\n",
    "                step_runoff_gen, step_discharge\n",
    "            )\n",
    "\n",
    "            if self.halted_prematurely:\n",
    "                logging.warning(f\"--- Halting simulation loop at step {i+1} due to: {self.halt_reason} ---\")\n",
    "                break\n",
    "\n",
    "        # Final Snapshot Logic\n",
    "        if not self.halted_prematurely:\n",
    "            final_sim_time_min = timeline_mins[-1]; final_sec = final_sim_time_min * 60.0\n",
    "            if not self.tree_snapshots or abs(self.tree_snapshots[-1][0] - final_sec) > 1e-6:\n",
    "                    self.tree_snapshots.append((final_sec, self._capture_tree_state()))\n",
    "\n",
    "        logging.info(f\"--- Simulation Run Finished. Halted Prematurely: {self.halted_prematurely} ---\")\n",
    "        return self.time_series, self.water_balance, self.tree_snapshots\n",
    "# 6) Visualization Functions\n",
    "\n",
    "def plot_basin_runoff_stages(basin_runoff_data: Optional[Dict],\n",
    "                             simulation_time_min: float, # Required Parameter\n",
    "                             rain_unit_pref: str = 'cm/hr'):\n",
    "    \"\"\"\n",
    "    Generates a 5-panel plot visualizing rainfall-runoff stages for a single basin.\n",
    "    Applies formatting and COLOR scheme consistent with other plots.\n",
    "    \"\"\"\n",
    "    if not basin_runoff_data:\n",
    "        logging.warning(\"No basin runoff data provided for plotting stages.\")\n",
    "        print(\"No basin runoff data provided for plotting stages.\")\n",
    "        return\n",
    "\n",
    "    basin_id = basin_runoff_data.get('basin_id', 'N/A')\n",
    "    logging.info(f\"Generating formatted runoff stage plots for Basin ID: {basin_id}\")\n",
    "\n",
    "    # Define Font Sizes\n",
    "    base_fontsize = 10\n",
    "    fontsize_title = base_fontsize + 4\n",
    "    fontsize_subtitle = base_fontsize + 2\n",
    "    fontsize_label = base_fontsize + 1\n",
    "    fontsize_legend = base_fontsize\n",
    "    fontsize_tick = base_fontsize\n",
    "\n",
    "    # Define Color Palette (Consistent with other plots)\n",
    "    colors = {\n",
    "        'Rainfall': '#a6cee3',\n",
    "        'Excess Rainfall': '#e31a1c',\n",
    "        'Runoff Hydrograph': '#2ca02c',\n",
    "        'Available Capacity': '#fdbf6f',\n",
    "        'Infiltration Volume': '#ff7f0e',\n",
    "        'Max Infil Line': '#e31a1c',\n",
    "        'Effective Max Line': '#ff7f0e'\n",
    "    }\n",
    "    # Define alphas separately for fills\n",
    "    alphas = {\n",
    "        'Rainfall': 0.6,\n",
    "        'Excess Rainfall': 0.6,\n",
    "        'Runoff Hydrograph': 0.6,\n",
    "        'Available Capacity': 0.6,\n",
    "        'Infiltration Volume': 0.5\n",
    "    }\n",
    "\n",
    "    # Extract Data & Validate\n",
    "    try:\n",
    "        timeline_min = np.array(basin_runoff_data['timeline_min']); conv_timeline_min = np.array(basin_runoff_data['convolution_timeline_min'])\n",
    "        rain_depth_m_step = np.array(basin_runoff_data['rain_depth_m_step']); simplified_excess_depth_m_step = np.array(basin_runoff_data.get('simplified_excess_depth_m_step', np.zeros_like(timeline_min)))\n",
    "        direct_flow_m3_s = np.array(basin_runoff_data['direct_flow_m3_s']); deficit_volume_step = np.array(basin_runoff_data.get('infiltration_deficit_vol_step', np.zeros_like(timeline_min)))\n",
    "        infiltration_vol_step = np.array(basin_runoff_data.get('infiltration_vol_step', np.zeros_like(timeline_min))) # Get actual infiltration\n",
    "        dt_sec = basin_runoff_data['dt_sec']; basin_area = basin_runoff_data['basin_area']\n",
    "        perv_area_eff = basin_runoff_data.get('perv_area_used_for_infiltration', basin_runoff_data.get('pervious_area', 0)) # Safer get\n",
    "        total_runoff_volume = basin_runoff_data.get('total_direct_runoff_vol', 0.0)\n",
    "        total_actual_infiltration = basin_runoff_data.get('total_infiltration_vol', 0.0)\n",
    "        if 'INFILTRATION_RATE_CM_S' not in globals(): raise NameError(\"Global INFILTRATION_RATE_CM_S not defined.\")\n",
    "        input_infiltration_rate_cm_s = globals()['INFILTRATION_RATE_CM_S']\n",
    "        if dt_sec <= 0: raise ValueError(\"dt_sec must be positive.\")\n",
    "        if basin_area <= 1e-9: basin_area = 1e-9 # Avoid division by zero but proceed\n",
    "        if len(timeline_min) == 0: raise ValueError(\"Input timeline empty.\")\n",
    "        # Ensure lengths match or handle gracefully\n",
    "        if len(rain_depth_m_step) != len(timeline_min): rain_depth_m_step = np.zeros_like(timeline_min); logging.warning(f\"{basin_id}: Rain depth length mismatch.\")\n",
    "        if len(simplified_excess_depth_m_step) != len(timeline_min): simplified_excess_depth_m_step = np.zeros_like(timeline_min); logging.warning(f\"{basin_id}: Simplified excess length mismatch.\")\n",
    "        if len(deficit_volume_step) != len(timeline_min): deficit_volume_step = np.zeros_like(timeline_min); logging.warning(f\"{basin_id}: Deficit volume length mismatch.\")\n",
    "        if len(infiltration_vol_step) != len(timeline_min): infiltration_vol_step = np.zeros_like(timeline_min); logging.warning(f\"{basin_id}: Infiltration volume length mismatch.\")\n",
    "        if len(conv_timeline_min) != len(direct_flow_m3_s): logging.warning(f\"{basin_id}: Convolution timeline/flow mismatch.\"); conv_timeline_min=np.array([]); direct_flow_m3_s=np.array([])\n",
    "    except KeyError as ke: logging.error(f\"Plot Stages {basin_id}: Missing key {ke}\"); traceback.print_exc(); return\n",
    "    except Exception as e: logging.error(f\"Plot Stages {basin_id}: Data extraction error: {e}\"); traceback.print_exc(); return\n",
    "\n",
    "    # Calculations for Plotting\n",
    "    try:\n",
    "        rain_rate_m_s = np.divide(rain_depth_m_step, dt_sec, out=np.zeros_like(rain_depth_m_step), where=dt_sec!=0)\n",
    "        simplified_excess_rate_m_s = np.divide(simplified_excess_depth_m_step, dt_sec, out=np.zeros_like(simplified_excess_depth_m_step), where=dt_sec!=0)\n",
    "        max_infil_rate_on_perv_m_s = input_infiltration_rate_cm_s * 0.01\n",
    "        effective_max_infil_rate_m_s = (max_infil_rate_on_perv_m_s * perv_area_eff / basin_area) if basin_area > 1e-9 else 0.0\n",
    "        deficit_rate_m_s = np.divide(deficit_volume_step, (basin_area * dt_sec), out=np.zeros_like(deficit_volume_step), where=(basin_area*dt_sec)!=0)\n",
    "        unit_factor=1.0; rate_unit_label='m/s'\n",
    "        if rain_unit_pref=='cm/hr': unit_factor=360000; rate_unit_label='cm/hr' # 100cm/m * 3600s/hr\n",
    "        elif rain_unit_pref=='mm/hr': unit_factor=3600000; rate_unit_label='mm/hr' # 1000mm/m * 3600s/hr\n",
    "        elif rain_unit_pref=='in/hr': unit_factor=(1/0.0254)*3600; rate_unit_label='in/hr'\n",
    "        else: rate_unit_label='m/s'\n",
    "        rain_rate_plot = rain_rate_m_s * unit_factor\n",
    "        simplified_excess_rate_plot = simplified_excess_rate_m_s * unit_factor\n",
    "        orig_infil_rate_plot = max_infil_rate_on_perv_m_s * unit_factor\n",
    "        eff_infil_rate_plot = effective_max_infil_rate_m_s * unit_factor\n",
    "        deficit_rate_plot = deficit_rate_m_s * unit_factor\n",
    "        runoff_response_m3_min = direct_flow_m3_s * 60.0\n",
    "    except Exception as e: logging.error(f\"Plot Stages {basin_id}: Calculation error: {e}\"); traceback.print_exc(); return\n",
    "\n",
    "    # Create Figure & Plot\n",
    "    try:\n",
    "        fig, axes = plt.subplots(5, 1, figsize=(10, 15), sharex=True)\n",
    "        fig.suptitle(f'Hydrologic Processes for Basin: {basin_id}', fontsize=fontsize_title, y=0.99)\n",
    "\n",
    "        legend_props = {'loc': 'upper right', 'fontsize': fontsize_legend, 'frameon': True, 'facecolor': 'white', 'edgecolor': 'black'}\n",
    "        legend_frame_lw = 0.8\n",
    "        fill_edge_color = 'black'; fill_edge_lw = 0.5\n",
    "\n",
    "        # Plot 1 (Rainfall & Capacity) - Use Light Blue for Rainfall\n",
    "        ax1 = axes[0]; handles1 = []\n",
    "        line_rain, = ax1.plot(timeline_min, rain_rate_plot, color=colors['Rainfall'], drawstyle='steps-post', lw=1.0) # Thinner line\n",
    "        fill_rain = ax1.fill_between(timeline_min, 0, rain_rate_plot, color=colors['Rainfall'], alpha=alphas['Rainfall'], step='post', edgecolor=fill_edge_color, lw=fill_edge_lw)\n",
    "        line_max_infil = ax1.axhline(orig_infil_rate_plot, color=colors['Max Infil Line'], linestyle='--', label=f'Max Infil. Rate ({orig_infil_rate_plot:.2f} {rate_unit_label})')\n",
    "        line_eff_infil = ax1.axhline(eff_infil_rate_plot, color=colors['Effective Max Line'], linestyle=':', linewidth=2, label=f'Effective Max Rate ({eff_infil_rate_plot:.2f} {rate_unit_label})')\n",
    "        handles1.append(mpatches.Patch(facecolor=colors['Rainfall'], alpha=alphas['Rainfall'], edgecolor=fill_edge_color, label='Rainfall Rate'))\n",
    "        handles1.append(line_max_infil); handles1.append(line_eff_infil)\n",
    "        ax1.set_ylabel(f'Rate ({rate_unit_label})', fontsize=fontsize_label); ax1.set_title('1. Rainfall & Infiltration Capacity', fontsize=fontsize_subtitle)\n",
    "        ax1.legend(handles=handles1, **legend_props).get_frame().set_linewidth(legend_frame_lw); ax1.grid(False)\n",
    "        ax1.tick_params(axis='both', labelsize=fontsize_tick); ax1.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.2f'))\n",
    "\n",
    "        # Plot 2 (Excess Rate) - Use Red\n",
    "        ax2 = axes[1]; handles2 = []\n",
    "        line_excess, = ax2.plot(timeline_min, simplified_excess_rate_plot, color=colors['Excess Rainfall'], drawstyle='steps-post', lw=1.0)\n",
    "        fill_excess = ax2.fill_between(timeline_min, 0, simplified_excess_rate_plot, color=colors['Excess Rainfall'], alpha=alphas['Excess Rainfall'], step='post', edgecolor=fill_edge_color, lw=fill_edge_lw)\n",
    "        handles2.append(mpatches.Patch(facecolor=colors['Excess Rainfall'], alpha=alphas['Excess Rainfall'], edgecolor=fill_edge_color, label='Excess Rate (Simplified)'))\n",
    "        ax2.set_ylabel(f'Rate ({rate_unit_label})', fontsize=fontsize_label); ax2.set_title('2. Excess Rainfall Rate', fontsize=fontsize_subtitle)\n",
    "        ax2.legend(handles=handles2, **legend_props).get_frame().set_linewidth(legend_frame_lw); ax2.grid(False)\n",
    "        ax2.tick_params(axis='both', labelsize=fontsize_tick); ax2.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.2f'))\n",
    "\n",
    "        # Plot 3 (Runoff Response) - Use Green\n",
    "        ax3 = axes[2]; handles3 = []\n",
    "        if len(conv_timeline_min) > 0 and len(runoff_response_m3_min) > 0:\n",
    "            line_runoff, = ax3.plot(conv_timeline_min, runoff_response_m3_min, color=colors['Runoff Hydrograph'], drawstyle='steps-post', lw=1.5)\n",
    "            fill_runoff = ax3.fill_between(conv_timeline_min, 0, runoff_response_m3_min, color=colors['Runoff Hydrograph'], alpha=alphas['Runoff Hydrograph'], step='post', edgecolor=fill_edge_color, lw=fill_edge_lw)\n",
    "            handles3.append(mpatches.Patch(facecolor=colors['Runoff Hydrograph'], alpha=alphas['Runoff Hydrograph'], edgecolor=fill_edge_color, label=f'Surface Runoff (Vol: {total_runoff_volume:.3f} m³)'))\n",
    "        else: ax3.text(0.5, 0.5, \"No runoff data\", ha='center', va='center', transform=ax3.transAxes, fontsize=fontsize_tick)\n",
    "        ax3.set_ylabel('Flow Rate (m³/min)', fontsize=fontsize_label); ax3.set_title('3. Surface Runoff Hydrograph', fontsize=fontsize_subtitle)\n",
    "        ax3.legend(handles=handles3, **legend_props).get_frame().set_linewidth(legend_frame_lw); ax3.grid(False)\n",
    "        ax3.tick_params(axis='both', labelsize=fontsize_tick); ax3.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.3f'))\n",
    "\n",
    "        # Plot 4 (Available Capacity Rate) - Use Yellow/Light Orange\n",
    "        ax4 = axes[3]; handles4 = []\n",
    "        line_def_rate, = ax4.plot(timeline_min, deficit_rate_plot, color=colors['Available Capacity'], drawstyle='steps-post', lw=1.0)\n",
    "        fill_def_rate = ax4.fill_between(timeline_min, 0, deficit_rate_plot, color=colors['Available Capacity'], alpha=alphas['Available Capacity'], step='post', edgecolor=fill_edge_color, lw=fill_edge_lw)\n",
    "        handles4.append(mpatches.Patch(facecolor=colors['Available Capacity'], alpha=alphas['Available Capacity'], edgecolor=fill_edge_color, label='Available Capacity Rate'))\n",
    "        line_eff_infil4 = ax4.axhline(eff_infil_rate_plot, color=colors['Effective Max Line'], linestyle=':', linewidth=2, label=f'Effective Max Rate ({eff_infil_rate_plot:.2f} {rate_unit_label})')\n",
    "        handles4.append(line_eff_infil4)\n",
    "        ax4.set_ylabel(f'Rate ({rate_unit_label})', fontsize=fontsize_label); ax4.set_title('4. Available Infiltration Capacity Rate', fontsize=fontsize_subtitle)\n",
    "        ax4.legend(handles=handles4, **legend_props).get_frame().set_linewidth(legend_frame_lw); ax4.grid(False)\n",
    "        ax4.tick_params(axis='both', labelsize=fontsize_tick); ax4.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.2f'))\n",
    "\n",
    "        # Plot 5 (Actual Infiltration Volume) - Use Orange\n",
    "        ax5 = axes[4]; handles5 = []\n",
    "        line_inf_vol, = ax5.plot(timeline_min, infiltration_vol_step, color=colors['Infiltration Volume'], drawstyle='steps-post', lw=1.0)\n",
    "        fill_inf_vol = ax5.fill_between(timeline_min, 0, infiltration_vol_step, color=colors['Infiltration Volume'], alpha=alphas['Infiltration Volume'], step='post', edgecolor=fill_edge_color, lw=fill_edge_lw)\n",
    "        handles5.append(mpatches.Patch(facecolor=colors['Infiltration Volume'], alpha=alphas['Infiltration Volume'], edgecolor=fill_edge_color, label=f'Infiltration Volume (Total: {total_actual_infiltration:.3f} m³)'))\n",
    "        ax5.set_xlabel(f'Time (min)', fontsize=fontsize_label); ax5.set_ylabel('Volume (m³ / step)', fontsize=fontsize_label)\n",
    "        ax5.set_title('5. Actual Infiltration Volume per Timestep', fontsize=fontsize_subtitle)\n",
    "        ax5.legend(handles=handles5, **legend_props).get_frame().set_linewidth(legend_frame_lw); ax5.grid(False)\n",
    "        ax5.tick_params(axis='both', labelsize=fontsize_tick); ax5.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.4f'))\n",
    "\n",
    "        # Final Formatting\n",
    "        axes[-1].set_xlim(0, simulation_time_min) # Set limit on last axis\n",
    "\n",
    "        # Synchronize Rate Axes (1, 2, 4)\n",
    "        max_y_rate = 0.01 # Min value\n",
    "        if handles1: max_y_rate = max(max_y_rate, ax1.get_ylim()[1])\n",
    "        if handles2: max_y_rate = max(max_y_rate, ax2.get_ylim()[1])\n",
    "        if handles4: max_y_rate = max(max_y_rate, ax4.get_ylim()[1])\n",
    "        ax1.set_ylim(0, max_y_rate); ax2.set_ylim(0, max_y_rate); ax4.set_ylim(0, max_y_rate)\n",
    "\n",
    "        # Auto-scale flow axis (3) and volume axis (5)\n",
    "        if len(runoff_response_m3_min) > 0: ax3.set_ylim(0, np.max(runoff_response_m3_min) * 1.1 if np.max(runoff_response_m3_min) > 0 else 0.001)\n",
    "        else: ax3.set_ylim(0, 0.001)\n",
    "        if len(infiltration_vol_step) > 0: ax5.set_ylim(0, np.max(infiltration_vol_step) * 1.1 if np.max(infiltration_vol_step) > 0 else 1e-5)\n",
    "        else: ax5.set_ylim(0, 1e-5)\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed during plotting setup/execution for {basin_id}: {e}\"); traceback.print_exc()\n",
    "        if 'fig' in locals() and fig is not None and plt.fignum_exists(fig.number): plt.close(fig)\n",
    "\n",
    "def _style_legend_and_place_text(ax, fig, text_str, loc='upper right'):\n",
    "    leg = ax.legend(loc=loc, facecolor='white', edgecolor='black', fontsize=9, frameon=True)\n",
    "    if leg:\n",
    "        leg.get_frame().set_linewidth(0.8); leg.get_frame().set_edgecolor('black')\n",
    "        try:\n",
    "            fig.canvas.draw_idle()\n",
    "            bbox = leg.get_window_extent(renderer=fig.canvas.get_renderer())\n",
    "            bbox_data = bbox.transformed(ax.transAxes.inverted())\n",
    "            x_text = bbox_data.x0; y_text = bbox_data.y0 - 0.02\n",
    "            ha = 'left'; va = 'top'\n",
    "        except Exception:\n",
    "            logging.debug(\"Could not get legend extent, placing text at default position.\")\n",
    "            x_text, y_text, ha, va = 0.98, 0.02, 'right', 'bottom'\n",
    "        ax.text(x_text, y_text, text_str, transform=ax.transAxes, ha=ha, va=va, fontsize=9, bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=0.2))\n",
    "    else:\n",
    "        ax.text(0.98, 0.02, text_str, transform=ax.transAxes, ha='right', va='bottom', fontsize=9, bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=0.2))\n",
    "\n",
    "def build_hierarchical_layout(current_state: Dict[str, Dict], original_inlet_ids: List[str]) -> Dict[str, Tuple[float, float]]:\n",
    "    \"\"\"\n",
    "    Computes node positions for a hierarchical tree-like layout.\n",
    "    Args:\n",
    "        current_state: A dictionary where keys are node IDs and values are dictionaries\n",
    "                        containing at least a 'parent_id' key.\n",
    "        original_inlet_ids: List of original inlet IDs (used if needed, though current logic focuses on parent_id links).\n",
    "    Returns:\n",
    "        A dictionary mapping node IDs to (x, y) coordinates.\n",
    "    \"\"\"\n",
    "    active_nodes = list(current_state.keys())\n",
    "    logging.debug(f\"[Layout Build] Attempting layout for {len(active_nodes)} active nodes: {active_nodes}\")\n",
    "    if not active_nodes:\n",
    "        logging.warning(\"[Layout Build] No active nodes provided to build_hierarchical_layout. Returning empty positions.\")\n",
    "        return {}\n",
    "\n",
    "    adj = {nid: [] for nid in active_nodes}\n",
    "    parent_map = {nid: current_state[nid].get('parent_id') for nid in active_nodes}\n",
    "    logging.debug(f\"[Layout Build] Parent map generated: {parent_map}\")\n",
    "\n",
    "    roots = []\n",
    "    node_has_parent_in_graph = set()\n",
    "    for nid in active_nodes:\n",
    "        pid = parent_map.get(nid)\n",
    "        if pid and pid in active_nodes: # Parent is also in the current snapshot's active set\n",
    "            adj[pid].append(nid)\n",
    "            node_has_parent_in_graph.add(nid)\n",
    "\n",
    "    for nid in active_nodes:\n",
    "        # A root is a node whose parent is None, or whose parent is not in the current\n",
    "        # set of active_nodes\n",
    "        if parent_map.get(nid) is None or parent_map.get(nid) not in active_nodes :\n",
    "            roots.append(nid)\n",
    "            \n",
    "    # If no roots were found by the above logic (e.g. a cycle involving all nodes, or\n",
    "    # all have parents within the set)\n",
    "    # and there are active_nodes, pick the first active node as a fallback root.\n",
    "    # Also, ensure that any node identified as an original inlet that doesn't have a\n",
    "    # parent in the graph is treated as a root.\n",
    "    explicit_roots_from_inlets = [in_id for in_id in original_inlet_ids if in_id in active_nodes and (parent_map.get(in_id) is None or parent_map.get(in_id) not in active_nodes)]\n",
    "    if explicit_roots_from_inlets:\n",
    "        roots = sorted(list(set(roots + explicit_roots_from_inlets))) # Combine and unique\n",
    "\n",
    "    logging.debug(f\"[Layout Build] Initial roots based on parent_map: {roots}\")\n",
    "\n",
    "    if not roots and active_nodes:\n",
    "        roots = [active_nodes[0]] # Fallback if no other roots found\n",
    "        logging.warning(f\"[Layout Build] No distinct roots found. Using fallback root: {roots[0]}\")\n",
    "    elif not roots and not active_nodes: # Should be caught by the first check\n",
    "        return {}\n",
    "        \n",
    "    # Ensure HORIZONTAL_SPACING and VERTICAL_SPACING are accessible\n",
    "    # These should be global variables defined in your USER INPUT section\n",
    "    node_size = HORIZONTAL_SPACING\n",
    "    subtree_widths: Dict[str, float] = {}\n",
    "    depth: Dict[str, int] = {}\n",
    "    visited_post_order: Set[str] = set()\n",
    "\n",
    "    def post_order_dfs(u_node_id: str, current_depth_level: int) -> Tuple[float, int]:\n",
    "        visited_post_order.add(u_node_id)\n",
    "        depth[u_node_id] = current_depth_level\n",
    "        max_child_depth = current_depth_level\n",
    "        \n",
    "        # Children are sorted for deterministic layout, if desired\n",
    "        children_of_u = sorted(adj.get(u_node_id, []))\n",
    "        \n",
    "        if not children_of_u:\n",
    "            subtree_widths[u_node_id] = node_size\n",
    "            return node_size, current_depth_level\n",
    "            \n",
    "        current_total_width = 0.0\n",
    "        valid_children_count = 0\n",
    "        for v_node_id in children_of_u:\n",
    "            if v_node_id in visited_post_order: # Avoid cycles\n",
    "                logging.warning(f\"[Layout Build] Cycle detected or re-visiting node {v_node_id} from {u_node_id} in post_order_dfs. Skipping.\")\n",
    "                continue\n",
    "            if v_node_id not in active_nodes: # Should not happen if adj list is built from active_nodes\n",
    "                logging.warning(f\"[Layout Build] Child {v_node_id} of {u_node_id} not in active_nodes list. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            child_width, child_max_d = post_order_dfs(v_node_id, current_depth_level + 1)\n",
    "            current_total_width += child_width\n",
    "            max_child_depth = max(max_child_depth, child_max_d)\n",
    "            valid_children_count += 1\n",
    "            \n",
    "        if valid_children_count > 1:\n",
    "            current_total_width += (valid_children_count - 1) * (HORIZONTAL_SPACING / 2.0) # Add spacing between subtrees\n",
    "            \n",
    "        final_width = max(node_size, current_total_width)\n",
    "        subtree_widths[u_node_id] = final_width\n",
    "        return final_width, max_child_depth\n",
    "\n",
    "    overall_max_depth_calculated = 0\n",
    "    total_graph_width_estimate = 0.0\n",
    "    processed_roots_for_width: Set[str] = set()\n",
    "\n",
    "    sorted_roots_list = sorted(list(set(roots))) # Ensure unique roots and sort for deterministic behavior\n",
    "    logging.debug(f\"[Layout Build] Final sorted roots for DFS: {sorted_roots_list}\")\n",
    "\n",
    "    for i, root_id in enumerate(sorted_roots_list):\n",
    "        if root_id not in visited_post_order: # Process each tree/component once\n",
    "            root_subtree_width, root_max_d = post_order_dfs(root_id, 0)\n",
    "            overall_max_depth_calculated = max(overall_max_depth_calculated, root_max_d)\n",
    "            total_graph_width_estimate += root_subtree_width\n",
    "            if i > 0: # Add spacing between separate trees\n",
    "                total_graph_width_estimate += HORIZONTAL_SPACING\n",
    "            processed_roots_for_width.add(root_id)\n",
    "\n",
    "    # Handle any nodes missed if roots were not exhaustive (e.g., disconnected\n",
    "    # components not in original roots)\n",
    "    for node_id_check in active_nodes:\n",
    "        if node_id_check not in visited_post_order:\n",
    "            logging.warning(f\"[Layout Build] Node {node_id_check} was not visited in post_order_dfs from main roots. Processing as a new component.\")\n",
    "            comp_width, comp_max_d = post_order_dfs(node_id_check, 0) # Start its depth from 0\n",
    "            overall_max_depth_calculated = max(overall_max_depth_calculated, comp_max_d)\n",
    "            total_graph_width_estimate += comp_width + HORIZONTAL_SPACING # Add its width and spacing\n",
    "\n",
    "    pos: Dict[str, Tuple[float, float]] = {}\n",
    "    visited_pre_order: Set[str] = set()\n",
    "    \n",
    "    def pre_order_dfs(u_node_id: str, current_x_center: float, current_y_level: float):\n",
    "        if u_node_id in visited_pre_order:\n",
    "            return\n",
    "        visited_pre_order.add(u_node_id)\n",
    "        \n",
    "        # Y coordinate is based on depth, making roots at top (y=0 or near y=0)\n",
    "        # Multiplying by VERTICAL_SPACING and negating for typical top-down tree drawing\n",
    "        pos[u_node_id] = (current_x_center, -depth.get(u_node_id, 0) * VERTICAL_SPACING)\n",
    "        \n",
    "        children_of_u = sorted(adj.get(u_node_id, []))\n",
    "        valid_children_for_pos = [v for v in children_of_u if v in active_nodes and v in subtree_widths and v not in visited_pre_order]\n",
    "\n",
    "        if not valid_children_for_pos:\n",
    "            return\n",
    "\n",
    "        # Calculate total width needed for children subtrees to center them under parent\n",
    "        total_children_width_span = sum(subtree_widths[v_id] for v_id in valid_children_for_pos)\n",
    "        if len(valid_children_for_pos) > 1:\n",
    "            total_children_width_span += (len(valid_children_for_pos) - 1) * (HORIZONTAL_SPACING / 2.0)\n",
    "            \n",
    "        child_start_x = current_x_center - (total_children_width_span / 2.0)\n",
    "        \n",
    "        for v_node_id in valid_children_for_pos:\n",
    "            child_subtree_w = subtree_widths[v_node_id]\n",
    "            # Center of the child's subtree block\n",
    "            child_x_center = child_start_x + (child_subtree_w / 2.0)\n",
    "            pre_order_dfs(v_node_id, child_x_center, current_y_level - VERTICAL_SPACING) # Y decreases for lower levels\n",
    "            child_start_x += child_subtree_w + (HORIZONTAL_SPACING / 2.0) # Move to start of next child block\n",
    "\n",
    "    current_root_x_offset = -total_graph_width_estimate / 2.0\n",
    "    for root_id in sorted_roots_list:\n",
    "        if root_id not in visited_pre_order:\n",
    "            root_subtree_w = subtree_widths.get(root_id, node_size)\n",
    "            root_center_x = current_root_x_offset + (root_subtree_w / 2.0)\n",
    "            pre_order_dfs(root_id, root_center_x, 0) # Roots start at y-level 0 (or close to it)\n",
    "            current_root_x_offset += root_subtree_w + HORIZONTAL_SPACING\n",
    "            \n",
    "    # Place any remaining unvisited nodes (should ideally not happen if graph is\n",
    "    # connected and roots are correct)\n",
    "    for node_id_check in active_nodes:\n",
    "        if node_id_check not in visited_pre_order:\n",
    "            logging.warning(f\"[Layout Build] Node {node_id_check} was not visited in pre_order_dfs. Placing.\")\n",
    "            comp_width = subtree_widths.get(node_id_check, node_size)\n",
    "            comp_center_x = current_root_x_offset + (comp_width / 2.0)\n",
    "            # Determine y based on its calculated depth, or a fallback depth\n",
    "            y_pos = -depth.get(node_id_check, overall_max_depth_calculated + 1) * VERTICAL_SPACING\n",
    "            pre_order_dfs(node_id_check, comp_center_x, y_pos) # Recursive call to place its children too\n",
    "            current_root_x_offset += comp_width + HORIZONTAL_SPACING\n",
    "\n",
    "    if not pos and active_nodes:\n",
    "        logging.error(f\"[Layout Build] Failed to generate any positions for {len(active_nodes)} active_nodes. Roots were: {sorted_roots_list}\")\n",
    "    elif pos and len(pos) != len(active_nodes):\n",
    "        logging.warning(f\"[Layout Build] Generated positions for {len(pos)} nodes, but expected {len(active_nodes)}. Missing: {set(active_nodes) - set(pos.keys())}\")\n",
    "        \n",
    "    return pos\n",
    "\n",
    "def plot_drainage_snapshot_with_ids(ax: plt.Axes,\n",
    "                                     forest: 'DrainageForest', # Unused in current drawing, but kept for signature\n",
    "                                     snapshot: Tuple[float, Dict],\n",
    "                                     inlet_id: str, # Main inlet ID for titling\n",
    "                                     time_series: List[Dict], # Unused in current drawing\n",
    "                                     global_cap_min: float,\n",
    "                                     global_cap_max: float,\n",
    "                                     original_inlet_ids: List[str]): # All selected inlets for highlighting paths\n",
    "    \"\"\"\n",
    "    Plots a single drainage network snapshot using a hierarchical layout.\n",
    "    \"\"\"\n",
    "    RADIUS_MIN = 0.25\n",
    "    RADIUS_MAX = 0.50\n",
    "    INLET_NODE_RADIUS = 0.50 # Slightly larger or distinct for inlet path nodes\n",
    "    RING_THICKNESS_FACTOR = 0.2\n",
    "    WHITE_GAP_FACTOR = 0.05 # Gap between fill circle and impervious/pervious ring\n",
    "\n",
    "    fill_cmap = cm.Blues\n",
    "    fill_norm = Normalize(vmin=0.0, vmax=100.0) # Normalize fill percentage 0-100\n",
    "\n",
    "    time_sec, current_snapshot_state = snapshot # current_snapshot_state is 'st'\n",
    "    time_min = time_sec / 60.0\n",
    "\n",
    "    logging.debug(f\"[Snapshot Plot {inlet_id} t={time_min:.2f}] Received state with keys: {current_snapshot_state.keys() if current_snapshot_state else 'None'}\")\n",
    "    if not current_snapshot_state:\n",
    "        ax.text(0.5, 0.5, f\"No active basins at t={time_min:.2f} min\", ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title(f\"Drainage Network State - Inlet {inlet_id} (t={time_min:.2f} min) - No State Data\")\n",
    "        ax.axis('off')\n",
    "        return\n",
    "\n",
    "    # Internal Helper Functions for Plotting\n",
    "    def is_on_selected_inlet_path(node_id_to_check: str, state_data_dict: Dict, all_orig_inlets: List[str]) -> bool:\n",
    "        node_info = state_data_dict.get(node_id_to_check)\n",
    "        if not node_info:\n",
    "            return False\n",
    "        merged_from_lineage = node_info.get('merged_from', [node_id_to_check]) # Default to self if not merged\n",
    "        return any(str(orig_inlet) in merged_from_lineage for orig_inlet in all_orig_inlets)\n",
    "\n",
    "    def capacity_to_radius(node_capacity_val: float) -> float:\n",
    "        if node_capacity_val <= 0: return RADIUS_MIN\n",
    "        # Use provided global_cap_min and global_cap_max for scaling\n",
    "        cap_min_log = max(global_cap_min, 1e-9) # Avoid log(0)\n",
    "        cap_max_log = max(global_cap_max, 1e-9)\n",
    "\n",
    "        if cap_min_log >= cap_max_log: # If min equals max or invalid range\n",
    "            return (RADIUS_MIN + RADIUS_MAX) * 0.5\n",
    "        try:\n",
    "            log_cap = math.log10(max(node_capacity_val, 1e-9))\n",
    "            log_min_val = math.log10(cap_min_log)\n",
    "            log_max_val = math.log10(cap_max_log)\n",
    "\n",
    "            if abs(log_max_val - log_min_val) < 1e-9: # Avoid division by zero if range is tiny\n",
    "                frac = 0.5\n",
    "            else:\n",
    "                frac = (log_cap - log_min_val) / (log_max_val - log_min_val)\n",
    "            \n",
    "            frac = max(0.0, min(1.0, frac)) # Clamp fraction\n",
    "            return RADIUS_MIN + (RADIUS_MAX - RADIUS_MIN) * frac\n",
    "        except ValueError: # Should be rare with max(cap, 1e-9)\n",
    "            logging.warning(f\"ValueError in capacity_to_radius for capacity={node_capacity_val}. Using RADIUS_MIN.\")\n",
    "            return RADIUS_MIN\n",
    "\n",
    "    # Layout Calculation\n",
    "    node_positions = None\n",
    "    try:\n",
    "        # Ensure HORIZONTAL_SPACING and VERTICAL_SPACING are globally defined\n",
    "        node_positions = build_hierarchical_layout(current_snapshot_state, original_inlet_ids)\n",
    "    except NameError as ne:\n",
    "        if 'build_hierarchical_layout' in str(ne):\n",
    "            logging.error(f\"[Snapshot Plot] LAYOUT FAILED: 'build_hierarchical_layout' function not found!\")\n",
    "        elif 'HORIZONTAL_SPACING' in str(ne) or 'VERTICAL_SPACING' in str(ne):\n",
    "            logging.error(\"[Snapshot Plot] LAYOUT FAILED: HORIZONTAL_SPACING or VERTICAL_SPACING global variables not defined.\")\n",
    "        else:\n",
    "            logging.error(f\"[Snapshot Plot] Layout failed due to NameError: {ne}\")\n",
    "        node_positions = None\n",
    "    except Exception as layout_err:\n",
    "        logging.error(f\"[Snapshot Plot] Layout generation failed for t={time_min:.2f} min: {layout_err}\", exc_info=True)\n",
    "        node_positions = None\n",
    "\n",
    "    if not node_positions: # Checks if pos is None or empty\n",
    "        ax.text(0.5, 0.5, \"Layout Error or No Nodes\", ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title(f\"Drainage Network State - Inlet {inlet_id} (t={time_min:.2f} min) - LAYOUT FAILED\")\n",
    "        ax.axis('off')\n",
    "        return\n",
    "\n",
    "    # Plot Edges (Arrows from Parent to Child)\n",
    "    edges_to_plot = []\n",
    "    for node_id, node_data in current_snapshot_state.items():\n",
    "        parent_id = node_data.get('parent_id')\n",
    "        if parent_id and parent_id in node_positions and node_id in node_positions:\n",
    "            # Ensure arrow goes from parent position to child position\n",
    "            edges_to_plot.append((node_positions[parent_id], node_positions[node_id]))\n",
    "    \n",
    "    if edges_to_plot:\n",
    "        for parent_pos, child_pos in edges_to_plot:\n",
    "            arrow = FancyArrowPatch(parent_pos, child_pos, arrowstyle='-|>', mutation_scale=15, color='gray', lw=0.8, alpha=0.7, zorder=1)\n",
    "            ax.add_patch(arrow)\n",
    "\n",
    "    # Plot Nodes\n",
    "    for node_id, node_data in current_snapshot_state.items():\n",
    "        if node_id not in node_positions:\n",
    "            logging.warning(f\"[Snapshot Plot] Node {node_id} has no position data. Skipping drawing.\")\n",
    "            continue\n",
    "            \n",
    "        x_coord, y_coord = node_positions[node_id]\n",
    "        is_primary_inlet_path_node = is_on_selected_inlet_path(node_id, current_snapshot_state, original_inlet_ids)\n",
    "        \n",
    "        node_cap = node_data.get('max_volume', 0.0)\n",
    "        current_vol = node_data.get('current_volume', 0.0)\n",
    "        \n",
    "        # Determine radius based on capacity, with special handling for inlet path nodes\n",
    "        # if desired\n",
    "        outer_radius = INLET_NODE_RADIUS if is_primary_inlet_path_node else capacity_to_radius(node_cap)\n",
    "        \n",
    "        ring_actual_thickness = outer_radius * RING_THICKNESS_FACTOR\n",
    "        white_gap_actual = outer_radius * WHITE_GAP_FACTOR\n",
    "        fill_circle_radius = max(0.01, outer_radius - ring_actual_thickness - white_gap_actual)\n",
    "\n",
    "        # Background for the ring (optional, or make it transparent if fill_circle is\n",
    "        # smaller)\n",
    "        # ring_background = Wedge((x_coord, y_coord), outer_radius, 0, 360,\n",
    "        # width=ring_actual_thickness, facecolor='lightgrey', edgecolor='none',\n",
    "        # alpha=0.3, zorder=2)\n",
    "        # ax.add_patch(ring_background)\n",
    "\n",
    "        # Impervious/Pervious Ring\n",
    "        imperv_a = node_data.get('impervious_area', 0.0)\n",
    "        perv_a = node_data.get('pervious_area', 0.0)\n",
    "        total_effective_area = imperv_a + perv_a\n",
    "        imperv_fraction = imperv_a / total_effective_area if total_effective_area > 1e-9 else 0.0\n",
    "        imperv_angle_deg = 360.0 * imperv_fraction\n",
    "\n",
    "        # Impervious part of the ring\n",
    "        arc_impervious = Wedge((x_coord, y_coord), outer_radius, 0, imperv_angle_deg, \n",
    "                                 width=ring_actual_thickness, facecolor='red', edgecolor='darkred', lw=0.5, zorder=3)\n",
    "        ax.add_patch(arc_impervious)\n",
    "        # Pervious part of the ring\n",
    "        arc_pervious = Wedge((x_coord, y_coord), outer_radius, imperv_angle_deg, 360.0, \n",
    "                             width=ring_actual_thickness, facecolor='green', edgecolor='darkgreen', lw=0.5, zorder=3)\n",
    "        ax.add_patch(arc_pervious)\n",
    "        \n",
    "        # Central Fill Circle\n",
    "        fill_percent_val = (current_vol / node_cap) * 100.0 if node_cap > 1e-9 else 0.0\n",
    "        fill_percent_val = max(0.0, min(fill_percent_val, 100.0)) # Clamp\n",
    "        \n",
    "        fill_color_val = fill_cmap(fill_norm(fill_percent_val))\n",
    "        center_circle = Circle((x_coord, y_coord), radius=fill_circle_radius, \n",
    "                               facecolor=fill_color_val, edgecolor='darkgray', linewidth=0.7, zorder=4)\n",
    "        ax.add_patch(center_circle)\n",
    "\n",
    "        # Basin ID Label\n",
    "        display_id = f\"ID: {node_id}\"\n",
    "        max_id_display_len = 15\n",
    "        if len(node_id) > max_id_display_len:\n",
    "            display_id = f\"ID: ..{node_id[-(max_id_display_len-3):]}\"\n",
    "        id_label_bbox = dict(boxstyle=\"round,pad=0.15\", fc=\"ivory\", ec=\"black\", lw=0.5, alpha=0.85)\n",
    "        ax.text(x_coord, y_coord, display_id, ha='center', va='center', fontsize=7, fontweight='normal', color='black', bbox=id_label_bbox, zorder=6)\n",
    "\n",
    "        # Stats Text Label below node\n",
    "        node_total_area = node_data.get('area', 0.0)\n",
    "        stats_text_content = (f\"A:{node_total_area:.1f} Vmx:{node_cap:.2f} Vol:{current_vol:.2f}\\n\"\n",
    "                              f\"Imp:{imperv_fraction*100:.0f}% Fill:{fill_percent_val:.1f}%\")\n",
    "        if is_primary_inlet_path_node: stats_text_content += \"\\n(INLET PATH)\"\n",
    "        stats_label_bbox = dict(boxstyle=\"round,pad=0.2\", fc=\"white\", ec=\"grey\", lw=0.5, alpha=0.75)\n",
    "        ax.text(x_coord, y_coord - outer_radius - 0.15, stats_text_content, ha='center', va='top', fontsize=6, color='black', bbox=stats_label_bbox, zorder=5)\n",
    "\n",
    "    # Legend, Title, Axis Limits, Colorbar\n",
    "    legend_handles = [\n",
    "        mpatches.Patch(color='red', label='Impervious Area (Ring %)'),\n",
    "        mpatches.Patch(color='green', label='Pervious Area (Ring %)'),\n",
    "        mpatches.Patch(color=fill_cmap(0.5), label='Fill % (Center Circle)', ec='darkgray'),\n",
    "        mpatches.Patch(facecolor='ivory', edgecolor='black', label='Basin ID Label')\n",
    "    ]\n",
    "    ax.legend(handles=legend_handles, loc='upper left', fontsize=8, bbox_to_anchor=(0.01, 0.99), frameon=True, facecolor='white', edgecolor='black').set_zorder(10)\n",
    "    \n",
    "    ax.set_title(f\"Drainage Network State - Inlet {inlet_id} (t={time_min:.2f} min)\", fontsize=14)\n",
    "    ax.axis('off') # Turn off axis lines and ticks\n",
    "    \n",
    "    # Auto-adjust plot limits\n",
    "    if node_positions: # Ensure node_positions is not empty\n",
    "        all_x_coords = [p[0] for p in node_positions.values()]\n",
    "        all_y_coords = [p[1] for p in node_positions.values()]\n",
    "        if all_x_coords and all_y_coords: # Check lists are not empty\n",
    "            # Use HORIZONTAL_SPACING and VERTICAL_SPACING for padding, if available\n",
    "            # globally\n",
    "            padding_x = HORIZONTAL_SPACING * 0.75 if 'HORIZONTAL_SPACING' in globals() else 1.0\n",
    "            padding_y = VERTICAL_SPACING * 0.75 if 'VERTICAL_SPACING' in globals() else 1.0\n",
    "            ax.set_xlim(min(all_x_coords) - padding_x, max(all_x_coords) + padding_x)\n",
    "            ax.set_ylim(min(all_y_coords) - padding_y, max(all_y_coords) + padding_y)\n",
    "    \n",
    "    ax.set_aspect('equal', adjustable='box') # Ensure aspect ratio is equal\n",
    "\n",
    "    # Colorbar for fill percentage\n",
    "    scalar_mappable = cm.ScalarMappable(norm=fill_norm, cmap=fill_cmap)\n",
    "    scalar_mappable.set_array([]) # Needed for colorbar to work with patches\n",
    "    plot_divider = make_axes_locatable(ax)\n",
    "    colorbar_axis = plot_divider.append_axes(\"right\", size=\"3%\", pad=0.1)\n",
    "    colorbar = plt.colorbar(scalar_mappable, cax=colorbar_axis)\n",
    "    colorbar.set_label(\"Fill Percentage (%)\", fontsize=10)\n",
    "    colorbar.ax.tick_params(labelsize=8)\n",
    "\n",
    "def plot_simulation_results(time_series: List[dict],\n",
    "                            outlet_runoff_data: Optional[Dict],\n",
    "                            primary_inlet_runoff_data: Optional[Dict],\n",
    "                            rain_unit: str,\n",
    "                            simulation_time_min: float,\n",
    "                            selected_inlets: List[str],\n",
    "                            recalculated_eia_df: Optional[pd.DataFrame] = None,\n",
    "                            export_discharge_csv: bool = True,\n",
    "                            export_interval_sec: int = 10,\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    Plots simulation results.\n",
    "    MODIFIED: Uses a recalculated DataFrame for Total, Impervious, and Pervious Area\n",
    "    based on arrival time, if provided.\n",
    "    \"\"\"\n",
    "    if not time_series:\n",
    "        logging.warning(\"Time series data is empty. Cannot plot simulation results.\")\n",
    "        return\n",
    "    logging.info(\"Plotting simulation results (Area, Rainfall, Discharge)...\")\n",
    "\n",
    "    base_fontsize = 10; fontsize_title = base_fontsize + 3; fontsize_subtitle = base_fontsize + 2\n",
    "    fontsize_label = base_fontsize + 1; fontsize_legend = base_fontsize; fontsize_tick = base_fontsize\n",
    "    colors = { 'Total Area': 'black', 'Impervious Area': '#e31a1c', 'Pervious Area': '#2ca02c',\n",
    "               'Rainfall': '#a6cee3', 'Outlet Discharge': '#1f77b4' }\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(9, 10), sharex=True, dpi = 450)\n",
    "    legend_props = {'loc': 'upper right', 'fontsize': fontsize_legend, 'frameon': True, 'facecolor': 'white', 'edgecolor': 'black'}\n",
    "\n",
    "    t = np.array([d['time'] for d in time_series])\n",
    "\n",
    "    # Plot Areas (ax1)\n",
    "    ax1 = axes[0]\n",
    "    if recalculated_eia_df is not None and not recalculated_eia_df.empty:\n",
    "        logging.info(\"Using recalculated areas (by arrival time) for plotting.\")\n",
    "        # Use the new, corrected data for all three area types\n",
    "        ax1.step(recalculated_eia_df['time'], recalculated_eia_df['total_area_by_arrival'], color=colors['Total Area'], linestyle='-', label='Total Area', where='post')\n",
    "        ax1.step(recalculated_eia_df['time'], recalculated_eia_df['impervious_area_by_arrival'], color=colors['Impervious Area'], linestyle='-', label='Impervious Area', where='post')\n",
    "        ax1.step(recalculated_eia_df['time'], recalculated_eia_df['pervious_area_by_arrival'], color=colors['Pervious Area'], linestyle='-', label='Pervious Area', where='post')\n",
    "        ax1.set_title(\"Outlet Basin Connected Area (by Arrival Time)\", fontsize=fontsize_subtitle)\n",
    "    else:\n",
    "        # Fallback to the old method if new data isn't provided or is empty\n",
    "        logging.info(\"Recalculated area data not available, using spill-time connected area for plotting.\")\n",
    "        tot_area=np.array([d.get('inlet_total_area',0) for d in time_series])\n",
    "        imp_area=np.array([d.get('inlet_impervious_area',0) for d in time_series])\n",
    "        perv_area=np.array([d.get('inlet_pervious_area',0) for d in time_series])\n",
    "        ax1.step(t, tot_area, color=colors['Total Area'], linestyle='-', label='Total Area', where='post')\n",
    "        ax1.step(t, imp_area, color=colors['Impervious Area'], linestyle='-', label='Impervious Area', where='post')\n",
    "        ax1.step(t, perv_area, color=colors['Pervious Area'], linestyle='-', label='Pervious Area', where='post')\n",
    "        ax1.set_title(\"Outlet Basin Area Composition (by Spill Time)\", fontsize=fontsize_subtitle)\n",
    "\n",
    "    ax1.set_ylabel(\"Area (m²)\", fontsize=fontsize_label)\n",
    "    ax1.legend(**legend_props).get_frame().set_linewidth(0.8); ax1.grid(False); ax1.set_ylim(bottom=0)\n",
    "    ax1.tick_params(axis='both', labelsize=fontsize_tick); ax1.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.0f'))\n",
    "\n",
    "    # Plot Rainfall (ax2)\n",
    "    ax2 = axes[1]; rain_timeline_min=None; rain_rates=None; ylabel_rain=f\"Rain Rate ({rain_unit})\"\n",
    "    if primary_inlet_runoff_data:\n",
    "        input_timeline=primary_inlet_runoff_data.get('timeline_min'); rain_depth_step=primary_inlet_runoff_data.get('rain_depth_m_step'); dt_sec=primary_inlet_runoff_data.get('dt_sec')\n",
    "        if input_timeline is not None and rain_depth_step is not None and dt_sec is not None and dt_sec > 1e-6:\n",
    "            input_timeline=np.array(input_timeline); rain_depth_step=np.array(rain_depth_step); min_len_rain=min(len(input_timeline),len(rain_depth_step))\n",
    "            if min_len_rain > 0:\n",
    "                rain_timeline_min=input_timeline[:min_len_rain]; rain_m_s=rain_depth_step[:min_len_rain]/dt_sec\n",
    "                if rain_unit=='cm/hr': rain_rates=(rain_m_s*3600.0)/0.01\n",
    "                elif rain_unit=='mm/hr': rain_rates=(rain_m_s*3600.0)/0.001\n",
    "                elif rain_unit=='in/hr': rain_rates=(rain_m_s*3600.0)/0.0254\n",
    "                else: rain_rates=rain_m_s; ylabel_rain=\"Rain Rate (m/s)\"\n",
    "    if rain_timeline_min is not None and rain_rates is not None:\n",
    "        plot_mask = rain_timeline_min <= simulation_time_min + 1e-9\n",
    "        ax2.plot(rain_timeline_min[plot_mask], rain_rates[plot_mask], color=colors['Rainfall'], drawstyle='steps-post')\n",
    "    else: ax2.text(0.5, 0.5, \"Rainfall data not available\", transform=ax2.transAxes, ha='center', fontsize=fontsize_tick)\n",
    "    ax2.set_ylabel(ylabel_rain, fontsize=fontsize_label); ax2.set_title(\"Rainfall Input\", fontsize=fontsize_subtitle)\n",
    "    ax2.grid(False); ax2.set_ylim(bottom=0); ax2.tick_params(axis='both', labelsize=fontsize_tick); ax2.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.2f'))\n",
    "\n",
    "    # Plot Hydrographs (ax3)\n",
    "    ax3 = axes[2]\n",
    "    system_discharge_m3_s = np.array([d.get('inlet_volume_rate', 0.0) for d in time_series]); system_discharge_m3_min = system_discharge_m3_s * 60.0\n",
    "    ax3.plot(t, system_discharge_m3_min, color=colors['Outlet Discharge'], linewidth=2, label='Outlet Discharge', drawstyle='steps-post')\n",
    "    ax3.set_xlabel(\"Time (min)\", fontsize=fontsize_label); ax3.set_ylabel(\"Flow Rate (m³/min)\", fontsize=fontsize_label)\n",
    "    ax3.set_title(\"Outlet Discharge Hydrograph\", fontsize=fontsize_subtitle)\n",
    "    ax3.grid(False); ax3.set_ylim(bottom=0); ax3.tick_params(axis='both', labelsize=fontsize_tick); ax3.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.3f'))\n",
    "\n",
    "    # Final Formatting\n",
    "    ax3.set_xlim(0, simulation_time_min)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "    plt.show()\n",
    "\n",
    "    # Export System Discharge to CSV\n",
    "    if export_discharge_csv:\n",
    "        try:\n",
    "            primary_inlet_id = str(selected_inlets[0]) if selected_inlets else \"unknown_inlet\"\n",
    "            dynamic_export_filename = f\"outlet_discharge_{primary_inlet_id}.csv\"\n",
    "\n",
    "            logging.info(f\"Exporting outlet discharge to '{dynamic_export_filename}' (interval: {export_interval_sec} sec)...\")\n",
    "            if not time_series or len(t) == 0: logging.warning(\"Cannot export discharge: time_series empty.\"); return\n",
    "\n",
    "            original_times_sec = t * 60.0; original_flow_m3s = system_discharge_m3_s\n",
    "            if original_times_sec.size == 0: logging.warning(\"Cannot export discharge: Time array empty.\"); return\n",
    "\n",
    "            max_time_sec = simulation_time_min * 60.0\n",
    "            export_times_sec = np.arange(0, max_time_sec + export_interval_sec, export_interval_sec)\n",
    "            max_data_time_sec = original_times_sec[-1] if original_times_sec.size > 0 else 0\n",
    "            export_times_sec = export_times_sec[export_times_sec <= max_data_time_sec + 1e-6]\n",
    "            if export_times_sec.size == 0: logging.warning(\"Cannot export discharge: Export time array empty.\"); return\n",
    "\n",
    "            if 'safe_interp' not in globals(): raise NameError(\"'safe_interp' function is needed for CSV export but not found.\")\n",
    "            interp_flow_m3s = safe_interp(export_times_sec, original_times_sec, original_flow_m3s, label=\"DischargeExport\")\n",
    "\n",
    "            df_export = pd.DataFrame({'Time (s)': export_times_sec, 'Outlet Discharge (m3/s)': interp_flow_m3s})\n",
    "            export_path = os.path.abspath(dynamic_export_filename)\n",
    "            df_export.to_csv(export_path, index=False, float_format='%.6f')\n",
    "            logging.info(f\"Successfully exported outlet discharge to {export_path}\")\n",
    "\n",
    "        except Exception as e: logging.error(f\"Failed to export discharge data to CSV: {e}\", exc_info=True)\n",
    "\n",
    "def create_corrected_basin_contributions_plot(snapshots, simulator: 'IntegratedSimulation', runoff_processor: 'RunoffProcessor') -> dict:\n",
    "    logging.info(\"Creating corrected basin contributions plot...\")\n",
    "    if not simulator or not runoff_processor or not snapshots or not simulator.time_series:\n",
    "        logging.error(\"Missing required data (simulator, processor, snapshots, or time_series) for contributions plot.\")\n",
    "        return {}\n",
    "    inlet_ids = simulator.selected_inlet_ids\n",
    "    if not inlet_ids:\n",
    "        logging.error(\"No inlet IDs found in simulator.\")\n",
    "        return {}\n",
    "    primary_inlet_id = inlet_ids[0]\n",
    "    reference_timeline = np.array([d['time'] for d in simulator.time_series])\n",
    "    if len(reference_timeline) == 0:\n",
    "        logging.error(\"Reference timeline from simulator is empty.\")\n",
    "        return {}\n",
    "\n",
    "    all_basin_ids_in_data = list(runoff_processor.basin_runoff_data.keys())\n",
    "    if not all_basin_ids_in_data:\n",
    "        logging.warning(\"Runoff processor contains no basin data. Cannot plot contributions.\")\n",
    "        return {}\n",
    "    distinct_colors = plt.cm.viridis(np.linspace(0, 1, len(all_basin_ids_in_data)))\n",
    "    color_map = {bid: distinct_colors[i % len(distinct_colors)] for i, bid in enumerate(all_basin_ids_in_data)}\n",
    "\n",
    "    final_time, final_state = snapshots[-1]\n",
    "    final_outlet_ids = []\n",
    "    if final_state:\n",
    "        for basin_id, state_data in final_state.items():\n",
    "            if any(orig_inlet in state_data.get('merged_from', []) for orig_inlet in inlet_ids):\n",
    "                final_outlet_ids.append(basin_id)\n",
    "    else:\n",
    "        logging.warning(\"Final snapshot state is empty. Attempting fallback to find outlet.\")\n",
    "        final_outlet_ids = [bid for bid in inlet_ids if bid in runoff_processor.basin_runoff_data]\n",
    "    if not final_outlet_ids:\n",
    "        logging.error(\"Could not identify final outlet node(s) containing original inlets. Cannot trace contributions.\")\n",
    "        return {}\n",
    "    logging.info(f\"Final outlet node(s) identified for contribution tracing: {final_outlet_ids}\")\n",
    "\n",
    "    processed_for_contrib = set(); contribution_details = {}; queue = list(final_outlet_ids); traced_ids = set()\n",
    "    while queue:\n",
    "        current_id = queue.pop(0)\n",
    "        if current_id in processed_for_contrib: continue\n",
    "        processed_for_contrib.add(current_id)\n",
    "        if current_id not in runoff_processor.basin_runoff_data:\n",
    "            logging.warning(f\"Runoff data missing for basin {current_id} during contribution trace.\")\n",
    "            continue\n",
    "        data = runoff_processor.basin_runoff_data[current_id]\n",
    "        timeline = data.get('convolution_timeline_min'); flow_m3_s = data.get('direct_flow_m3_s'); merge_time = data.get('merge_time_min', 0.0)\n",
    "        if timeline is None or flow_m3_s is None or len(timeline) == 0:\n",
    "            logging.warning(f\"Missing timeline/flow for basin {current_id}. Skipping contribution.\")\n",
    "            continue\n",
    "        min_len = min(len(timeline), len(flow_m3_s)); timeline = timeline[:min_len]; flow_m3_s = flow_m3_s[:min_len]\n",
    "        interp_flow_m3_min = np.interp(reference_timeline, timeline, flow_m3_s * 60.0, left=0, right=0)\n",
    "        label = f\"Basin {current_id}\"; max_label_len = 40\n",
    "        if current_id in inlet_ids: label = f\"Inlet {current_id}\"\n",
    "        if len(label) > max_label_len: label = label[:max_label_len-3] + \"...\"\n",
    "        if merge_time > 0: label += f\" (M @ {merge_time:.1f}m)\"\n",
    "        contribution_details[current_id] = {'flow': interp_flow_m3_min, 'label': label, 'color': color_map.get(current_id, 'gray'), 'merge_time': merge_time}\n",
    "        if 'merged_from_ids' in data:\n",
    "            for component_id in data['merged_from_ids']:\n",
    "                if component_id not in processed_for_contrib and component_id not in traced_ids:\n",
    "                    queue.append(component_id); traced_ids.add(component_id)\n",
    "    logging.info(f\"Identified {len(contribution_details)} contributing basins/hydrographs for plot.\")\n",
    "    if not contribution_details: return {}\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    sorted_basin_ids = sorted(contribution_details.keys(), key=lambda bid: contribution_details[bid]['merge_time'])\n",
    "    stack_flows = [contribution_details[bid]['flow'] for bid in sorted_basin_ids]\n",
    "    stack_labels = [contribution_details[bid]['label'] for bid in sorted_basin_ids]\n",
    "    stack_colors = [contribution_details[bid]['color'] for bid in sorted_basin_ids]\n",
    "    valid_indices = [i for i, flow in enumerate(stack_flows) if np.max(flow) > 1e-6]\n",
    "    if not valid_indices:\n",
    "        logging.warning(\"No significant contributing flows found to plot.\")\n",
    "        plt.close(fig); return {}\n",
    "    stack_flows = [stack_flows[i] for i in valid_indices]; stack_labels = [stack_labels[i] for i in valid_indices]; stack_colors = [stack_colors[i] for i in valid_indices]\n",
    "    try:\n",
    "        ax.stackplot(reference_timeline, stack_flows, labels=stack_labels, colors=stack_colors, alpha=0.7, step='post')\n",
    "    except Exception as stack_err:\n",
    "        logging.error(f\"Error during stackplot generation: {stack_err}\")\n",
    "        plt.close(fig); return {}\n",
    "    total_system_response_m3_min = np.array([d['inlet_volume_rate'] * 60 for d in simulator.time_series])\n",
    "    ax.plot(reference_timeline, total_system_response_m3_min, 'k-', linewidth=2.5, label='Total System Discharge', drawstyle='steps-post')\n",
    "    ax.set_xlabel('Time (min)', fontsize=12); ax.set_ylabel('Flow Rate (m³/min)', fontsize=12)\n",
    "    ax.set_title(f\"Basin Contributions to Discharge Hydrograph (Outlet Tree: {primary_inlet_id})\", fontsize=14)\n",
    "    ax.set_xlim(0, reference_timeline[-1] if len(reference_timeline)>0 else 1)\n",
    "    max_y_stack = np.max(np.sum(stack_flows, axis=0)) if stack_flows else 0\n",
    "    max_y_total = np.max(total_system_response_m3_min) if len(total_system_response_m3_min)>0 else 0\n",
    "    max_y = max(max_y_stack, max_y_total) * 1.1; ax.set_ylim(0, max(max_y, 0.1))\n",
    "    num_labels = len(stack_labels); legend_cols = max(1, num_labels // 15 + (1 if num_labels % 15 > 0 else 0))\n",
    "    ax.legend(loc='upper right', fontsize=8, ncol=legend_cols); ax.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout(); figname = \"basin_contributions_plot.png\"; plt.savefig(figname); plt.close(fig)\n",
    "    logging.info(f\"Basin contributions plot saved as {figname}\")\n",
    "    return contribution_details\n",
    "\n",
    "def plot_timestep_water_balance(\n",
    "    water_balance_log: List[dict],\n",
    "    runon_time_series_data: List[Dict],\n",
    "    simulation_time_min_param: float,\n",
    "    figsize=(8, 8),\n",
    "    dpi=450\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots key water balance components PER 1-MINUTE TIMESTEP as LINES (NON-CUMULATIVE).\n",
    "    \"Surface Runoff Transfer\" and \"Run-on Infiltration\" are combined into a single\n",
    "    orange line: \"Surface Runoff Loss (Transfer & Run-on Infiltration)\".\n",
    "    Total Rainfall line is EXCLUDED.\n",
    "    Y-axis starts at 0 unless data is negative.\n",
    "    \"\"\"\n",
    "    if not water_balance_log:\n",
    "        logging.warning(\"plot_timestep_water_balance (1-min lines): Water balance data empty; skipping plot.\")\n",
    "        return None, None\n",
    "\n",
    "    logging.info(\"Plotting NON-CUMULATIVE 1-min aggregated water balance components as LINES...\")\n",
    "\n",
    "    original_times_log = np.array([d['time'] for d in water_balance_log])\n",
    "    if original_times_log.size == 0:\n",
    "        logging.error(\"plot_timestep_water_balance (1-min lines): original_times_log is empty.\")\n",
    "        return None, None\n",
    "\n",
    "    dt_approx_for_label = 1.0\n",
    "\n",
    "    # 1. Create a new 1-minute timeline for interpolation/aggregation\n",
    "    aggregation_timeline_max = simulation_time_min_param\n",
    "    bin_edges = np.arange(0, aggregation_timeline_max + 1, 1.0)\n",
    "    plot_timeline_1min = bin_edges[:-1] if len(bin_edges) > 1 else np.array([0.0])\n",
    "\n",
    "    if plot_timeline_1min.size == 0 and aggregation_timeline_max > 0:\n",
    "        logging.warning(f\"plot_timestep_water_balance (1-min lines): plot_timeline_1min is unexpectedly empty for duration {aggregation_timeline_max}. Defaulting to [0.0].\")\n",
    "        plot_timeline_1min = np.array([0.0])\n",
    "    elif plot_timeline_1min.size == 0 and aggregation_timeline_max == 0:\n",
    "        logging.info(\"plot_timestep_water_balance: simulation duration is 0, plot will be empty.\")\n",
    "        fig, ax = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_title(\"Water Balance Components per 1-min Timestep (No Data)\")\n",
    "        return fig, ax\n",
    "\n",
    "    def _aggregate_per_1min_step(key: str, fill_val=0.0) -> np.ndarray:\n",
    "        raw_values = np.array([float(d.get(key, fill_val)) if isinstance(d.get(key, 0.0), (int, float, np.number)) and np.isfinite(d.get(key, 0.0)) else fill_val for d in water_balance_log])\n",
    "        current_original_times = original_times_log\n",
    "        current_raw_values = raw_values\n",
    "\n",
    "        if current_original_times.size == 0:\n",
    "            return np.full(len(plot_timeline_1min), fill_val, dtype=float)\n",
    "\n",
    "        if abs(current_original_times[0]) > 1e-6:\n",
    "            current_original_times = np.insert(current_original_times, 0, 0.0)\n",
    "            current_raw_values = np.insert(current_raw_values, 0, 0.0)\n",
    "\n",
    "        aggregated_values = np.zeros(len(plot_timeline_1min), dtype=float)\n",
    "        \n",
    "        if current_original_times.size < 2:\n",
    "            if current_original_times.size == 1:\n",
    "                bin_index = np.searchsorted(bin_edges, current_original_times[0], side='right') - 1\n",
    "                if 0 <= bin_index < len(aggregated_values):\n",
    "                    aggregated_values[bin_index] = current_raw_values[0]\n",
    "            return aggregated_values\n",
    "\n",
    "        cumulative_orig = np.cumsum(current_raw_values)\n",
    "        interp_cum_at_bin_edges = safe_interp(bin_edges, current_original_times, cumulative_orig, default_val=0.0, label=f\"{key}_agg_interp\")\n",
    "        per_bin_values = np.diff(interp_cum_at_bin_edges)\n",
    "\n",
    "        if len(per_bin_values) == len(plot_timeline_1min):\n",
    "            aggregated_values = per_bin_values\n",
    "        elif len(per_bin_values) > len(plot_timeline_1min):\n",
    "            aggregated_values = per_bin_values[:len(plot_timeline_1min)]\n",
    "            logging.warning(f\"Aggregation for key '{key}' (diff method) resulted in more values than bins. Truncating.\")\n",
    "        elif len(per_bin_values) < len(plot_timeline_1min) and len(per_bin_values) > 0:\n",
    "            logging.warning(f\"Aggregation for key '{key}' (diff method) resulted in fewer values than bins. Padding with last.\")\n",
    "            aggregated_values[:len(per_bin_values)] = per_bin_values\n",
    "            aggregated_values[len(per_bin_values):] = per_bin_values[-1]\n",
    "        elif len(per_bin_values) == 0 and len(plot_timeline_1min) > 0:\n",
    "            logging.warning(f\"Aggregation for key '{key}' (diff method) resulted in zero values for non-zero bins. Using zeros.\")\n",
    "        return aggregated_values\n",
    "\n",
    "    step_discharge_1min = _aggregate_per_1min_step('inlet_discharge')\n",
    "    step_storage_change_1min = _aggregate_per_1min_step('storage_change')\n",
    "    step_active_runoff_generated_1min = _aggregate_per_1min_step('active_runoff_generated')\n",
    "\n",
    "    step_runon_inf_calculated_1min_agg = np.zeros_like(plot_timeline_1min, dtype=float)\n",
    "    if runon_time_series_data:\n",
    "        for entry_idx, entry in enumerate(runon_time_series_data):\n",
    "            event_times = np.array(entry.get('times', []), dtype=float)\n",
    "            event_vols_step = np.array(entry.get('runon_inf_vol_step', []), dtype=float)\n",
    "\n",
    "            if event_times.size > 0 and event_times.size == event_vols_step.size:\n",
    "                current_event_times = event_times\n",
    "                current_event_vols_step = event_vols_step\n",
    "                if abs(current_event_times[0]) > 1e-6:\n",
    "                    current_event_times = np.insert(current_event_times, 0, 0.0)\n",
    "                    current_event_vols_step = np.insert(current_event_vols_step, 0, 0.0)\n",
    "                if len(current_event_times) < 2: continue\n",
    "\n",
    "                cumulative_event_runon_orig_time = np.cumsum(current_event_vols_step)\n",
    "                interp_cum_event_runon_at_bin_edges = safe_interp(bin_edges, current_event_times, cumulative_event_runon_orig_time)\n",
    "                per_1min_bin_event_runon = np.diff(interp_cum_event_runon_at_bin_edges)\n",
    "                \n",
    "                target_len = len(step_runon_inf_calculated_1min_agg)\n",
    "                if len(per_1min_bin_event_runon) == target_len:\n",
    "                    step_runon_inf_calculated_1min_agg += per_1min_bin_event_runon\n",
    "                elif len(per_1min_bin_event_runon) > target_len:\n",
    "                    step_runon_inf_calculated_1min_agg += per_1min_bin_event_runon[:target_len]\n",
    "                elif len(per_1min_bin_event_runon) < target_len and len(per_1min_bin_event_runon) > 0:\n",
    "                    padded_runon = np.zeros(target_len)\n",
    "                    padded_runon[:len(per_1min_bin_event_runon)] = per_1min_bin_event_runon\n",
    "                    step_runon_inf_calculated_1min_agg += padded_runon\n",
    "                else:\n",
    "                    logging.warning(f\"Runon series entry {entry_idx} for 1-min plot has mismatched times/vols or is empty.\")\n",
    "\n",
    "    positive_storage_change_step_1min = np.maximum(0, step_storage_change_1min)\n",
    "    combined_loss_per_1min_step = step_active_runoff_generated_1min - positive_storage_change_step_1min - step_discharge_1min\n",
    "    combined_loss_per_1min_step = np.maximum(0, combined_loss_per_1min_step)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "    fontsize_title = 14; fontsize_labels = 12; fontsize_legend = 10\n",
    "    dt_plot_label = 1.0\n",
    "\n",
    "    colors_timestep_lines = {\n",
    "        'Active Runoff Generated': '#5DBB63',\n",
    "        'Surface Storage Change': '#7f7f7f',\n",
    "        'Surface Runoff Loss (Transfer & Run-on Infiltration)': '#ff7f0e',\n",
    "        'Outlet Discharge': '#1f77b4',\n",
    "    }\n",
    "    labels_timestep_lines = {\n",
    "        'Active Runoff Generated': f'Active Runoff Gen.',\n",
    "        'Surface Storage Change': f'Surface Storage Change',\n",
    "        'Surface Runoff Loss (Transfer & Run-on Infiltration)': f'Runoff Loss (Transfer & Run-on)',\n",
    "        'Outlet Discharge': f'Outlet Discharge',\n",
    "    }\n",
    "    line_linewidth = 1.8; ref_linewidth = 1.5\n",
    "\n",
    "    num_plot_points = len(plot_timeline_1min)\n",
    "\n",
    "    ax.plot(plot_timeline_1min, step_storage_change_1min[:num_plot_points],\n",
    "            color=colors_timestep_lines['Surface Storage Change'], linewidth=line_linewidth,\n",
    "            label=labels_timestep_lines['Surface Storage Change'], drawstyle='steps-post')\n",
    "    ax.plot(plot_timeline_1min, combined_loss_per_1min_step[:num_plot_points],\n",
    "            color=colors_timestep_lines['Surface Runoff Loss (Transfer & Run-on Infiltration)'], linewidth=line_linewidth,\n",
    "            label=labels_timestep_lines['Surface Runoff Loss (Transfer & Run-on Infiltration)'], drawstyle='steps-post')\n",
    "    ax.plot(plot_timeline_1min, step_discharge_1min[:num_plot_points],\n",
    "            color=colors_timestep_lines['Outlet Discharge'], linewidth=line_linewidth,\n",
    "            label=labels_timestep_lines['Outlet Discharge'], drawstyle='steps-post')\n",
    "    ax.plot(plot_timeline_1min, step_active_runoff_generated_1min[:num_plot_points],\n",
    "            label=labels_timestep_lines['Active Runoff Generated'],\n",
    "            color=colors_timestep_lines['Active Runoff Generated'], linewidth=ref_linewidth, linestyle='--', drawstyle='steps-post', alpha=0.8)\n",
    "\n",
    "    ax.set_xlabel(\"Time (min)\", fontsize=fontsize_labels)\n",
    "    ax.set_ylabel(f\"Volume per 1-min Timestep (m³)\", fontsize=fontsize_labels)\n",
    "    ax.set_title(\"Water Balance Components per 1-min Timestep (Lines, Non-Cumulative)\", fontsize=fontsize_title)\n",
    "    ax.legend(loc='upper right', fontsize=fontsize_legend, frameon=True, facecolor='white', edgecolor='black', ncol=1)\n",
    "    leg = ax.get_legend()\n",
    "    if leg: leg.get_frame().set_linewidth(0.8)\n",
    "    \n",
    "    ax.grid(False)\n",
    "    \n",
    "    ax.tick_params(axis='both', which='major', labelsize=fontsize_labels-1)\n",
    "    ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.3f'))\n",
    "\n",
    "    all_lines_for_ylim_agg = [\n",
    "        step_storage_change_1min[:num_plot_points],\n",
    "        combined_loss_per_1min_step[:num_plot_points],\n",
    "        step_discharge_1min[:num_plot_points],\n",
    "        step_active_runoff_generated_1min[:num_plot_points]\n",
    "    ]\n",
    "    min_y_overall = 0\n",
    "    max_y_overall = 0.001\n",
    "\n",
    "    for arr in all_lines_for_ylim_agg:\n",
    "        if arr is not None and arr.size > 0:\n",
    "            finite_arr = arr[np.isfinite(arr)]\n",
    "            if finite_arr.size > 0:\n",
    "                min_y_overall = min(min_y_overall, np.min(finite_arr))\n",
    "                max_y_overall = max(max_y_overall, np.max(finite_arr))\n",
    "\n",
    "    bottom_y_limit = min(0, min_y_overall - 0.05 * abs(min_y_overall) if min_y_overall < -1e-6 else -0.0001)\n",
    "    top_y_limit = max_y_overall * 1.1 if max_y_overall > 1e-6 else 0.01\n",
    "    ax.set_ylim(bottom_y_limit, top_y_limit)\n",
    "    \n",
    "    actual_plot_end_time = min(plot_timeline_1min[-1] + 0.5, 250.0) if plot_timeline_1min.size > 0 else 40.0\n",
    "    ax.set_xlim(0, actual_plot_end_time)\n",
    "    \n",
    "    plt.axhline(0, color='black', linewidth=0.7)\n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "def plot_mass_balance(water_balance: List[dict],\n",
    "                      runon_time_series_data: List[Dict],\n",
    "                      total_potential_runoff_input: Optional[float] = None):\n",
    "    \"\"\"\n",
    "    Plots key cumulative water balance components as a stacked area chart.\n",
    "    MODIFIED: Combines Surface Runoff Transfer and Run-on Infiltration.\n",
    "    Applies specific formatting, labels, colors, and stacking order.\n",
    "\n",
    "    Stacking Order (Bottom to Top):\n",
    "    1. Surface Depression Storage (Dark Gray)\n",
    "    2. Surface Runoff Transfer & Run-on Infiltration (Orange)\n",
    "    3. Outlet Discharge (Blue)\n",
    "    \"\"\"\n",
    "    # Initial Checks\n",
    "    if not water_balance:\n",
    "        logging.warning(\"plot_mass_balance: Water balance data empty; skipping plot.\")\n",
    "        return\n",
    "    if water_balance:\n",
    "        required_keys = ['time', 'active_runoff_generated', 'inlet_discharge', 'total_storage']\n",
    "        missing_keys = [k for k in required_keys if k not in water_balance[0]]\n",
    "        if missing_keys:\n",
    "            logging.error(f\"plot_mass_balance: Log missing required keys: {missing_keys}. Cannot proceed.\")\n",
    "            return\n",
    "\n",
    "    # 1) Calculate Cumulative Components\n",
    "    times_log = np.array([d['time'] for d in water_balance])\n",
    "    if times_log.size == 0:\n",
    "        logging.error(\"plot_mass_balance: times_log is empty.\")\n",
    "        return\n",
    "    prepend_zeros = False\n",
    "    if times_log.size > 0 and times_log[0] != 0:\n",
    "        times_log = np.insert(times_log, 0, 0.0); prepend_zeros = True\n",
    "\n",
    "    # Internal helper functions (with length adjustment)\n",
    "    def _get_cumulative(key: str) -> np.ndarray:\n",
    "        values = [d.get(key, 0.0) for d in water_balance]\n",
    "        numeric_values = [float(v) if isinstance(v, (int, float)) else 0.0 for v in values]\n",
    "        data = np.cumsum(numeric_values)\n",
    "        if prepend_zeros: data = np.insert(data, 0, 0.0)\n",
    "        if len(data) != len(times_log):\n",
    "            logging.warning(f\"_get_cumulative: Len mismatch key '{key}'. Adjusting.\"); target_len=len(times_log); current_len=len(data)\n",
    "            if current_len > target_len: data = data[:target_len]\n",
    "            elif current_len < target_len: data = np.pad(data, (0, target_len - current_len), 'edge')\n",
    "        return data\n",
    "\n",
    "    def _get_timeseries(key: str) -> np.ndarray:\n",
    "        values = [d.get(key, 0.0) for d in water_balance]\n",
    "        numeric_values = [float(v) if isinstance(v, (int, float)) else 0.0 for v in values]\n",
    "        data = np.array(numeric_values)\n",
    "        if prepend_zeros: data = np.insert(data, 0, 0.0)\n",
    "        if len(data) != len(times_log):\n",
    "            logging.warning(f\"_get_timeseries: Len mismatch key '{key}'. Adjusting.\"); target_len=len(times_log); current_len=len(data)\n",
    "            if current_len > target_len: data = data[:target_len]\n",
    "            elif current_len < target_len: data = np.pad(data, (0, target_len - current_len), 'edge')\n",
    "        return data\n",
    "\n",
    "    # Get Data Components\n",
    "    cum_runoff_active = _get_cumulative('active_runoff_generated')\n",
    "    cum_discharge_log = _get_cumulative('inlet_discharge')\n",
    "    storage_timeseries = _get_timeseries('total_storage')\n",
    "\n",
    "    # Calculate Recalculated Run-on\n",
    "    logging.info(\"plot_mass_balance: Calculating cumulative run-on infiltration from detailed time series...\")\n",
    "    total_runon_step_interp = np.zeros_like(times_log, dtype=float)\n",
    "    if runon_time_series_data:\n",
    "        for i,entry in enumerate(runon_time_series_data):\n",
    "            merged_id=entry.get('merged_id',f'Unknown_{i}'); event_times=np.array(entry.get('times',[]),dtype=float); event_vols=np.array(entry.get('runon_inf_vol_step',[]),dtype=float)\n",
    "            if event_times.size > 0 and event_times.size==event_vols.size:\n",
    "                sort_indices=np.argsort(event_times); times_sorted=event_times[sort_indices]; vols_sorted=event_vols[sort_indices]\n",
    "                unique_indices=np.unique(times_sorted,return_index=True)[1]\n",
    "                if len(unique_indices)>=2:\n",
    "                    times_unique=times_sorted[unique_indices]; vols_unique=vols_sorted[unique_indices]\n",
    "                    interp_vols=np.interp(times_log, times_unique, vols_unique, left=0.0, right=0.0); total_runon_step_interp += interp_vols\n",
    "            else: logging.warning(f\"Invalid data for run-on recalc '{merged_id}'.\")\n",
    "    else: logging.warning(\"No runon_time_series_data provided.\")\n",
    "\n",
    "    cum_runon_inf_recalculated = np.cumsum(total_runon_step_interp) # Recalculated Runon\n",
    "\n",
    "    # Prepare Plot Data\n",
    "    total_input_vol_summary = total_potential_runoff_input if total_potential_runoff_input is not None else float(cum_runoff_active[-1]) if len(cum_runoff_active) > 0 else 0.0\n",
    "    final_storage = storage_timeseries[-1] if len(storage_timeseries) > 0 else 0.0\n",
    "    total_runon_inf_final = cum_runon_inf_recalculated[-1] if len(cum_runon_inf_recalculated) > 0 else 0.0\n",
    "    total_discharge_final = cum_discharge_log[-1] if len(cum_discharge_log) > 0 else 0.0\n",
    "\n",
    "    # Scale potential runoff input line shape\n",
    "    final_active_runoff = cum_runoff_active[-1] if len(cum_runoff_active) > 0 else 0.0\n",
    "    scale_factor_input = (total_input_vol_summary / final_active_runoff) if final_active_runoff > 1e-9 else 1.0\n",
    "    cum_potential_runoff_plot = cum_runoff_active * scale_factor_input\n",
    "\n",
    "    # Assign plot variables\n",
    "    storage_plot = storage_timeseries\n",
    "    runon_inf_plot = cum_runon_inf_recalculated # This is Cumulative Run-on Infiltration\n",
    "    discharge_plot = cum_discharge_log\n",
    "\n",
    "    # Calculate Residual (\"Surface Runoff Transfer\")\n",
    "    min_len_resid = min(len(cum_potential_runoff_plot), len(storage_plot), len(runon_inf_plot), len(discharge_plot), len(times_log))\n",
    "    residual_calc = (cum_potential_runoff_plot[:min_len_resid] -\n",
    "                     storage_plot[:min_len_resid] -\n",
    "                     runon_inf_plot[:min_len_resid] -\n",
    "                     discharge_plot[:min_len_resid])\n",
    "    residual_calc = np.maximum(residual_calc, 0)\n",
    "    final_residual_val = residual_calc[-1] if len(residual_calc) > 0 else 0.0\n",
    "    residual_plot_padded = np.zeros_like(times_log)\n",
    "    residual_plot_padded[:min_len_resid] = residual_calc\n",
    "    if min_len_resid > 0 and min_len_resid < len(times_log): residual_plot_padded[min_len_resid:] = residual_calc[-1]\n",
    "\n",
    "    # MODIFICATION START: Combine Residual and Run-on Infiltration\n",
    "    combined_transfer_runon_plot = residual_plot_padded + runon_inf_plot\n",
    "    final_combined_transfer_runon_val = final_residual_val + total_runon_inf_final\n",
    "    # Modification End\n",
    "\n",
    "    # Stacking Bases (Updated Order)\n",
    "    y0_base = np.zeros_like(times_log)\n",
    "    y1_storage_top = storage_plot\n",
    "    y2_combined_top = y1_storage_top + combined_transfer_runon_plot\n",
    "    y3_discharge_top = y2_combined_top + discharge_plot\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(9, 9), dpi=450)\n",
    "    fontsize_title = 16\n",
    "    fontsize_labels = 14\n",
    "    fontsize_legend = 11\n",
    "\n",
    "    colors_mass_balance = {\n",
    "        'Surface Depression Storage': '#7f7f7f',\n",
    "        'Surface Runoff Transfer & Run-on Infiltration': '#ff7f0e',\n",
    "        'Outlet Discharge': '#1f77b4',\n",
    "        'Potential Input': 'black'\n",
    "    }\n",
    "    labels_mass_balance = {\n",
    "        'Surface Depression Storage': 'Surface Depression Storage',\n",
    "        'Surface Runoff Transfer & Run-on Infiltration': 'Surface Runoff Transfer & Run-on Infiltration',\n",
    "        'Outlet Discharge': 'Outlet Discharge',\n",
    "        'Potential Input': 'Cumulative Potential Runoff Input'\n",
    "    }\n",
    "\n",
    "    plot_arrays_check = [y0_base, y1_storage_top, y2_combined_top, y3_discharge_top, cum_potential_runoff_plot]\n",
    "    if not all(len(arr) == len(times_log) for arr in plot_arrays_check):\n",
    "        logging.error(f\"plot_mass_balance: Final array length mismatch before plotting. Aborting.\")\n",
    "        if fig: plt.close(fig);\n",
    "        return\n",
    "\n",
    "    ax.fill_between(times_log, y0_base, y1_storage_top, step='post', color=colors_mass_balance['Surface Depression Storage'], alpha=0.8, label=labels_mass_balance['Surface Depression Storage'])\n",
    "    ax.fill_between(times_log, y1_storage_top, y2_combined_top, step='post', color=colors_mass_balance['Surface Runoff Transfer & Run-on Infiltration'], alpha=0.8, label=labels_mass_balance['Surface Runoff Transfer & Run-on Infiltration'])\n",
    "    ax.fill_between(times_log, y2_combined_top, y3_discharge_top, step='post', color=colors_mass_balance['Outlet Discharge'], alpha=0.8, label=labels_mass_balance['Outlet Discharge'])\n",
    "\n",
    "    ax.step(times_log, cum_potential_runoff_plot, where='post',\n",
    "            label=labels_mass_balance['Potential Input'],\n",
    "            color=colors_mass_balance['Potential Input'], linewidth=2.0, linestyle='-')\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_xlabel(\"Time (min)\", fontsize=fontsize_labels)\n",
    "    ax.set_ylabel(\"Cumulative Volume (m³)\", fontsize=fontsize_labels)\n",
    "    ax.set_title(\"Cumulative Water Balance Components\", fontsize=fontsize_title)\n",
    "    ax.legend(loc='upper left', fontsize=fontsize_legend, frameon=True, facecolor='white', edgecolor='black')\n",
    "    leg = ax.get_legend()\n",
    "    if leg: leg.get_frame().set_linewidth(1.0)\n",
    "    ax.grid(False)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=fontsize_labels)\n",
    "    ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.2f'))\n",
    "    ax.set_ylim(bottom=0)\n",
    "    max_y_plot = np.max(cum_potential_runoff_plot) if len(cum_potential_runoff_plot) > 0 else 1.0\n",
    "    ax.set_ylim(bottom=0, top=max_y_plot * 1.05)\n",
    "    max_time = times_log[-1] if len(times_log) > 0 else 1.0\n",
    "    ax.set_xlim(0, max_time)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # PRINT RESULTS (Water Balance Summary - Updated for Combined Category)\n",
    "    print(\"\\n--- Water Balance Summary (Recalculated Run-on, Logged Discharge) ---\")\n",
    "    input_is_zero = abs(total_input_vol_summary) < 1e-9\n",
    "    print(f\"Total Potential Runoff Input: {total_input_vol_summary:10.2f} m³ (100.00%)\")\n",
    "    print(f\"    Accounted Outputs:\")\n",
    "    print(f\"      Surface Runoff Transfer & Run-on Infiltration: {final_combined_transfer_runon_val:10.2f} m³ \", end=\"\")\n",
    "    if not input_is_zero: print(f\"({final_combined_transfer_runon_val/total_input_vol_summary*100:6.1f}%)\")\n",
    "    else: print(\"(  N/A %)\")\n",
    "    print(f\"      Outlet Discharge:        {total_discharge_final:10.2f} m³ \", end=\"\")\n",
    "    if not input_is_zero: print(f\"({total_discharge_final/total_input_vol_summary*100:6.1f}%)\")\n",
    "    else: print(\"(  N/A %)\")\n",
    "    print(f\"      Surf. Depress. Storage:  {final_storage:10.2f} m³ \", end=\"\")\n",
    "    if not input_is_zero: print(f\"({final_storage/total_input_vol_summary*100:6.1f}%)\")\n",
    "    else: print(\"(  N/A %)\")\n",
    "    print(f\"      ---------------------------------------------------\")\n",
    "    summary_residual_error = total_input_vol_summary - final_combined_transfer_runon_val - total_discharge_final - final_storage\n",
    "    print(f\"      Mass Balance Difference (Error): {summary_residual_error:10.2f} m³ \", end=\"\")\n",
    "    if not input_is_zero: print(f\"({summary_residual_error/total_input_vol_summary*100:6.1f}%)\")\n",
    "    else: print(\"(  N/A %)\")\n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "def visualize_merge_framework(snapshots: List[Tuple[float, Dict]],\n",
    "                              simulator: Optional['IntegratedSimulation'],\n",
    "                              runoff_processor: Optional['RunoffProcessor'],\n",
    "                              selected_inlets: List[str],\n",
    "                              non_merged_ids: Set[str],\n",
    "                              simulation_time_min: float\n",
    "                             ) -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Visualizes merge events focusing on hydrographs and run-on calculation.\n",
    "    Applies specific formatting, labels, colors, and layout requested by user.\n",
    "    Helper functions (prep, safe_interp) are defined internally.\n",
    "    Uses recalculated run-on and heuristic discharge logic. Includes DEFENSIVE prep.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import logging\n",
    "    import matplotlib.gridspec as gridspec\n",
    "    import matplotlib.patches as mpatches\n",
    "\n",
    "    logging.info(\"Starting merge framework visualization (Hydrograph Focus)...\")\n",
    "\n",
    "    merge_volume_details = []\n",
    "    runon_time_series = []\n",
    "\n",
    "    if runoff_processor is None or not hasattr(runoff_processor, 'basin_runoff_data'):\n",
    "        logging.warning(\"visualize_merge_framework: RunoffProcessor/data missing.\")\n",
    "        return merge_volume_details, runon_time_series\n",
    "    if not runoff_processor.basin_runoff_data:\n",
    "        logging.warning(\"visualize_merge_framework: runoff_processor.basin_runoff_data is empty.\")\n",
    "        return merge_volume_details, runon_time_series\n",
    "\n",
    "    # === Internal Helper Functions === \n",
    "    def match_length(x: np.ndarray, y: np.ndarray):\n",
    "        n = min(x.size, y.size); return x[:n], y[:n]\n",
    "\n",
    "    def prep(d, tkey, qkey):\n",
    "        \"\"\"DEFENSIVE: Prepares timeline and flow data, converting flow to m³/min.\"\"\"\n",
    "        if d is None: return None, None\n",
    "        try:\n",
    "            t = d.get(tkey)\n",
    "            if t is None:\n",
    "                return None, None\n",
    "        except Exception as e_get_t:\n",
    "            logging.warning(f\"prep: Error retrieving t ({tkey}): {e_get_t}\")\n",
    "            return None, None\n",
    "\n",
    "        try:\n",
    "            q = d.get(qkey)\n",
    "            if q is None:\n",
    "                return None, None\n",
    "        except Exception as e_get_q:\n",
    "            logging.warning(f\"prep: Error retrieving q ({qkey}): {e_get_q}\")\n",
    "            return None, None\n",
    "\n",
    "        if not isinstance(t, (list, tuple, np.ndarray)) or \\\n",
    "           not isinstance(q, (list, tuple, np.ndarray)):\n",
    "            logging.warning(f\"prep: Invalid data type for {tkey}({type(t)}) or {qkey}({type(q)}).\")\n",
    "            return None, None\n",
    "\n",
    "        try:\n",
    "            ta = np.asarray(t, dtype=float)\n",
    "            qa = np.asarray(q, dtype=float) # m³/s\n",
    "            if np.any(np.isnan(ta)) or np.any(np.isinf(ta)): logging.warning(f\"prep: NaN/Inf time {tkey}.\"); return None, None\n",
    "            if np.any(np.isnan(qa)) or np.any(np.isinf(qa)): logging.warning(f\"prep: NaN/Inf quantity {qkey}.\"); return None, None\n",
    "\n",
    "            n = min(ta.size, qa.size)\n",
    "            if n == 0: logging.warning(f\"prep: Zero length array {tkey}/{qkey}.\"); return None, None\n",
    "            t_out, q_out = match_length(ta, qa)\n",
    "            return t_out, q_out * 60.0 # m³/min\n",
    "\n",
    "        except (ValueError, TypeError, MemoryError) as e:\n",
    "            logging.warning(f\"prep: Conversion/Processing error {tkey}/{qkey}: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def safe_interp(x_new, x_old, y_old, default_val=0.0, label=\"\"):\n",
    "        # Keep the robust safe_interp from previous versions\n",
    "        try:\n",
    "            x_new=np.asarray(x_new,dtype=float); x_old=np.asarray(x_old,dtype=float); y_old=np.asarray(y_old,dtype=float)\n",
    "            if x_new.size==0: return np.full_like(x_new, default_val, dtype=float)\n",
    "            if x_old.size==0 or y_old.size==0: return np.full(x_new.shape, default_val, dtype=float)\n",
    "            if x_old.size!=y_old.size: logging.warning(f\"safe_interp ({label}): x/y size mismatch. Defaulting.\"); return np.full(x_new.shape, default_val, dtype=float)\n",
    "            if np.any(np.isnan(x_old)) or np.any(np.isinf(x_old)): logging.warning(f\"safe_interp ({label}): NaN/Inf x_old. Defaulting.\"); return np.full(x_new.shape, default_val, dtype=float)\n",
    "            if np.any(np.isnan(y_old)) or np.any(np.isinf(y_old)): y_old=np.nan_to_num(y_old,nan=default_val,posinf=default_val,neginf=default_val); logging.warning(f\"safe_interp ({label}): NaN/Inf y_old replaced.\")\n",
    "            if not np.all(np.diff(x_old)>=0): sort_indices=np.argsort(x_old); x_old=x_old[sort_indices]; y_old=y_old[sort_indices]\n",
    "            unique_indices=np.unique(x_old,return_index=True)[1]\n",
    "            if len(unique_indices)<len(x_old): x_old=x_old[unique_indices]; y_old=y_old[unique_indices]\n",
    "            if x_old.size<2: logging.warning(f\"safe_interp ({label}): < 2 unique points. Cannot interp.\"); return np.full(x_new.shape, default_val, dtype=float)\n",
    "            interp_result=np.interp(x_new, x_old, y_old, left=default_val, right=default_val)\n",
    "            return np.nan_to_num(interp_result.astype(float), nan=default_val, posinf=default_val, neginf=default_val)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"safe_interp ({label}) failed: {e}\", exc_info=True); return np.full(np.asarray(x_new).shape, default_val, dtype=float)\n",
    "\n",
    "    # End Internal Helper Functions\n",
    "\n",
    "    # Gather Merge Events\n",
    "    merges = []\n",
    "    for bid, data in runoff_processor.basin_runoff_data.items():\n",
    "        if not isinstance(data, dict) or '+' not in bid: continue\n",
    "        required_keys = ('merged_from_ids', 'merge_time_min', 'convolution_timeline_min','original_lagged_flow_m3s', 'dt_sec', 'lag_used_sec','adjusted_upstream_lagged_flow_m3s')\n",
    "        if not all(k in data for k in required_keys): continue\n",
    "        if not isinstance(data['merged_from_ids'], list) or len(data['merged_from_ids']) < 2: continue\n",
    "        try: merge_time=float(data['merge_time_min']); lag_sec=float(data['lag_used_sec']); dt_sec=float(data['dt_sec']); assert dt_sec > 1e-9\n",
    "        except: continue\n",
    "        up_id, down_id = data['merged_from_ids'][1], data['merged_from_ids'][0]\n",
    "        merges.append({'merged_id': bid, 'up_id': str(up_id), 'down_id': str(down_id), 'time': merge_time, 'lag_sec': lag_sec, 'ut': np.array(data['convolution_timeline_min'], float), 'orig_flow_m3s': np.array(data['original_lagged_flow_m3s'], float), 'adj_flow_m3s': np.array(data['adjusted_upstream_lagged_flow_m3s'], float), 'dt': dt_sec})\n",
    "    merges.sort(key=lambda e: e['time'])\n",
    "    if not merges: print(\"No merge events found.\"); logging.warning(\"No merge events found.\"); return [], []\n",
    "\n",
    "    # Define Colors\n",
    "    colors = {\n",
    "        'Flow Rate': '#6c757d', 'Flow Rate Post-Merge': '#b2df8a', # Upstream: Gray base, Lt Green hatch\n",
    "        'Downstream Flow': '#1f77b4', # Blue for Downstream Base Flow\n",
    "        'Combined Flow': '#1f77b4',    # Blue\n",
    "        'Run-on Infiltration': '#ff7f0e', 'Surface Runoff Transfer': '#2ca02c', # Green\n",
    "        'Capacity Limit': '#adb5bd', 'Incoming Lagged Vol': '#6a3d9a',\n",
    "        'Original Lagged Flow': '#6a3d9a', 'Merge Time': '#e31a1c', 'Arrival Time': '#ff7f0e'\n",
    "    }\n",
    "\n",
    "    # Determine Axis Limits\n",
    "    max_y_flow = 0.01; max_y_vol = 0.01\n",
    "    for i_ev, ev in enumerate(merges):\n",
    "        up_data = runoff_processor.basin_runoff_data.get(ev['up_id'])\n",
    "        up_t, up_q_m3min = prep(up_data,'convolution_timeline_min','direct_flow_m3_s')\n",
    "        if up_q_m3min is not None: max_y_flow=max(max_y_flow, np.max(up_q_m3min) if up_q_m3min.size > 0 else 0)\n",
    "        dn_data = runoff_processor.basin_runoff_data.get(ev['down_id'])\n",
    "        dn_t, dn_q_m3min = prep(dn_data,'convolution_timeline_min','direct_flow_m3_s')\n",
    "        if dn_q_m3min is not None: max_y_flow=max(max_y_flow, np.max(dn_q_m3min) if dn_q_m3min.size > 0 else 0)\n",
    "        mg_data = runoff_processor.basin_runoff_data.get(ev['merged_id'])\n",
    "        mg_t, mg_q_m3min = prep(mg_data,'convolution_timeline_min','direct_flow_m3_s')\n",
    "        if mg_q_m3min is not None: max_y_flow=max(max_y_flow, np.max(mg_q_m3min) if mg_q_m3min.size > 0 else 0)\n",
    "        orig_lag_m3min=ev['orig_flow_m3s']*60.0; adj_lag_m3min=ev['adj_flow_m3s']*60.0\n",
    "        max_y_flow=max(max_y_flow, np.max(orig_lag_m3min) if orig_lag_m3min.size > 0 else 0); max_y_flow=max(max_y_flow, np.max(adj_lag_m3min) if adj_lag_m3min.size > 0 else 0)\n",
    "        dn_data_for_vol=runoff_processor.basin_runoff_data.get(ev['down_id'],{}); t_dn_def=np.array(dn_data_for_vol.get('timeline_min',[]),float); dn_def_vol=np.array(dn_data_for_vol.get('infiltration_deficit_vol_step',[]),float)\n",
    "        aligned_def=safe_interp(ev['ut'],t_dn_def,dn_def_vol); up_vol_step=ev['orig_flow_m3s']*ev['dt']; runon_vol_step=np.minimum(aligned_def, up_vol_step)\n",
    "        max_y_vol=max(max_y_vol, np.max(aligned_def) if aligned_def.size > 0 else 0); max_y_vol=max(max_y_vol, np.max(up_vol_step) if up_vol_step.size > 0 else 0); max_y_vol=max(max_y_vol, np.max(runon_vol_step) if runon_vol_step.size > 0 else 0)\n",
    "\n",
    "    x_limit = simulation_time_min\n",
    "    max_y_flow *= 1.1; max_y_vol *= 1.1\n",
    "\n",
    "    # Define Consistent Font Size\n",
    "    base_fontsize = 9; fontsize_title = base_fontsize + 2; fontsize_label = base_fontsize + 1; fontsize_legend = base_fontsize; fontsize_tick = base_fontsize\n",
    "\n",
    "    # Plotting Loop\n",
    "    for i, ev in enumerate(merges):\n",
    "        up_id, down_id, merged_id = ev['up_id'], ev['down_id'], ev['merged_id']; t_merge = ev['time']; lag_min = ev['lag_sec'] / 60.0; t_arrival = t_merge + lag_min\n",
    "        ut, orig_flow_m3s, adj_flow_m3s, dt = ev['ut'], ev['orig_flow_m3s'], ev['adj_flow_m3s'], ev['dt']\n",
    "        dn_data=runoff_processor.basin_runoff_data.get(down_id,{}); t_dn_def=np.array(dn_data.get('timeline_min',[]),float); dn_def_vol=np.array(dn_data.get('infiltration_deficit_vol_step',[]),float)\n",
    "        aligned_def_vol=safe_interp(ut,t_dn_def,dn_def_vol,label=f\"AlignDef {i}\")\n",
    "        incoming_lagged_vol=orig_flow_m3s*dt; runon_inf_vol_step=np.minimum(aligned_def_vol, incoming_lagged_vol); transfer_vol_step=np.maximum(0, incoming_lagged_vol - runon_inf_vol_step)\n",
    "        runon_time_series.append({'merged_id': merged_id, 'times': ut.tolist(), 'runon_inf_vol_step': runon_inf_vol_step.tolist()})\n",
    "        total_runon_calculated = float(np.sum(runon_inf_vol_step))\n",
    "        merge_volume_details.append({'merged_id': merged_id, 'up_id': up_id, 'down_id': down_id, 'infiltration_total': total_runon_calculated})\n",
    "\n",
    "        fig = plt.figure(figsize=(10, 11), dpi = 450); gs = gridspec.GridSpec(3, 2, height_ratios=[1, 1, 1.2], width_ratios=[1, 1], hspace=0.3, wspace=0.25)\n",
    "        legend_props = {'loc': 'upper right', 'fontsize': base_fontsize, 'frameon': True, 'facecolor': 'white', 'edgecolor': 'black'}\n",
    "        legend_frame_lw = 0.8; fill_edge_color = 'black'; fill_edge_lw = 0.5\n",
    "\n",
    "        ax_up = fig.add_subplot(gs[0, 0]); ax_up.set_title(f\"Upstream Response ({up_id})\", fontsize=fontsize_title)\n",
    "        up_t, up_q_m3min = prep(runoff_processor.basin_runoff_data.get(up_id), 'convolution_timeline_min', 'direct_flow_m3_s')\n",
    "        leg_handles_up = []\n",
    "        if up_t is not None and up_q_m3min is not None:\n",
    "            ax_up.fill_between(up_t, 0, up_q_m3min, step='post', color=colors['Flow Rate'], alpha=0.7, edgecolor=fill_edge_color, lw=fill_edge_lw)\n",
    "            leg_handles_up.append(mpatches.Patch(facecolor=colors['Flow Rate'], alpha=0.7, edgecolor=fill_edge_color, label='Depression Storage'))\n",
    "            idx_merge = np.searchsorted(up_t, t_merge, 'left')\n",
    "            ax_up.fill_between(up_t[idx_merge:], 0, up_q_m3min[idx_merge:], step='post', color=colors['Flow Rate Post-Merge'], alpha=0.8, hatch='///', edgecolor=fill_edge_color, lw=fill_edge_lw)\n",
    "            leg_handles_up.append(mpatches.Patch(facecolor=colors['Flow Rate Post-Merge'], alpha=0.8, hatch='///', edgecolor=fill_edge_color, label='Flow Post-Merge Trigger'))\n",
    "        line_merge_up = ax_up.axvline(t_merge, color=colors['Merge Time'], linestyle='--', linewidth=1.5, label=f\"Merge Time ({t_merge:.2f} min)\")\n",
    "        leg_handles_up.append(line_merge_up)\n",
    "        ax_up.set_ylabel(\"Flow Rate (m³/min)\", fontsize=fontsize_label); ax_up.legend(handles=leg_handles_up, **legend_props).get_frame().set_linewidth(legend_frame_lw)\n",
    "        ax_up.grid(False); ax_up.set_xlim(0, x_limit); ax_up.set_ylim(0, max_y_flow)\n",
    "        ax_up.tick_params(axis='both', labelsize=fontsize_tick); ax_up.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.3f'))\n",
    "\n",
    "        ax_dn = fig.add_subplot(gs[0, 1]); ax_dn.set_title(f\"Downstream Response ({down_id})\", fontsize=fontsize_title)\n",
    "        dn_t, dn_q_m3min = prep(runoff_processor.basin_runoff_data.get(down_id), 'convolution_timeline_min', 'direct_flow_m3_s')\n",
    "        leg_handles_dn = []\n",
    "        if dn_t is not None and dn_q_m3min is not None:\n",
    "            ax_dn.fill_between(dn_t, 0, dn_q_m3min, step='post', color=colors['Downstream Flow'], alpha=0.7, edgecolor=fill_edge_color, lw=fill_edge_lw)\n",
    "            leg_handles_dn.append(mpatches.Patch(facecolor=colors['Downstream Flow'], alpha=0.7, edgecolor=fill_edge_color, label='Flow Rate'))\n",
    "        line_merge_dn = ax_dn.axvline(t_merge, color=colors['Merge Time'], linestyle='--', linewidth=1.5, label=f\"Merge Time ({t_merge:.2f} min)\")\n",
    "        leg_handles_dn.append(line_merge_dn)\n",
    "        ax_dn.set_ylabel(\"Flow Rate (m³/min)\", fontsize=fontsize_label); ax_dn.legend(handles=leg_handles_dn, **legend_props).get_frame().set_linewidth(legend_frame_lw)\n",
    "        ax_dn.grid(False); ax_dn.set_xlim(0, x_limit); ax_dn.set_ylim(0, max_y_flow)\n",
    "        ax_dn.tick_params(axis='both', labelsize=fontsize_tick); ax_dn.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.3f'))\n",
    "\n",
    "        ax_runon = fig.add_subplot(gs[1, 0]); ax_runon.set_title(\"Run-on Calculation Detail\", fontsize=fontsize_title)\n",
    "        leg_handles_runon = []\n",
    "        fill_inf = ax_runon.fill_between(ut, 0, runon_inf_vol_step, step='post', color=colors['Run-on Infiltration'], alpha=0.8, edgecolor=fill_edge_color, lw=fill_edge_lw)\n",
    "        fill_trans = ax_runon.fill_between(ut, runon_inf_vol_step, runon_inf_vol_step + transfer_vol_step, step='post', interpolate=False, color=colors['Surface Runoff Transfer'], alpha=0.7, edgecolor=fill_edge_color, lw=fill_edge_lw)\n",
    "        leg_handles_runon.append(mpatches.Patch(facecolor=colors['Run-on Infiltration'], alpha=0.8, edgecolor=fill_edge_color, label=f'Run-on Infiltration ({total_runon_calculated:.3f} m³)'))\n",
    "        leg_handles_runon.append(mpatches.Patch(facecolor=colors['Surface Runoff Transfer'], alpha=0.7, edgecolor=fill_edge_color, label='Surface Runoff Transfer'))\n",
    "        line_cap, = ax_runon.step(ut, aligned_def_vol, where='post', color=colors['Capacity Limit'], linestyle='--', linewidth=1.5, label='Downstream Infil. Capacity')\n",
    "        line_mrg_r = ax_runon.axvline(t_merge, color=colors['Merge Time'], linestyle='--', linewidth=1.5, label=f\"Merge Time ({t_merge:.2f} min)\")\n",
    "        line_arr = ax_runon.axvline(t_arrival, color=colors['Arrival Time'], linestyle='-.', linewidth=1.5, label=f'Arrival Time ({t_arrival:.2f} min)')\n",
    "        leg_handles_runon.extend([line_cap, line_mrg_r, line_arr])\n",
    "        ax_runon.set_ylabel(\"Volume per Timestep (m³)\", fontsize=fontsize_label); ax_runon.legend(handles=leg_handles_runon, **legend_props).get_frame().set_linewidth(legend_frame_lw)\n",
    "        ax_runon.grid(False); ax_runon.set_xlim(0, x_limit); ax_runon.set_ylim(0, max_y_vol)\n",
    "        ax_runon.tick_params(axis='both', labelsize=fontsize_tick); ax_runon.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.4f'))\n",
    "\n",
    "        ax_lag = fig.add_subplot(gs[1, 1]); ax_lag.set_title(\"Lagged Upstream Flow (Post-Transfer)\", fontsize=fontsize_title)\n",
    "        orig_lag_m3min = orig_flow_m3s * 60.0; adj_lag_m3min = adj_flow_m3s * 60.0\n",
    "        min_len_lag = min(len(ut), len(orig_lag_m3min), len(adj_lag_m3min)); ut_lag, orig_lag_m3min, adj_lag_m3min = ut[:min_len_lag], orig_lag_m3min[:min_len_lag], adj_lag_m3min[:min_len_lag]\n",
    "        leg_handles_lag = []\n",
    "        fill_adj = ax_lag.fill_between(ut_lag, 0, adj_lag_m3min, step='post', color=colors['Surface Runoff Transfer'], alpha=0.7, edgecolor=fill_edge_color, lw=fill_edge_lw)\n",
    "        leg_handles_lag.append(mpatches.Patch(facecolor=colors['Surface Runoff Transfer'], alpha=0.7, edgecolor=fill_edge_color, label='Surface Runoff Transfer'))\n",
    "        line_orig, = ax_lag.step(ut_lag, orig_lag_m3min, where='post', color=colors['Original Lagged Flow'], linestyle=':', linewidth=1.5, label='Original Lagged Flow')\n",
    "        line_mrg_l = ax_lag.axvline(t_merge, color=colors['Merge Time'], linestyle='--', linewidth=1.5, label=f\"Merge Time ({t_merge:.2f} min)\")\n",
    "        line_arr_l = ax_lag.axvline(t_arrival, color=colors['Arrival Time'], linestyle='-.', linewidth=1.5, label=f\"Arrival Time ({t_arrival:.2f} min)\")\n",
    "        leg_handles_lag.extend([line_orig, line_mrg_l, line_arr_l])\n",
    "        ax_lag.set_ylabel(\"Flow Rate (m³/min)\", fontsize=fontsize_label); ax_lag.legend(handles=leg_handles_lag, **legend_props).get_frame().set_linewidth(legend_frame_lw)\n",
    "        ax_lag.grid(False); ax_lag.set_xlim(0, x_limit); ax_lag.set_ylim(0, max_y_flow)\n",
    "        ax_lag.tick_params(axis='both', labelsize=base_fontsize); ax_lag.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.3f'))\n",
    "\n",
    "        ax_comb = fig.add_subplot(gs[2, :]); ax_comb.set_title(f\"Combined Outlet Response ({merged_id})\", fontsize=fontsize_title + 1)\n",
    "        mg_t, mg_q_m3min = prep(runoff_processor.basin_runoff_data.get(merged_id), 'convolution_timeline_min', 'direct_flow_m3_s')\n",
    "        dn_t_comb, dn_q_m3min_comb = prep(runoff_processor.basin_runoff_data.get(down_id), 'convolution_timeline_min', 'direct_flow_m3_s')\n",
    "        ut_lag_comb = ut_lag; adj_lag_m3min_comb = adj_lag_m3min\n",
    "        leg_handles_comb = []\n",
    "        if mg_t is not None and mg_q_m3min is not None:\n",
    "            ax_comb.fill_between(mg_t, 0, mg_q_m3min, step='post', color=colors['Combined Flow'], alpha=0.7, edgecolor=fill_edge_color, lw=fill_edge_lw)\n",
    "            total_comb_vol = 0.0\n",
    "            if len(mg_t) > 1: total_comb_vol = float(np.trapz(mg_q_m3min, mg_t)) if hasattr(np,'trapezoid') else float(np.trapz(mg_q_m3min, mg_t))\n",
    "            leg_handles_comb.append(mpatches.Patch(facecolor=colors['Combined Flow'], alpha=0.7, edgecolor=fill_edge_color, label=f\"Combined Flow (Vol: {total_comb_vol:.3f} m³)\"))\n",
    "            if dn_t_comb is not None and dn_q_m3min_comb is not None:\n",
    "                line_dn_c, = ax_comb.step(dn_t_comb, dn_q_m3min_comb, where='post', color=colors['Downstream Flow'], linestyle='--', linewidth=1.5, label='Downstream Component')\n",
    "                leg_handles_comb.append(line_dn_c)\n",
    "            line_trn_c, = ax_comb.step(ut_lag_comb, adj_lag_m3min_comb, where='post', color=colors['Surface Runoff Transfer'], linestyle=':', linewidth=1.5, label='Surface Runoff Transfer')\n",
    "            leg_handles_comb.append(line_trn_c)\n",
    "        else: ax_comb.text(0.5, 0.5, \"Combined data not available\", ha='center', va='center', fontsize=base_fontsize)\n",
    "        ax_comb.set_xlabel(f\"Time (min)\", fontsize=fontsize_label + 1); ax_comb.set_ylabel(\"Flow Rate (m³/min)\", fontsize=fontsize_label + 1)\n",
    "        comb_legend_props = legend_props.copy(); comb_legend_props['fontsize'] = base_fontsize\n",
    "        ax_comb.legend(handles=leg_handles_comb, **comb_legend_props).get_frame().set_linewidth(legend_frame_lw)\n",
    "        ax_comb.grid(False); ax_comb.set_xlim(0, x_limit); ax_comb.set_ylim(0, max_y_flow)\n",
    "        ax_comb.tick_params(axis='both', labelsize=fontsize_tick + 1); ax_comb.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.3f'))\n",
    "\n",
    "        fig.suptitle(f\"Merge Event {i+1} (t={t_merge:.2f} min)\", fontsize=fontsize_title+2, y=0.98)\n",
    "        plt.tight_layout(rect=[0, 0.02, 1, 0.95])\n",
    "        plt.show(); plt.close(fig)\n",
    "\n",
    "    # Summary Table\n",
    "    print(\"\\n--- Run-on Infiltration Volume per Merge Event ---\")\n",
    "    print(f\"{'Merge Event (Upstream -> Downstream)':<45} | {'Run-on Infiltration (m³)':<18}\")\n",
    "    print(\"-\" * 45 + \"-+-\" + \"-\" * 18); total_all_runon = 0.0\n",
    "    for d in merge_volume_details: desc = f\"{d['up_id']} -> {d['down_id']}\"; vol = d['infiltration_total']; print(f\"{desc:<45} | {vol:<18.4f}\"); total_all_runon += vol\n",
    "    print(\"-\" * 45 + \"-+-\" + \"-\" * 18); print(f\"{'TOTAL':<45} | {total_all_runon:<18.4f}\\n\")\n",
    "\n",
    "    logging.info(\"Finished visualizing merge events.\")\n",
    "    return merge_volume_details, runon_time_series\n",
    "\n",
    "def plot_runon_time_series(runon_time_series: List[Dict],\n",
    "                           true_total_runon: float,\n",
    "                           simulation_time_min: float,\n",
    "                           figsize=(10,6)):\n",
    "    \"\"\"\n",
    "    Plots the *cumulative* run-on infiltration for all merge events,\n",
    "    and sets the x-axis limit to 150% of the simulation duration.\n",
    "    MODIFIED: Scales the final cumulative value to match the provided true_total_runon.\n",
    "\n",
    "    Parameters:\n",
    "    - runon_time_series: List of dicts with keys:\n",
    "        'merged_id', 'times', 'runon_inf_vol_step'\n",
    "    - true_total_runon: The sum of infiltration totals from merge_volume_details.\n",
    "    - simulation_time_min: total simulation duration (minutes)\n",
    "    - figsize: tuple for figure size\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import logging\n",
    "\n",
    "    if not runon_time_series:\n",
    "        print(\"No run-on infiltration data available.\")\n",
    "        return\n",
    "\n",
    "    # build a common time grid\n",
    "    all_times = []\n",
    "    for entry in runon_time_series:\n",
    "        t = entry.get('times')\n",
    "        if t is not None and len(t) > 0:\n",
    "            all_times.append(np.array(t, dtype=float))\n",
    "\n",
    "    if not all_times:\n",
    "        logging.warning(\"No valid time data in runon_time_series for plotting.\")\n",
    "        return\n",
    "\n",
    "    # Determine dt from the first valid event for simplicity, or use a default\n",
    "    dt = 1.0 # Default\n",
    "    for entry in runon_time_series:\n",
    "        t = np.array(entry.get('times', []), dtype=float)\n",
    "        if len(t) > 1:\n",
    "            diffs = np.diff(t)\n",
    "            valid_diffs = diffs[diffs > 1e-9]\n",
    "            if valid_diffs.size > 0:\n",
    "                dt = np.min(valid_diffs)\n",
    "                break\n",
    "\n",
    "    # grid runs from 0 to 1.5 * simulation_time_min\n",
    "    t_start = 0.0\n",
    "    t_end = simulation_time_min * 1.5\n",
    "    common_t = np.arange(t_start, t_end + dt, dt)\n",
    "    if common_t.size == 0: # Handle case where range is invalid\n",
    "        logging.warning(\"Could not create valid common time grid for runon plot.\")\n",
    "        return\n",
    "\n",
    "    # interpolate each event's step-vol onto common grid\n",
    "    stacked_steps = []\n",
    "    for entry in runon_time_series:\n",
    "        times = np.array(entry.get('times', []), dtype=float)\n",
    "        vols  = np.array(entry.get('runon_inf_vol_step', []), dtype=float)\n",
    "        if times.size > 0 and times.size == vols.size: # Check for valid data\n",
    "            # Ensure times are monotonically increasing for interpolation\n",
    "            sort_indices = np.argsort(times)\n",
    "            times_sorted = times[sort_indices]\n",
    "            vols_sorted = vols[sort_indices]\n",
    "            # Remove duplicates that might cause issues\n",
    "            unique_indices = np.unique(times_sorted, return_index=True)[1]\n",
    "            if len(unique_indices) >= 2: # Need at least 2 points to interpolate\n",
    "                times_unique = times_sorted[unique_indices]\n",
    "                vols_unique = vols_sorted[unique_indices]\n",
    "                vols_i = np.interp(common_t, times_unique, vols_unique, left=0.0, right=0.0)\n",
    "                stacked_steps.append(vols_i)\n",
    "            else:\n",
    "                # Handle case with < 2 unique points - add zeros\n",
    "                logging.debug(f\"Skipping interpolation for merge '{entry.get('merged_id', 'Unknown')}' due to insufficient unique time points.\")\n",
    "                stacked_steps.append(np.zeros_like(common_t))\n",
    "        else:\n",
    "            logging.warning(f\"Invalid/empty time/volume data for merge '{entry.get('merged_id', 'Unknown')}' in runon plot.\")\n",
    "            stacked_steps.append(np.zeros_like(common_t)) # Add zeros if data invalid\n",
    "\n",
    "    if not stacked_steps: # Check if any valid data was processed\n",
    "        logging.warning(\"No valid runon data found to stack for plot.\")\n",
    "        return\n",
    "\n",
    "    stacked_arr = np.vstack(stacked_steps)\n",
    "\n",
    "    # sum and cumulative sum\n",
    "    total_step = stacked_arr.sum(axis=0)\n",
    "    cum_total_calculated = np.cumsum(total_step)\n",
    "\n",
    "    # Scaling Step\n",
    "    calculated_final_val = cum_total_calculated[-1] if cum_total_calculated.size > 0 else 0.0\n",
    "    scale_factor = 1.0 # Default scale factor\n",
    "    if abs(calculated_final_val) > 1e-9: # Avoid division by zero\n",
    "        scale_factor = true_total_runon / calculated_final_val\n",
    "    elif abs(true_total_runon) > 1e-9:\n",
    "        # If calculated is zero but true total isn't, scaling is problematic.\n",
    "        # Log a warning, plot will likely show zero.\n",
    "        logging.warning(f\"Calculated final run-on volume is near zero ({calculated_final_val:.4e}), \"\n",
    "                        f\"but true total is {true_total_runon:.4f}. Cannot scale accurately.\")\n",
    "        # Keep scale_factor = 1.0, the plot will show the (near) zero calculated value.\n",
    "\n",
    "    logging.info(f\"Run-on Plot: True Total={true_total_runon:.4f}, Calculated Final={calculated_final_val:.4f}, Scale Factor={scale_factor:.4f}\")\n",
    "    cum_total_scaled = cum_total_calculated * scale_factor\n",
    "    # End Scaling Step\n",
    "\n",
    "    # plotting\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    # Plot the SCALED cumulative total\n",
    "    ax.plot(common_t, cum_total_scaled, linewidth=2,\n",
    "             label=f\"Cumulative Run-on Volume (Final ≈ {true_total_runon:.2f} m³)\")\n",
    "    ax.set_xlim(0, t_end)\n",
    "    ax.set_xlabel(\"Time (min)\")\n",
    "    ax.set_ylabel(\"Cumulative Infiltration Volume (m³)\")\n",
    "    ax.set_title(\"Cumulative Run-on Infiltration Over Time (Scaled to Match Total)\")\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def export_impervious_area_timeseries(\n",
    "    all_scenario_time_series: Dict[str, List[Dict]],\n",
    "    simulation_time_min_param: float,\n",
    "    start_datetime: pd.Timestamp,\n",
    "    export_interval_min: float = 1.0,\n",
    "    output_filename: str = \"all_inlets_impervious_area_timeseries.csv\") -> None:\n",
    "    \"\"\"\n",
    "    Exports a CSV file with impervious area timeseries for all inlets,\n",
    "    using absolute datetime for the time column.\n",
    "    \"\"\"\n",
    "    if not all_scenario_time_series:\n",
    "        logging.warning(\"No scenario time series data provided for export.\")\n",
    "        return\n",
    "\n",
    "    logging.info(f\"Exporting combined impervious area timeseries with datetime format to '{output_filename}'...\")\n",
    "\n",
    "    # 1. Create a common output timeline in minutes\n",
    "    common_timeline_min = np.arange(0, simulation_time_min_param + export_interval_min, export_interval_min)\n",
    "    common_timeline_min = common_timeline_min[common_timeline_min <= simulation_time_min_param + 1e-9]\n",
    "\n",
    "    if common_timeline_min.size == 0:\n",
    "        logging.warning(\"Could not create a valid common timeline for CSV export. Aborting.\")\n",
    "        return\n",
    "\n",
    "    # 2. NEW: Convert the relative minutes timeline to an absolute datetime timeline\n",
    "    # Use the provided start_datetime and add timedelta for each interval\n",
    "    absolute_datetimes = start_datetime + pd.to_timedelta(common_timeline_min, unit='m')\n",
    "\n",
    "    # Format the datetimes into the desired string format\n",
    "    formatted_datetimes = absolute_datetimes.strftime('%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "    # Initialize a DataFrame with the formatted, absolute datetime column\n",
    "    df_export = pd.DataFrame({'Time': formatted_datetimes})\n",
    "\n",
    "\n",
    "    # 3. For each inlet scenario, interpolate its impervious area onto the common timeline\n",
    "    for inlet_id, scenario_ts_log in all_scenario_time_series.items():\n",
    "        if not scenario_ts_log:\n",
    "            logging.warning(f\"Time series log for inlet '{inlet_id}' is empty. Skipping.\")\n",
    "            df_export[f'Inlet_{inlet_id}_Impervious_Area (m^2)'] = np.nan\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            original_times_min = np.array([d['time'] for d in scenario_ts_log], dtype=float)\n",
    "            impervious_areas = np.array([d.get('inlet_impervious_area', 0.0) for d in scenario_ts_log], dtype=float)\n",
    "\n",
    "            if original_times_min.size == 0 or impervious_areas.size == 0 or original_times_min.size != impervious_areas.size:\n",
    "                logging.warning(f\"Invalid time or impervious area array for inlet '{inlet_id}'. Skipping.\")\n",
    "                df_export[f'Inlet_{inlet_id}_Impervious_Area (m^2)'] = np.nan\n",
    "                continue\n",
    "\n",
    "            # Interpolate the area data onto the common timeline of *minutes*\n",
    "            interp_impervious_areas = safe_interp(\n",
    "                common_timeline_min, # Interpolate onto the numeric minute timeline\n",
    "                original_times_min,\n",
    "                impervious_areas,\n",
    "                default_val=0.0,\n",
    "                label=f\"ImpervAreaInlet_{inlet_id}\"\n",
    "            )\n",
    "            df_export[f'Inlet_{inlet_id}_Impervious_Area (m^2)'] = interp_impervious_areas\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to process impervious area for inlet '{inlet_id}': {e}\", exc_info=True)\n",
    "            df_export[f'Inlet_{inlet_id}_Impervious_Area (m^2)'] = np.nan\n",
    "\n",
    "    # 4. Export the combined DataFrame to CSV\n",
    "    try:\n",
    "        export_path = os.path.abspath(output_filename)\n",
    "        df_export.to_csv(export_path, index=False, float_format='%.3f')\n",
    "        logging.info(f\"Successfully exported combined impervious area timeseries to {export_path}\")\n",
    "        print(f\"Successfully exported combined impervious area timeseries to {export_path}\")\n",
    "    except Exception as e_export:\n",
    "        logging.error(f\"Failed to export impervious area timeseries CSV: {e_export}\", exc_info=True)\n",
    "        print(f\"ERROR: Failed to export impervious area timeseries CSV. Details: {traceback.format_exc()}\")\n",
    "\n",
    "def export_total_area_timeseries(\n",
    "    all_scenario_time_series: Dict[str, List[Dict]],\n",
    "    simulation_time_min_param: float,\n",
    "    start_datetime: pd.Timestamp,\n",
    "    export_interval_min: float = 1.0,\n",
    "    output_filename: str = \"all_inlets_total_area_timeseries.csv\") -> None:\n",
    "    \"\"\"\n",
    "    Exports a CSV file with total connected area timeseries for all inlets,\n",
    "    using absolute datetime for the time column.\n",
    "    \"\"\"\n",
    "    if not all_scenario_time_series:\n",
    "        logging.warning(\"No scenario time series data provided for total area export.\")\n",
    "        return\n",
    "\n",
    "    logging.info(f\"Exporting combined total area timeseries with datetime format to '{output_filename}'...\")\n",
    "\n",
    "    # 1. Create a common output timeline in minutes\n",
    "    common_timeline_min = np.arange(0, simulation_time_min_param + export_interval_min, export_interval_min)\n",
    "    common_timeline_min = common_timeline_min[common_timeline_min <= simulation_time_min_param + 1e-9]\n",
    "\n",
    "    if common_timeline_min.size == 0:\n",
    "        logging.warning(\"Could not create a valid common timeline for total area CSV export. Aborting.\")\n",
    "        return\n",
    "\n",
    "    # 2. Convert the relative minutes timeline to an absolute datetime timeline\n",
    "    absolute_datetimes = start_datetime + pd.to_timedelta(common_timeline_min, unit='m')\n",
    "    formatted_datetimes = absolute_datetimes.strftime('%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "    # Initialize a DataFrame with the formatted, absolute datetime column\n",
    "    df_export = pd.DataFrame({'Time': formatted_datetimes})\n",
    "\n",
    "    # 3. For each inlet scenario, interpolate its total area onto the common timeline\n",
    "    for inlet_id, scenario_ts_log in all_scenario_time_series.items():\n",
    "        if not scenario_ts_log:\n",
    "            logging.warning(f\"Time series log for inlet '{inlet_id}' is empty. Skipping total area export.\")\n",
    "            df_export[f'Inlet_{inlet_id}_Total_Area (m^2)'] = np.nan\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            original_times_min = np.array([d['time'] for d in scenario_ts_log], dtype=float)\n",
    "            # MODIFICATION: Target 'inlet_total_area' key\n",
    "            total_areas = np.array([d.get('inlet_total_area', 0.0) for d in scenario_ts_log], dtype=float)\n",
    "\n",
    "            if original_times_min.size == 0 or total_areas.size == 0 or original_times_min.size != total_areas.size:\n",
    "                logging.warning(f\"Invalid time or total area array for inlet '{inlet_id}'. Skipping.\")\n",
    "                df_export[f'Inlet_{inlet_id}_Total_Area (m^2)'] = np.nan\n",
    "                continue\n",
    "\n",
    "            # Interpolate the area data onto the common timeline of *minutes*\n",
    "            interp_total_areas = safe_interp(\n",
    "                common_timeline_min,\n",
    "                original_times_min,\n",
    "                total_areas,\n",
    "                default_val=0.0,\n",
    "                label=f\"TotalAreaInlet_{inlet_id}\"\n",
    "            )\n",
    "            # MODIFICATION: Update column header\n",
    "            df_export[f'Inlet_{inlet_id}_Total_Area (m^2)'] = interp_total_areas\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to process total area for inlet '{inlet_id}': {e}\", exc_info=True)\n",
    "            df_export[f'Inlet_{inlet_id}_Total_Area (m^2)'] = np.nan\n",
    "\n",
    "    # 4. Export the combined DataFrame to CSV\n",
    "    try:\n",
    "        export_path = os.path.abspath(output_filename)\n",
    "        df_export.to_csv(export_path, index=False, float_format='%.3f')\n",
    "        logging.info(f\"Successfully exported combined total area timeseries to {export_path}\")\n",
    "        print(f\"Successfully exported combined total area timeseries to {export_path}\")\n",
    "    except Exception as e_export:\n",
    "        logging.error(f\"Failed to export total area timeseries CSV: {e_export}\", exc_info=True)\n",
    "        print(f\"ERROR: Failed to export total area timeseries CSV. Details: {traceback.format_exc()}\")\n",
    "\n",
    "def get_node_color_and_size(state_data: dict, min_cap: float, max_cap: float, min_size=500, max_size=5000) -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Determines node color based on fill percentage and size based on capacity.\n",
    "    \"\"\"\n",
    "    volume = state_data.get('current_volume', 0.0)\n",
    "    capacity = state_data.get('max_volume', 0.0)\n",
    "\n",
    "    # Determine fill percentage\n",
    "    fill_percentage = (volume / capacity) * 100 if capacity > 1e-9 else 0\n",
    "    fill_percentage = max(0, min(fill_percentage, 100)) # Clamp between 0 and 100\n",
    "\n",
    "    # Determine color: Blue scale for fill percentage\n",
    "    norm = colors.Normalize(vmin=0, vmax=100)\n",
    "    cmap = cm.get_cmap('Blues')\n",
    "    color = colors.to_hex(cmap(norm(fill_percentage)))\n",
    "\n",
    "    # Determine size: Log scale based on capacity relative to min/max capacity\n",
    "    if capacity <= min_cap:\n",
    "        size = min_size\n",
    "    elif capacity >= max_cap:\n",
    "        size = max_size\n",
    "    else:\n",
    "        if min_cap > 0 and max_cap > min_cap: # Avoid log(0) or division by zero\n",
    "            log_min = np.log10(min_cap)\n",
    "            log_max = np.log10(max_cap)\n",
    "            log_cap = np.log10(max(capacity, 1e-9))\n",
    "            scale_factor = (log_cap - log_min) / (log_max - log_min)\n",
    "            size = min_size + (max_size - min_size) * scale_factor\n",
    "            size = max(min_size, min(size, max_size)) # Clamp size\n",
    "        else: # Fallback if capacities are problematic\n",
    "            size = (min_size + max_size) / 2\n",
    "\n",
    "    return color, size\n",
    "\n",
    "def plot_drainage_snapshots(forest: 'DrainageForest',\n",
    "                            snapshots: List[Tuple[float, Dict]],\n",
    "                            inlet_id: str,\n",
    "                            time_series: List[Dict],\n",
    "                            global_cap_min: float,\n",
    "                            global_cap_max: float,\n",
    "                            original_inlet_ids: List[str],\n",
    "                            output_dir: str = \"drainage_snapshots\",\n",
    "                            structure_changes_only: bool = False,\n",
    "                            max_plots: Optional[int] = None,\n",
    "                            runoff_processor: Optional['RunoffProcessor'] = None):\n",
    "    \"\"\"\n",
    "    Filters and plots drainage network snapshots based on specified criteria.\n",
    "    Calls plot_drainage_snapshot_with_ids for each selected snapshot.\n",
    "    Includes logic for an optional extra plot around the time rainfall stops.\n",
    "\n",
    "    Args:\n",
    "        forest: The DrainageForest object.\n",
    "        snapshots: A list of snapshot tuples (time_sec, state_dict).\n",
    "        inlet_id: The primary inlet ID for titling general snapshot plots.\n",
    "        time_series: The simulation time series log (currently unused by plotting but kept for signature).\n",
    "        global_cap_min: Minimum capacity found across all nodes for consistent sizing.\n",
    "        global_cap_max: Maximum capacity found across all nodes for consistent sizing.\n",
    "        original_inlet_ids: List of all originally selected inlet IDs for path highlighting.\n",
    "        output_dir: Directory to save snapshots (currently not used for saving in this function).\n",
    "        structure_changes_only: If True, plots only when network structure changes.\n",
    "        max_plots: Maximum number of plots to generate if structure_changes_only is False.\n",
    "                    If None and structure_changes_only is False, all snapshots are plotted.\n",
    "        runoff_processor: The RunoffProcessor instance, used for the rainfall end plot.\n",
    "    \"\"\"\n",
    "    if not snapshots:\n",
    "        logging.warning(\"No snapshots available to plot in plot_drainage_snapshots.\")\n",
    "        return\n",
    "\n",
    "    # Snapshot Filtering Logic\n",
    "    filtered_snapshots_to_plot: List[Tuple[float, Dict]] = []\n",
    "    logging.info(f\"Filtering {len(snapshots)} snapshots for plotting. Max_plots={max_plots}, Structure_only={structure_changes_only}\")\n",
    "\n",
    "    if structure_changes_only and len(snapshots) > 1:\n",
    "        filtered_snapshots_to_plot.append(snapshots[0]) # Always include the first snapshot\n",
    "        # Ensure state_dict exists before accessing keys\n",
    "        prev_snapshot_data = snapshots[0][1] if snapshots[0] and len(snapshots[0]) > 1 else {}\n",
    "        prev_node_ids_set = set(prev_snapshot_data.keys()) if prev_snapshot_data else set()\n",
    "        prev_parent_links = {k: v.get('parent_id') for k, v in prev_snapshot_data.items()} if prev_snapshot_data else {}\n",
    "\n",
    "        for i in range(1, len(snapshots)):\n",
    "            current_snap_tuple = snapshots[i]\n",
    "            # Basic validation of snapshot tuple structure\n",
    "            if not current_snap_tuple or len(current_snap_tuple) < 2:\n",
    "                logging.warning(f\"Skipping invalid snapshot at index {i} during filtering.\")\n",
    "                continue\n",
    "            \n",
    "            current_nodes_data_dict = current_snap_tuple[1]\n",
    "            if current_nodes_data_dict is None: # Handle cases where state_dict might be None\n",
    "                logging.warning(f\"Snapshot at index {i} (time {current_snap_tuple[0]}) has None state_dict. Comparing as empty.\")\n",
    "                current_node_ids_set = set()\n",
    "                current_parent_links = {}\n",
    "            else:\n",
    "                current_node_ids_set = set(current_nodes_data_dict.keys())\n",
    "                current_parent_links = {k: v.get('parent_id') for k, v in current_nodes_data_dict.items()}\n",
    "\n",
    "            # Check for changes in the set of nodes or their parent links\n",
    "            has_structure_changed = (current_node_ids_set != prev_node_ids_set) or \\\n",
    "                                     (current_parent_links != prev_parent_links)\n",
    "\n",
    "            if has_structure_changed:\n",
    "                filtered_snapshots_to_plot.append(current_snap_tuple)\n",
    "                prev_node_ids_set = current_node_ids_set\n",
    "                prev_parent_links = current_parent_links\n",
    "        \n",
    "        # Ensure the very last snapshot is included if its structure differs from the\n",
    "        # last one added,\n",
    "        # or if only the first was added (meaning no structural changes were found after\n",
    "        # the first).\n",
    "        if snapshots and (snapshots[-1] not in filtered_snapshots_to_plot):\n",
    "            should_add_final_snapshot = True\n",
    "            if filtered_snapshots_to_plot: # If any snapshots were already added\n",
    "                # Get data of the last snapshot ADDED to filtered_snapshots_to_plot\n",
    "                last_added_snap_data = filtered_snapshots_to_plot[-1][1] if filtered_snapshots_to_plot[-1] and len(filtered_snapshots_to_plot[-1]) > 1 else {}\n",
    "                last_added_ids = set(last_added_snap_data.keys()) if last_added_snap_data else set()\n",
    "                last_added_parents = {k:v.get('parent_id') for k,v in last_added_snap_data.items()} if last_added_snap_data else {}\n",
    "                \n",
    "                # Get data of the ACTUAL last snapshot in the full list\n",
    "                final_actual_snap_data = snapshots[-1][1] if snapshots[-1] and len(snapshots[-1]) > 1 else {}\n",
    "                final_actual_ids = set(final_actual_snap_data.keys()) if final_actual_snap_data else set()\n",
    "                final_actual_parents = {k:v.get('parent_id') for k,v in final_actual_snap_data.items()} if final_actual_snap_data else {}\n",
    "\n",
    "                # If the actual last snapshot is structurally identical to the last one\n",
    "                # already chosen, don't add it again\n",
    "                if (final_actual_ids == last_added_ids) and (final_actual_parents == last_added_parents):\n",
    "                    should_add_final_snapshot = False\n",
    "            \n",
    "            if should_add_final_snapshot:\n",
    "                    filtered_snapshots_to_plot.append(snapshots[-1])\n",
    "        \n",
    "        # If structure_only was true, but no changes were found after the first\n",
    "        # snapshot,\n",
    "        # ensure at least the first and last are plotted if they are different.\n",
    "        if not filtered_snapshots_to_plot and snapshots: \n",
    "            filtered_snapshots_to_plot.append(snapshots[0])\n",
    "            if len(snapshots) > 1 and snapshots[0] != snapshots[-1]: # Check if first and last are actually different\n",
    "                # This condition might be too strict if only content changed but not\n",
    "                # structure\n",
    "                # The above logic for should_add_final_snapshot is generally better\n",
    "                if snapshots[-1] not in filtered_snapshots_to_plot : # Add if not already there\n",
    "                    filtered_snapshots_to_plot.append(snapshots[-1])\n",
    "\n",
    "\n",
    "    elif max_plots is not None and max_plots > 0 and len(snapshots) > 0:\n",
    "        if len(snapshots) <= max_plots:\n",
    "            # Plot all valid snapshots if total is less than or equal to max_plots\n",
    "            filtered_snapshots_to_plot = [s for s in snapshots if s and len(s) > 1 and s[1] is not None]\n",
    "        else:\n",
    "            # Select first, last, and evenly spaced intermediate snapshots\n",
    "            plot_indices = set()\n",
    "            if max_plots >= 1: plot_indices.add(0)\n",
    "            if max_plots >= 2: plot_indices.add(len(snapshots) - 1)\n",
    "            \n",
    "            num_intermediate_to_select = max_plots - len(plot_indices)\n",
    "            if num_intermediate_to_select > 0 and len(snapshots) > 2: # Need at least 3 snapshots to pick intermediates\n",
    "                # Generate indices for intermediate snapshots, excluding 0 and len-1\n",
    "                # Linspace from index 1 to len-2 to pick from the \"middle\"\n",
    "                intermediate_indices_options = np.linspace(1, len(snapshots) - 2, num=len(snapshots)-2, dtype=int)\n",
    "                if len(intermediate_indices_options) >= num_intermediate_to_select :\n",
    "                    step = len(intermediate_indices_options) // num_intermediate_to_select\n",
    "                    selected_intermediate_indices = intermediate_indices_options[::step][:num_intermediate_to_select]\n",
    "                    for idx in selected_intermediate_indices:\n",
    "                        plot_indices.add(idx)\n",
    "                else: # Not enough options to pick distinct intermediates, add what we can\n",
    "                    for idx in intermediate_indices_options:\n",
    "                        plot_indices.add(idx)\n",
    "\n",
    "            sorted_indices = sorted(list(plot_indices))\n",
    "            # Ensure we don't exceed max_plots due to forcing first/last\n",
    "            if len(sorted_indices) > max_plots:\n",
    "                # This case should ideally be handled by adjusting\n",
    "                # num_intermediate_to_select more carefully\n",
    "                # For now, just take the first max_plots indices from the sorted list\n",
    "                sorted_indices = sorted_indices[:max_plots]\n",
    "\n",
    "            filtered_snapshots_to_plot = [snapshots[i] for i in sorted_indices if snapshots[i] and len(snapshots[i]) > 1 and snapshots[i][1] is not None]\n",
    "    else: # Default: Plot all valid snapshots if no specific filtering criteria met (max_plots is None or 0)\n",
    "        filtered_snapshots_to_plot = [s for s in snapshots if s and len(s) > 1 and s[1] is not None]\n",
    "    # End Snapshot Filtering\n",
    "\n",
    "    logging.info(f\"Plotting {len(filtered_snapshots_to_plot)} filtered snapshots (out of {len(snapshots)} total).\")\n",
    "\n",
    "    # Plotting Loop\n",
    "    for i, snapshot_data_tuple in enumerate(filtered_snapshots_to_plot):\n",
    "        time_sec_val, state_dict_val = snapshot_data_tuple # snapshot_data_tuple is (time_sec, state_dict)\n",
    "        if state_dict_val is None:\n",
    "            logging.warning(f\"Snapshot {i} (original time {time_sec_val/60.0:.2f} min) has None state. Skipping plot.\")\n",
    "            continue\n",
    "            \n",
    "        time_min_val = time_sec_val / 60.0\n",
    "        # Using original_inlet_ids[0] for the title if available, otherwise a generic\n",
    "        # title.\n",
    "        plot_title_inlet_id = str(original_inlet_ids[0]) if original_inlet_ids else inlet_id\n",
    "\n",
    "        print(f\"\\n--- Generating Plot for Filtered Snapshot {i+1}/{len(filtered_snapshots_to_plot)} (Simulation Time t={time_min_val:.2f} min) ---\")\n",
    "        fig, ax = plt.subplots(figsize=(14, 10))\n",
    "        try:\n",
    "            # Call the function that draws a single snapshot\n",
    "            plot_drainage_snapshot_with_ids(\n",
    "                ax=ax, \n",
    "                forest=forest, # Pass the forest object\n",
    "                snapshot=snapshot_data_tuple, \n",
    "                inlet_id=plot_title_inlet_id, # Main inlet for title\n",
    "                time_series=time_series, # Pass if needed by the drawing function\n",
    "                global_cap_min=global_cap_min,\n",
    "                global_cap_max=global_cap_max, \n",
    "                original_inlet_ids=original_inlet_ids # Pass all selected inlets for path highlighting\n",
    "            )\n",
    "            plt.show()\n",
    "        except Exception as plot_err:\n",
    "            logging.error(f\"Error plotting snapshot {i} (t={time_min_val:.2f} min): {plot_err}\")\n",
    "            traceback.print_exc()\n",
    "        finally:\n",
    "            # Ensure figure is closed to free memory, especially in loops\n",
    "            if 'fig' in locals() and fig is not None and plt.fignum_exists(fig.number):\n",
    "                    plt.close(fig)\n",
    "    # End Plotting Loop\n",
    "\n",
    "    # Logic for Extra Plot at Rainfall End\n",
    "    time_rain_stops_sec_calc = None\n",
    "    if runoff_processor and original_inlet_ids: \n",
    "        try:\n",
    "            primary_inlet_for_rain_check = str(original_inlet_ids[0])\n",
    "            inlet_rp_data = runoff_processor.basin_runoff_data.get(primary_inlet_for_rain_check)\n",
    "\n",
    "            if inlet_rp_data:\n",
    "                rain_timeline = np.array(inlet_rp_data.get('timeline_min', []))\n",
    "                rain_depths_step = np.array(inlet_rp_data.get('rain_depth_m_step', []))\n",
    "                dt_seconds = inlet_rp_data.get('dt_sec')\n",
    "\n",
    "                if rain_timeline.size > 0 and rain_depths_step.size == rain_timeline.size and dt_seconds is not None and dt_seconds > 0:\n",
    "                    significant_rain_threshold = 1e-7 \n",
    "                    significant_rain_indices = np.where(rain_depths_step > significant_rain_threshold)[0]\n",
    "                    \n",
    "                    if significant_rain_indices.size > 0:\n",
    "                        last_significant_rain_idx = significant_rain_indices[-1]\n",
    "                        if last_significant_rain_idx + 1 < len(rain_timeline):\n",
    "                            time_rain_stops_min_calc = rain_timeline[last_significant_rain_idx + 1]\n",
    "                        else: \n",
    "                            time_rain_stops_min_calc = rain_timeline[-1] + (dt_seconds / 60.0)\n",
    "                        time_rain_stops_sec_calc = time_rain_stops_min_calc * 60.0\n",
    "                        logging.info(f\"Rainfall for primary inlet '{primary_inlet_for_rain_check}' effectively stops around t={time_rain_stops_min_calc:.2f} min.\")\n",
    "                    else:\n",
    "                        logging.info(f\"No significant rainfall found in data for primary inlet '{primary_inlet_for_rain_check}'.\")\n",
    "                else:\n",
    "                    logging.warning(\"Rainfall data for primary inlet is incomplete for determining stop time.\")\n",
    "            else:\n",
    "                logging.warning(f\"No runoff data found for primary inlet '{primary_inlet_for_rain_check}' to determine rainfall stop time.\")\n",
    "            if time_rain_stops_sec_calc is None:\n",
    "                    logging.warning(\"Could not determine rainfall stop time for extra snapshot plot.\")\n",
    "        except IndexError:\n",
    "            logging.warning(\"original_inlet_ids list is empty, cannot determine primary inlet for rainfall stop time.\")\n",
    "        except Exception as e_rain_stop:\n",
    "            logging.error(f\"Error determining rainfall stop time: {e_rain_stop}\")\n",
    "    else:\n",
    "        logging.warning(\"RunoffProcessor or original_inlet_ids not available, cannot add rainfall end plot.\")\n",
    "\n",
    "    target_snapshot_for_rain_end = None\n",
    "    if time_rain_stops_sec_calc is not None and snapshots:\n",
    "        try:\n",
    "            target_snapshot_for_rain_end = min(snapshots, key=lambda s_item: abs(s_item[0] - time_rain_stops_sec_calc))\n",
    "            logging.info(f\"Snapshot at t={target_snapshot_for_rain_end[0]/60.0:.2f} min selected as closest to rainfall end (target: {time_rain_stops_sec_calc/60.0:.2f} min).\")\n",
    "        except Exception as e_find_snap:\n",
    "            logging.error(f\"Error finding snapshot closest to rainfall end: {e_find_snap}\")\n",
    "            target_snapshot_for_rain_end = None\n",
    "\n",
    "    if target_snapshot_for_rain_end:\n",
    "        logging.info(\"Generating extra plot for snapshot around rainfall end time.\")\n",
    "        time_sec_extra, state_dict_extra = target_snapshot_for_rain_end\n",
    "        if state_dict_extra is None:\n",
    "            logging.warning(\"State dictionary for rainfall-end snapshot is None. Skipping extra plot.\")\n",
    "        else:\n",
    "            time_min_extra = time_sec_extra / 60.0\n",
    "            plot_title_inlet_id_extra = str(original_inlet_ids[0]) if original_inlet_ids else inlet_id\n",
    "            print(f\"\\n--- Generating Plot for Rainfall End Snapshot (Approx. t={time_min_extra:.2f} min) ---\")\n",
    "            fig_extra, ax_extra = plt.subplots(figsize=(14, 10))\n",
    "            try:\n",
    "                plot_drainage_snapshot_with_ids(\n",
    "                    ax=ax_extra, forest=forest, snapshot=target_snapshot_for_rain_end, \n",
    "                    inlet_id=plot_title_inlet_id_extra,\n",
    "                    time_series=time_series, global_cap_min=global_cap_min,\n",
    "                    global_cap_max=global_cap_max, \n",
    "                    original_inlet_ids=original_inlet_ids\n",
    "                )\n",
    "                title_str = f\"Network State Approx. When Rainfall Stops (Snapshot at t={time_min_extra:.2f} min)\"\n",
    "                subtitle_str = f\"(Rainfall stop for primary inlet '{plot_title_inlet_id_extra}' estimated at t={time_rain_stops_sec_calc/60.0:.2f} min)\"\n",
    "                ax_extra.set_title(title_str + \"\\n\" + subtitle_str, fontsize=12)\n",
    "                plt.show()\n",
    "            except Exception as plot_err_extra:\n",
    "                logging.error(f\"Error plotting rainfall-end snapshot (t={time_min_extra:.2f} min): {plot_err_extra}\")\n",
    "                traceback.print_exc()\n",
    "            finally:\n",
    "                if 'fig_extra' in locals() and fig_extra is not None and plt.fignum_exists(fig_extra.number):\n",
    "                    plt.close(fig_extra)\n",
    "    else:\n",
    "        logging.info(\"Skipping extra snapshot plot as rainfall end time/snapshot was not determined or no snapshots available.\")\n",
    "    # End Added Extra Plot Logic\n",
    "\n",
    "    logging.info(f\"Finished plotting drainage snapshots.\")\n",
    "\n",
    "def filter_snapshots_for_plotting(tree_snapshots_log: List[Tuple[float, Dict]],\n",
    "                                   max_plots: Optional[int] = 5,\n",
    "                                   structure_only: bool = False) -> List[Tuple[float, Dict]]:\n",
    "    \"\"\"\n",
    "    Filters snapshots for plotting based on structure changes or a maximum number.\n",
    "    (This is a more robust version of the placeholder)\n",
    "    \"\"\"\n",
    "    if not tree_snapshots_log:\n",
    "        logging.warning(\"filter_snapshots_for_plotting: No snapshots to filter.\")\n",
    "        return []\n",
    "\n",
    "    filtered_snapshots = []\n",
    "    logging.info(f\"Filtering {len(tree_snapshots_log)} snapshots. Max_plots={max_plots}, Structure_only={structure_only}\")\n",
    "\n",
    "\n",
    "    if structure_only and len(tree_snapshots_log) > 1:\n",
    "        filtered_snapshots.append(tree_snapshots_log[0]) # Always include the first\n",
    "        # Ensure the state dictionary is not None before trying to access keys\n",
    "        prev_nodes_data = tree_snapshots_log[0][1] if tree_snapshots_log[0] and len(tree_snapshots_log[0]) > 1 else {}\n",
    "        prev_node_ids = set(prev_nodes_data.keys()) if prev_nodes_data else set()\n",
    "        prev_parents = {k: v.get('parent_id') for k, v in prev_nodes_data.items()} if prev_nodes_data else {}\n",
    "\n",
    "        for i in range(1, len(tree_snapshots_log)):\n",
    "            current_snapshot_tuple = tree_snapshots_log[i]\n",
    "            if not current_snapshot_tuple or len(current_snapshot_tuple) < 2:\n",
    "                logging.warning(f\"Skipping invalid snapshot at index {i} during filtering.\")\n",
    "                continue\n",
    "            current_nodes_data = current_snapshot_tuple[1]\n",
    "            if current_nodes_data is None: # Handle cases where state_dict might be None\n",
    "                logging.warning(f\"Snapshot at index {i} (time {current_snapshot_tuple[0]}) has None state_dict. Comparing as empty.\")\n",
    "                current_node_ids = set()\n",
    "                current_parents = {}\n",
    "            else:\n",
    "                current_node_ids = set(current_nodes_data.keys())\n",
    "                current_parents = {k: v.get('parent_id') for k, v in current_nodes_data.items()}\n",
    "\n",
    "            structure_changed = (current_node_ids != prev_node_ids) or (current_parents != prev_parents)\n",
    "\n",
    "            if structure_changed:\n",
    "                filtered_snapshots.append(current_snapshot_tuple)\n",
    "                prev_node_ids = current_node_ids\n",
    "                prev_parents = current_parents\n",
    "                # prev_nodes_data = current_nodes_data # Not strictly needed if only\n",
    "                # comparing IDs/parents\n",
    "\n",
    "        # Ensure the last snapshot is considered if it's different or if filtering was\n",
    "        # too aggressive\n",
    "        if tree_snapshots_log and tree_snapshots_log[-1] not in filtered_snapshots:\n",
    "            # Check if the last snapshot's structure is different from the last one\n",
    "            # added to filtered_snapshots\n",
    "            should_add_last = True\n",
    "            if filtered_snapshots: # if any snapshot was added other than the first\n",
    "                last_added_snapshot_data = filtered_snapshots[-1][1] if filtered_snapshots[-1] and len(filtered_snapshots[-1]) > 1 else {}\n",
    "\n",
    "                last_snapshot_data = tree_snapshots_log[-1][1] if tree_snapshots_log[-1] and len(tree_snapshots_log[-1]) > 1 else {}\n",
    "\n",
    "                last_added_ids = set(last_added_snapshot_data.keys()) if last_added_snapshot_data else set()\n",
    "                last_added_parents = {k:v.get('parent_id') for k,v in last_added_snapshot_data.items()} if last_added_snapshot_data else {}\n",
    "                \n",
    "                last_snapshot_ids = set(last_snapshot_data.keys()) if last_snapshot_data else set()\n",
    "                last_snapshot_parents = {k:v.get('parent_id') for k,v in last_snapshot_data.items()} if last_snapshot_data else {}\n",
    "\n",
    "                if (last_snapshot_ids == last_added_ids) and (last_snapshot_parents == last_added_parents):\n",
    "                    should_add_last = False\n",
    "            \n",
    "            if should_add_last:\n",
    "                    filtered_snapshots.append(tree_snapshots_log[-1])\n",
    "        \n",
    "        if not filtered_snapshots and tree_snapshots_log: # If structure_only and no changes, at least plot first and last\n",
    "            filtered_snapshots.append(tree_snapshots_log[0])\n",
    "            if len(tree_snapshots_log) > 1 and tree_snapshots_log[0] != tree_snapshots_log[-1]:\n",
    "                    filtered_snapshots.append(tree_snapshots_log[-1])\n",
    "\n",
    "\n",
    "    elif max_plots and max_plots > 0 and len(tree_snapshots_log) > 0 :\n",
    "        # Ensure at least first and last are included if max_plots allows\n",
    "        if max_plots == 1:\n",
    "            indices = [0]\n",
    "        elif max_plots == 2 and len(tree_snapshots_log) > 1:\n",
    "            indices = [0, len(tree_snapshots_log) - 1]\n",
    "        elif len(tree_snapshots_log) <= max_plots:\n",
    "            indices = list(range(len(tree_snapshots_log)))\n",
    "        else: # Select first, last, and evenly spaced in between\n",
    "            indices = sorted(list(set([0, len(tree_snapshots_log) - 1] + list(np.linspace(0, len(tree_snapshots_log) - 1, max_plots, dtype=int)))))\n",
    "            indices = indices[:max_plots] # Ensure we don't exceed max_plots\n",
    "\n",
    "        filtered_snapshots = [tree_snapshots_log[i] for i in indices if tree_snapshots_log[i] and tree_snapshots_log[i][1] is not None]\n",
    "    else: # Plot all if no filtering or max_plots is None/0\n",
    "        filtered_snapshots = [s for s in tree_snapshots_log if s and s[1] is not None]\n",
    "\n",
    "    logging.info(f\"Selected {len(filtered_snapshots)} snapshots for plotting.\")\n",
    "    return filtered_snapshots\n",
    "\n",
    "\n",
    "# 7) Main Functions\n",
    "def create_drainage_forest(lowest_points_file: str, basins_file: str,\n",
    "                           inlets_file: str, study_area_file: str,\n",
    "                           impervious_tif: str) -> Optional[DrainageForest]:\n",
    "    try:\n",
    "        logging.info(\"Loading input files for forest creation...\")\n",
    "        lp = gpd.read_file(lowest_points_file); bas = gpd.read_file(basins_file)\n",
    "        inl = gpd.read_file(inlets_file); std = gpd.read_file(study_area_file)\n",
    "        if lp.empty or bas.empty or inl.empty or std.empty:\n",
    "            logging.error(\"One or more input shapefiles are empty.\"); return None\n",
    "        req_bas_cols = ['basin_id', 'water_volu', 'area']\n",
    "        if not all(col in bas.columns for col in req_bas_cols):\n",
    "            logging.error(f\"Basins shapefile missing required columns: {req_bas_cols}.\"); return None\n",
    "        req_lp_cols = ['basin_id', 'to_basin']\n",
    "        if not all(col in lp.columns for col in req_lp_cols):\n",
    "            logging.error(f\"Lowest points shapefile missing required columns: {req_lp_cols}.\"); return None\n",
    "\n",
    "        forest = DrainageForest()\n",
    "        forest.build_forest(lp, bas, inl, std, impervious_tif)\n",
    "        if not forest.all_nodes:\n",
    "            logging.error(\"Drainage forest construction resulted in no nodes.\"); return None\n",
    "        return forest\n",
    "    except ImportError as e: logging.error(f\"Missing required Python package: {e}.\"); return None\n",
    "    except FileNotFoundError as e: logging.error(f\"Input file not found during forest creation: {e}\"); return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating drainage forest: {e}\"); import traceback; traceback.print_exc(); return None\n",
    "\n",
    "def run_integrated_simulation(forest: DrainageForest,\n",
    "                              inlet_ids: List[str],\n",
    "                              basins_file: str,\n",
    "                              sinks_file: str,\n",
    "                              lowest_points_file: str,\n",
    "                              dem_file: str,\n",
    "                              rainfall_csv: str,\n",
    "                              simulation_time_min: float,\n",
    "                              timestep_min: float,\n",
    "                              rain_unit: str = 'cm/hr',\n",
    "                              P2_in: float = 4.53\n",
    "                              ) -> Optional[Tuple[List[Dict], List[Dict], bool, str, Optional[IntegratedSimulation], RunoffProcessor]]:\n",
    "    if forest is None:\n",
    "        logging.error(\"No valid DrainageForest object provided.\"); return None\n",
    "    runoff_processor = None; simulator = None\n",
    "    try:\n",
    "        for node in forest.all_nodes.values():\n",
    "            min_cap = 1e-5\n",
    "            if node.storage_capacity < min_cap:\n",
    "                logging.debug(f\"Adjusting storage capacity for node {node.basin_id} from {node.storage_capacity:.2e} to {min_cap:.1e}\")\n",
    "                node.storage_capacity = min_cap; node.water_volume = min_cap\n",
    "                if node.area > 1e-9: node.effective_depth = node.storage_capacity / node.area\n",
    "                else: node.effective_depth = 0.0\n",
    "\n",
    "        basins_gdf = gpd.read_file(basins_file); sinks_gdf = gpd.read_file(sinks_file)\n",
    "        if 'basin_id_str' not in basins_gdf.columns: basins_gdf['basin_id_str'] = basins_gdf['basin_id'].astype(str)\n",
    "        if 'basin_id_str' not in sinks_gdf.columns: sinks_gdf['basin_id_str'] = sinks_gdf['basin_id'].astype(str)\n",
    "\n",
    "        logging.info(\"Initializing Runoff Processor...\")\n",
    "        runoff_processor = RunoffProcessor(\n",
    "            forest=forest, basins_gdf=basins_gdf, sinks_gdf=sinks_gdf,\n",
    "            lowest_points_file=lowest_points_file, dem_file=dem_file,\n",
    "            P2_in=P2_in\n",
    "        )\n",
    "        runoff_processor.rainfall_csv = rainfall_csv\n",
    "        runoff_processor.rain_unit = rain_unit\n",
    "\n",
    "        runoff_processor.calculate_runoff_for_all_basins(\n",
    "            rainfall_csv=rainfall_csv, sim_time_min=simulation_time_min,\n",
    "            timestep_min=timestep_min, rain_unit=rain_unit, inlet_ids=inlet_ids\n",
    "        )\n",
    "        if not runoff_processor.basin_runoff_data:\n",
    "            logging.error(\"Runoff processor failed to generate data for any relevant basins.\")\n",
    "            return None, None, True, \"Runoff calculation failed\", None, runoff_processor\n",
    "\n",
    "        logging.info(\"Initializing Integrated Simulator...\")\n",
    "        simulator = IntegratedSimulation(\n",
    "            forest=forest, runoff_processor=runoff_processor, selected_inlet_ids=inlet_ids\n",
    "        )\n",
    "        if not simulator.initialization_successful:\n",
    "            logging.error(\"IntegratedSimulation failed to initialize (e.g., could not derive sim time/step from runoff data).\")\n",
    "            return None, None, True, \"Simulator initialization failed\", simulator, runoff_processor\n",
    "\n",
    "        time_series_log, water_balance_log, tree_snapshots_log = simulator.run_simulation()\n",
    "\n",
    "        return time_series_log, water_balance_log, simulator.halted_prematurely, simulator.halt_reason, simulator, runoff_processor\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred during the integrated simulation run: {e}\")\n",
    "        import traceback; traceback.print_exc()\n",
    "        rp_instance = locals().get('runoff_processor', None); sim_instance = locals().get('simulator', None)\n",
    "        return None, None, True, f\"Unexpected Error: {e}\", sim_instance, rp_instance\n",
    "\n",
    "\n",
    "# 8) Main Execution Block (using main function)\n",
    "\n",
    "def main(P2_value: float):\n",
    "    \"\"\"\n",
    "    Main function to execute the drainage simulation and visualization.\n",
    "    MODIFIED: Performs a post-simulation recalculation of EIA based on arrival\n",
    "    time at the inlet and passes this corrected data for plotting.\n",
    "    NOW CALLS EXPORT FUNCTIONS FOR BOTH IMPERVIOUS AND TOTAL AREA.\n",
    "    \"\"\"\n",
    "    logging.info(\"--- Main Execution Started (Sequential Inlet Processing) ---\")\n",
    "    overall_start_time_main = time.time()\n",
    "\n",
    "    # Get the absolute start time for the entire simulation\n",
    "    simulation_start_datetime = get_rainfall_start_time(rainfall_csv)\n",
    "    if simulation_start_datetime is None:\n",
    "        logging.error(\"Could not determine simulation start time from rainfall CSV. Datetime export will not be possible.\")\n",
    "    else:\n",
    "        logging.info(f\"Simulation absolute start time determined: {simulation_start_datetime}\")\n",
    "\n",
    "    # Step 1 (Overall): Build the Master Drainage Forest (once for all scenarios)\n",
    "    master_forest: Optional[DrainageForest] = None\n",
    "    try:\n",
    "        logging.info(\"Building Master Drainage Forest (once for all scenarios)...\")\n",
    "        master_forest = create_drainage_forest(\n",
    "            lowest_points_file, basins_file, inlets_file,\n",
    "            study_area_file, impervious_tif\n",
    "        )\n",
    "        if master_forest is None or not master_forest.all_nodes:\n",
    "            raise RuntimeError(\"Failed to build a non-empty master drainage forest. Aborting all scenarios.\")\n",
    "        logging.info(\"Master drainage forest built successfully.\")\n",
    "    except Exception as e_forest:\n",
    "        logging.critical(f\"Failed to create master drainage forest: {e_forest}\", exc_info=True)\n",
    "        print(f\"CRITICAL ERROR: Failed to create master drainage forest. Details: {traceback.format_exc()}\", flush=True)\n",
    "        return # Exit main if forest creation fails\n",
    "\n",
    "    # Step 2: Loop through each selected inlet and run the scenario\n",
    "    if not selected_inlets or not isinstance(selected_inlets, list):\n",
    "        logging.warning(\"No inlets specified in 'selected_inlets' list or it's not a list. Nothing to simulate.\")\n",
    "        return\n",
    "\n",
    "    all_inlets_scenario_time_series_data: Dict[str, List[Dict]] = {}\n",
    "\n",
    "    for i, current_inlet_id_str in enumerate(selected_inlets):\n",
    "        scenario_start_time = time.time()\n",
    "        logging.info(f\"--- Starting Scenario {i+1}/{len(selected_inlets)} for Inlet: '{current_inlet_id_str}' ---\")\n",
    "        print(f\"\\n\\n======================================================================\")\n",
    "        print(f\"PROCESSING SCENARIO FOR INLET: {current_inlet_id_str}\")\n",
    "        print(f\"======================================================================\\n\")\n",
    "\n",
    "        current_selected_inlets_for_scenario = [str(current_inlet_id_str)]\n",
    "\n",
    "        simulator: Optional[IntegratedSimulation] = None\n",
    "        runoff_processor: Optional[RunoffProcessor] = None\n",
    "        time_series_log: Optional[List[Dict]] = None\n",
    "        water_balance_log: Optional[List[Dict]] = None\n",
    "        tree_snapshots_log: Optional[List[Tuple[float, Dict]]] = None\n",
    "        halted: bool = True\n",
    "        halt_reason: str = \"Initialization or simulation did not complete for scenario\"\n",
    "        merge_details: List[Dict] = []\n",
    "        runon_series: List[Dict] = []\n",
    "        final_outlet_id_scenario: Optional[str] = None\n",
    "        originals_scenario: Set[str] = set()\n",
    "        scenario_cap_min: float = 1e-3\n",
    "        scenario_cap_max: float = 1e-2\n",
    "\n",
    "        try:\n",
    "            sim_results = run_integrated_simulation(\n",
    "                forest=master_forest,\n",
    "                inlet_ids=current_selected_inlets_for_scenario,\n",
    "                basins_file=basins_file,\n",
    "                sinks_file=sinks_file,\n",
    "                lowest_points_file=lowest_points_file,\n",
    "                dem_file=dem_file,\n",
    "                rainfall_csv=rainfall_csv,\n",
    "                simulation_time_min=simulation_time_min,\n",
    "                timestep_min=timestep_min,\n",
    "                rain_unit=rain_unit,\n",
    "                P2_in=P2_value\n",
    "            )\n",
    "\n",
    "            if sim_results is None:\n",
    "                raise RuntimeError(f\"Integrated simulation returned None for inlet(s) {current_selected_inlets_for_scenario}.\")\n",
    "\n",
    "            time_series_log, water_balance_log, halted, halt_reason, simulator, runoff_processor = sim_results\n",
    "\n",
    "            if time_series_log:\n",
    "                all_inlets_scenario_time_series_data[current_inlet_id_str] = time_series_log\n",
    "            else:\n",
    "                logging.warning(f\"Time series log for inlet {current_inlet_id_str} is None. It will not be included in the combined CSV.\")\n",
    "\n",
    "            if simulator:\n",
    "                tree_snapshots_log = getattr(simulator, 'tree_snapshots', [])\n",
    "                originals_scenario = getattr(simulator, 'original_basin_ids_in_sim', set())\n",
    "            else:\n",
    "                tree_snapshots_log = []\n",
    "                originals_scenario = set()\n",
    "                logging.warning(f\"Simulator object is None after simulation run for {current_selected_inlets_for_scenario}.\")\n",
    "\n",
    "            if halted:\n",
    "                logging.warning(f\"Simulation for {current_selected_inlets_for_scenario} halted prematurely: {halt_reason}\")\n",
    "            else:\n",
    "                logging.info(f\"Simulation for {current_selected_inlets_for_scenario} completed.\")\n",
    "\n",
    "            if master_forest and master_forest.all_nodes:\n",
    "                node_capacities = [node.storage_capacity for node in master_forest.all_nodes.values() if node.storage_capacity > 1e-9]\n",
    "                if node_capacities:\n",
    "                    scenario_cap_min = max(min(node_capacities), 1e-6)\n",
    "                    scenario_cap_max = max(max(node_capacities), scenario_cap_min * 10.0 if scenario_cap_min > 0 else scenario_cap_min + 1e-2)\n",
    "            logging.info(f\"Node capacity range for snapshot sizing: min={scenario_cap_min:.3e}, max={scenario_cap_max:.3e}\")\n",
    "\n",
    "            merged_components_scenario = set()\n",
    "            if simulator:\n",
    "                history = getattr(simulator, 'merged_states_history', [])\n",
    "                for ev in history:\n",
    "                    if isinstance(ev, dict):\n",
    "                        merged_components_scenario.add(ev.get('upstream_id'))\n",
    "                        merged_components_scenario.add(ev.get('downstream_id'))\n",
    "            non_merged_ids_scenario = originals_scenario - merged_components_scenario\n",
    "\n",
    "            primary_inlet_for_scenario = str(current_selected_inlets_for_scenario[0]) if current_selected_inlets_for_scenario else None\n",
    "            final_outlet_id_scenario = primary_inlet_for_scenario\n",
    "            if simulator and runoff_processor and runoff_processor.basin_runoff_data and primary_inlet_for_scenario:\n",
    "                candidate_outlet = primary_inlet_for_scenario\n",
    "                latest_merge_time = -1.0\n",
    "                for basin_id_rp, data_rp in runoff_processor.basin_runoff_data.items():\n",
    "                    merged_from_ids_rp = [str(m_id) for m_id in data_rp.get('merged_from_ids', [])]\n",
    "                    if primary_inlet_for_scenario in merged_from_ids_rp:\n",
    "                        current_merge_time = data_rp.get('merge_time_min', 0.0)\n",
    "                        if current_merge_time > latest_merge_time:\n",
    "                            latest_merge_time = current_merge_time\n",
    "                            candidate_outlet = basin_id_rp\n",
    "                final_outlet_id_scenario = candidate_outlet\n",
    "            logging.info(f\"Final outlet ID for scenario {current_selected_inlets_for_scenario} plots: {final_outlet_id_scenario}\")\n",
    "\n",
    "            if simulator and runoff_processor and time_series_log and water_balance_log:\n",
    "                logging.info(f\"Starting plotting section for {current_selected_inlets_for_scenario}...\")\n",
    "\n",
    "                # Recalculate EIA based on arrival time\n",
    "                corrected_eia_df = simulator.calculate_eia_by_arrival_at_inlet()\n",
    "                if corrected_eia_df is not None:\n",
    "                    logging.info(\"Successfully recalculated EIA timeseries.\")\n",
    "                else:\n",
    "                    logging.warning(\"Failed to recalculate EIA timeseries.\")\n",
    "                \n",
    "                if 'visualize_merge_framework' in globals() and callable(visualize_merge_framework):\n",
    "                    merge_details, runon_series = visualize_merge_framework(\n",
    "                        tree_snapshots_log if tree_snapshots_log else [],\n",
    "                        simulator, runoff_processor,\n",
    "                        current_selected_inlets_for_scenario,\n",
    "                        non_merged_ids_scenario,\n",
    "                        simulation_time_min=simulation_time_min\n",
    "                    )\n",
    "                else: logging.warning(\"visualize_merge_framework function not found.\"); merge_details, runon_series = [], []\n",
    "                \n",
    "                if 'plot_runon_time_series' in globals() and callable(plot_runon_time_series) and runon_series:\n",
    "                    true_total_runon_for_plot = sum(d.get('infiltration_total', 0.0) for d in merge_details)\n",
    "                    plot_runon_time_series(runon_series, true_total_runon_for_plot, simulation_time_min)\n",
    "                else: logging.warning(\"plot_runon_time_series function not found or no runon_series data.\")\n",
    "                \n",
    "                if tree_snapshots_log:\n",
    "                    if 'plot_drainage_snapshots' in globals() and callable(plot_drainage_snapshots):\n",
    "                        plot_drainage_snapshots(\n",
    "                            forest=master_forest,\n",
    "                            snapshots=tree_snapshots_log,\n",
    "                            inlet_id=str(current_selected_inlets_for_scenario[0]) if current_selected_inlets_for_scenario else \"N/A\",\n",
    "                            time_series=time_series_log,\n",
    "                            global_cap_min=scenario_cap_min,\n",
    "                            global_cap_max=scenario_cap_max,\n",
    "                            original_inlet_ids=current_selected_inlets_for_scenario,\n",
    "                            structure_changes_only=True,\n",
    "                            max_plots=None,\n",
    "                            runoff_processor=runoff_processor\n",
    "                        )\n",
    "                    else: logging.warning(f\"No tree snapshots for {current_selected_inlets_for_scenario} to plot.\")\n",
    "                \n",
    "                if 'plot_timestep_water_balance' in globals() and callable(plot_timestep_water_balance) and water_balance_log:\n",
    "                    fig_ts_wb, ax_ts_wb = plot_timestep_water_balance(\n",
    "                        water_balance_log=water_balance_log,\n",
    "                        runon_time_series_data=runon_series,\n",
    "                        simulation_time_min_param=simulation_time_min\n",
    "                    )\n",
    "                    if fig_ts_wb: plt.show()\n",
    "                else: logging.warning(\"'plot_timestep_water_balance' not found or log missing.\")\n",
    "                \n",
    "                if 'plot_mass_balance' in globals() and callable(plot_mass_balance) and water_balance_log:\n",
    "                    total_potential_runoff_for_mb = 0.0\n",
    "                    if originals_scenario and runoff_processor and runoff_processor.basin_runoff_data:\n",
    "                        potential_volumes = [\n",
    "                            float(runoff_processor.basin_runoff_data[b_id].get('total_direct_runoff_vol', 0.0))\n",
    "                            for b_id in originals_scenario if b_id in runoff_processor.basin_runoff_data and\n",
    "                            isinstance(runoff_processor.basin_runoff_data[b_id].get('total_direct_runoff_vol', 0.0), (int, float, np.number)) and\n",
    "                            np.isfinite(runoff_processor.basin_runoff_data[b_id].get('total_direct_runoff_vol', 0.0))\n",
    "                        ]\n",
    "                        total_potential_runoff_for_mb = sum(potential_volumes)\n",
    "                    fig_stacked_mb, ax_stacked_mb = plot_mass_balance(\n",
    "                        water_balance=water_balance_log,\n",
    "                        runon_time_series_data=runon_series,\n",
    "                        total_potential_runoff_input=total_potential_runoff_for_mb\n",
    "                    )\n",
    "                    if fig_stacked_mb: plt.show()\n",
    "                else: logging.warning(\"'plot_mass_balance' not found or log missing.\")\n",
    "                \n",
    "                if 'plot_simulation_results' in globals() and callable(plot_simulation_results):\n",
    "                    primary_inlet_rp_data_scenario = runoff_processor.basin_runoff_data.get(str(current_selected_inlets_for_scenario[0])) if current_selected_inlets_for_scenario and runoff_processor else None\n",
    "                    final_outlet_rp_data_scenario = runoff_processor.basin_runoff_data.get(final_outlet_id_scenario) if final_outlet_id_scenario and runoff_processor else None\n",
    "                    plot_simulation_results(\n",
    "                        time_series=time_series_log,\n",
    "                        outlet_runoff_data=final_outlet_rp_data_scenario,\n",
    "                        primary_inlet_runoff_data=primary_inlet_rp_data_scenario,\n",
    "                        rain_unit=rain_unit,\n",
    "                        simulation_time_min=simulation_time_min,\n",
    "                        selected_inlets=current_selected_inlets_for_scenario,\n",
    "                        recalculated_eia_df=corrected_eia_df,\n",
    "                        export_discharge_csv=True\n",
    "                    )\n",
    "                else: logging.error(\"'plot_simulation_results' function not found.\")\n",
    "                \n",
    "                if 'plot_basin_runoff_stages' in globals() and callable(plot_basin_runoff_stages):\n",
    "                    if originals_scenario and runoff_processor and runoff_processor.basin_runoff_data:\n",
    "                        for b_id_orig in sorted(list(originals_scenario)):\n",
    "                            basin_data_for_stage = runoff_processor.basin_runoff_data.get(b_id_orig)\n",
    "                            if basin_data_for_stage:\n",
    "                                plot_basin_runoff_stages(\n",
    "                                    basin_runoff_data=basin_data_for_stage,\n",
    "                                    simulation_time_min=simulation_time_min,\n",
    "                                    rain_unit_pref=rain_unit\n",
    "                                )\n",
    "                else: logging.error(\"'plot_basin_runoff_stages' function not found.\")\n",
    "                logging.info(f\"Plotting section for {current_selected_inlets_for_scenario} finished.\")\n",
    "            else:\n",
    "                reason_skip = \"Simulation halted for scenario\" if halted else \"Simulator/RunoffProcessor/Logs missing for scenario\"\n",
    "                logging.warning(f\"Skipping plotting for {current_selected_inlets_for_scenario} because: {reason_skip}.\")\n",
    "\n",
    "        except RuntimeError as re_scenario:\n",
    "            logging.critical(f\"A RuntimeError occurred during processing for inlet(s) {current_selected_inlets_for_scenario}: {re_scenario}\", exc_info=False)\n",
    "            print(f\"CRITICAL RuntimeError for inlet(s) {current_selected_inlets_for_scenario}: {re_scenario}. Check logs for details.\", flush=True)\n",
    "        except Exception as e_scenario:\n",
    "            logging.critical(f\"An unexpected error occurred during processing for inlet(s) {current_selected_inlets_for_scenario}: {e_scenario}\", exc_info=True)\n",
    "            print(f\"ERROR for inlet(s) {current_selected_inlets_for_scenario}: {e_scenario}. Details: {traceback.format_exc()}\", flush=True)\n",
    "        finally:\n",
    "            elapsed_scenario = time.time() - scenario_start_time\n",
    "            logging.info(f\"--- Scenario for Inlet(s): {current_selected_inlets_for_scenario} finished in {elapsed_scenario:.2f}s ---\")\n",
    "            print(f\"--- Scenario for Inlet(s): {current_selected_inlets_for_scenario} finished in {elapsed_scenario:.2f}s ---\", flush=True)\n",
    "\n",
    "    # After all scenarios, call the export functions\n",
    "    if simulation_start_datetime:\n",
    "        if all_inlets_scenario_time_series_data:\n",
    "            # Export Impervious Area\n",
    "            if 'export_impervious_area_timeseries' in globals() and callable(export_impervious_area_timeseries):\n",
    "                export_impervious_area_timeseries(\n",
    "                    all_scenario_time_series=all_inlets_scenario_time_series_data,\n",
    "                    simulation_time_min_param=simulation_time_min,\n",
    "                    start_datetime=simulation_start_datetime,\n",
    "                    export_interval_min=0.5,\n",
    "                    output_filename=\"combined_inlet_impervious_areas.csv\"\n",
    "                )\n",
    "            else:\n",
    "                logging.error(\"'export_impervious_area_timeseries' function not defined. Cannot export impervious area CSV.\")\n",
    "\n",
    "            # Export Total Area\n",
    "            if 'export_total_area_timeseries' in globals() and callable(export_total_area_timeseries):\n",
    "                export_total_area_timeseries(\n",
    "                    all_scenario_time_series=all_inlets_scenario_time_series_data,\n",
    "                    simulation_time_min_param=simulation_time_min,\n",
    "                    start_datetime=simulation_start_datetime,\n",
    "                    export_interval_min=0.5,\n",
    "                    output_filename=\"combined_inlet_total_areas.csv\"\n",
    "                )\n",
    "            else:\n",
    "                logging.error(\"'export_total_area_timeseries' function not defined. Cannot export total area CSV.\")\n",
    "        else:\n",
    "            logging.warning(\"No data collected from any scenario; skipping CSV exports.\")\n",
    "    else:\n",
    "        logging.error(\"Cannot export with datetime format because simulation start time could not be determined.\")\n",
    "\n",
    "    overall_elapsed_main = time.time() - overall_start_time_main\n",
    "    logging.info(f\"--- Main execution (all scenarios) finished in {overall_elapsed_main:.2f}s ---\")\n",
    "    print(f\"--- Main execution (all scenarios) finished in {overall_elapsed_main:.2f}s ---\", flush=True)\n",
    "\n",
    "# 9) Main Execution Block (if __name__ == \"__main__\":)\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"--- PYTHON SCRIPT ENTRY POINT REACHED (Version: {sys.version}) ---\", flush=True)\n",
    "    script_start_time_overall = time.time()\n",
    "    \n",
    "    try:\n",
    "        # P2_USER_VALUE is a global from your USER INPUT SECTION\n",
    "        logging.info(f\"Global P2 value for Tc/Lag calculation being passed to main: {P2_USER_VALUE} inches\")\n",
    "        print(f\"Script using P2 value: {P2_USER_VALUE} inches\", flush=True)\n",
    "\n",
    "        # Call the main function which now handles the loop internally\n",
    "        main(P2_USER_VALUE)\n",
    "\n",
    "    except NameError as ne_global:\n",
    "        logging.critical(f\"A NameError occurred at the global/entry point level: {ne_global}. Ensure all required global variables are defined.\", exc_info=True)\n",
    "        print(f\"CRITICAL NameError (Global Scope): {ne_global}. Check script's USER INPUT SECTION. Details: {traceback.format_exc()}\", flush=True)\n",
    "    except RuntimeError as rt_global:\n",
    "        logging.critical(f\"A RuntimeError occurred: {rt_global}\", exc_info=True)\n",
    "        print(f\"CRITICAL RuntimeError: {rt_global}. Details: {traceback.format_exc()}\", flush=True)\n",
    "    except Exception as e_global:\n",
    "        logging.critical(f\"A critical error occurred at the top level of script execution: {e_global}\", exc_info=True)\n",
    "        print(f\"CRITICAL ERROR (Global Scope): {e_global}. Details: {traceback.format_exc()}\", flush=True)\n",
    "    finally:\n",
    "        script_end_time_overall = time.time()\n",
    "        elapsed_time_overall = script_end_time_overall - script_start_time_overall\n",
    "        logging.info(f\"--- Total Script Execution Time: {elapsed_time_overall:.2f} seconds ---\")\n",
    "        print(f\"--- PYTHON SCRIPT FINISHING --- Total Elapsed Time: {elapsed_time_overall:.2f} seconds\", flush=True)\n",
    "        try:\n",
    "            if logging.getLogger().hasHandlers():\n",
    "                logging.shutdown()\n",
    "        except Exception:\n",
    "            pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
